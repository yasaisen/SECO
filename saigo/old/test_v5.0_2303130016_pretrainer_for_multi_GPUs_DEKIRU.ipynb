{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MXcbCW8f7Eh"
   },
   "source": [
    "### Some Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.26.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.42.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.18.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: Scikit-learn in /opt/conda/lib/python3.7/site-packages (1.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from Scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from Scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from Scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /opt/conda/lib/python3.7/site-packages (from Scikit-learn) (1.18.1)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: Skipping opencv-python as it is not installed.\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: opencv-contrib-python in /opt/conda/lib/python3.7/site-packages (4.7.0.72)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.7/site-packages (from opencv-contrib-python) (1.18.1)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Hit:1 http://free.nchc.org.tw/ubuntu bionic InRelease\n",
      "Hit:2 http://free.nchc.org.tw/ubuntu bionic-updates InRelease                  \n",
      "Hit:3 http://free.nchc.org.tw/ubuntu bionic-backports InRelease                \n",
      "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1581 B]\n",
      "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
      "Hit:7 http://security.ubuntu.com/ubuntu bionic-security InRelease              \n",
      "Err:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC\n",
      "Ign:8 https://deb.obspy.org xenial InRelease                               \n",
      "Err:10 https://deb.obspy.org xenial Release    \n",
      "  Certificate verification failed: The certificate is NOT trusted. The certificate chain uses expired certificate.  Could not handshake: Error in the certificate verification. [IP: 95.217.194.95 443]\n",
      "Reading package lists... Done                  \n",
      "W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC\n",
      "E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease' is not signed.\n",
      "N: Updating from such a repository can't be done securely, and is therefore disabled by default.\n",
      "N: See apt-secure(8) manpage for repository creation and user configuration details.\n",
      "E: The repository 'http://deb.obspy.org xenial Release' does not have a Release file.\n",
      "N: Updating from such a repository can't be done securely, and is therefore disabled by default.\n",
      "N: See apt-secure(8) manpage for repository creation and user configuration details.\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install transformers\n",
    "!pip install Scikit-learn\n",
    "!pip uninstall opencv-python\n",
    "!pip install opencv-contrib-python\n",
    "!apt-get update && apt-get install ffmpeg libsm6 libxext6  -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 12 16:13:48 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.76       Driver Version: 515.76       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:1D:00.0 Off |                  N/A |\n",
      "| 31%   48C    P8    27W / 200W |   2039MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:23:00.0 Off |                  N/A |\n",
      "| 32%   49C    P8    22W / 200W |   2025MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "### for_multi_GPU\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5122,
     "status": "ok",
     "timestamp": 1677512171549,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "CvAFrPlS4bLU",
    "outputId": "2900b98f-10a0-48ae-ae6a-0aff7f787c20"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import json\n",
    "import math\n",
    "# import os\n",
    "# import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# import segmentation_models_pytorch as smp\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_rtoBGhgSae"
   },
   "source": [
    "### Set Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30443,
     "status": "ok",
     "timestamp": 1677512201987,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "UKCi74HiH1A9",
    "outputId": "779d592f-9214-477e-c922-233d7845cb7c"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "ROOT_PATH = '/home'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1677512201988,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "Uh-pffjlIDDw"
   },
   "outputs": [],
   "source": [
    "def checkpath(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 2263,
     "status": "ok",
     "timestamp": 1677512204231,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "SRDsG25wIKf-"
   },
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "Version = '230312_v0.1.0'\n",
    "\n",
    "root_folder = os.path.abspath(os.path.join(ROOT_PATH, Version))\n",
    "\n",
    "# model_DIR = os.path.abspath(os.path.join(root_folder, 'model'))\n",
    "# checkpath(root_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupVit Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2463,
     "status": "ok",
     "timestamp": 1677512217559,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "1tdXYOBI7W7Q"
   },
   "outputs": [],
   "source": [
    "import collections.abc\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "\n",
    "# from ...activations import ACT2FN\n",
    "from transformers.activations import ACT2FN\n",
    "\n",
    "# from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n",
    "from transformers.modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n",
    "# from ...modeling_utils import PreTrainedModel\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "# from ...utils import (\n",
    "#     ModelOutput,\n",
    "#     add_start_docstrings,\n",
    "#     add_start_docstrings_to_model_forward,\n",
    "#     logging,\n",
    "#     replace_return_docstrings,\n",
    "# )\n",
    "from transformers.utils import (\n",
    "    ModelOutput,\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "# from .configuration_groupvit import GroupViTConfig, GroupViTTextConfig, GroupViTVisionConfig\n",
    "from transformers.models.groupvit.configuration_groupvit import GroupViTConfig, GroupViTTextConfig, GroupViTVisionConfig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217559,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "M97ik37v9rbD"
   },
   "outputs": [],
   "source": [
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "_CHECKPOINT_FOR_DOC = \"nvidia/groupvit-gcc-yfcc\"\n",
    "\n",
    "GROUPVIT_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
    "    \"nvidia/groupvit-gcc-yfcc\",\n",
    "    # See all GroupViT models at https://huggingface.co/models?filter=groupvit\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217560,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "TfjSCoDn9utr"
   },
   "outputs": [],
   "source": [
    "# Copied from transformers.models.bart.modeling_bart._expand_mask\n",
    "def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n",
    "    \"\"\"\n",
    "    bsz, src_len = mask.size()\n",
    "    tgt_len = tgt_len if tgt_len is not None else src_len\n",
    "\n",
    "    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
    "\n",
    "    inverted_mask = 1.0 - expanded_mask\n",
    "\n",
    "    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217560,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "6PRDe_fu9xoT"
   },
   "outputs": [],
   "source": [
    "# contrastive loss function, adapted from\n",
    "# https://sachinruk.github.io/blog/pytorch/pytorch%20lightning/loss%20function/gpu/2021/03/07/CLIP.html\n",
    "def contrastive_loss(logits: torch.Tensor) -> torch.Tensor:\n",
    "    return nn.functional.cross_entropy(logits, torch.arange(len(logits), device=logits.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217560,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "NTvFTrtG9z1z"
   },
   "outputs": [],
   "source": [
    "# Copied from transformers.models.clip.modeling_clip.clip_loss with clip->groupvit\n",
    "def groupvit_loss(similarity: torch.Tensor) -> torch.Tensor:\n",
    "    caption_loss = contrastive_loss(similarity)\n",
    "    image_loss = contrastive_loss(similarity.t())\n",
    "    return (caption_loss + image_loss) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217560,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "iOvAJP0L91ns"
   },
   "outputs": [],
   "source": [
    "def hard_softmax(logits: torch.Tensor, dim: int):\n",
    "    y_soft = logits.softmax(dim)\n",
    "    # Straight through.\n",
    "    index = y_soft.max(dim, keepdim=True)[1]\n",
    "    y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(dim, index, 1.0)\n",
    "    ret = y_hard - y_soft.detach() + y_soft\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217560,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "6-b2H_fE93b1"
   },
   "outputs": [],
   "source": [
    "def gumbel_softmax(logits: torch.Tensor, tau: float = 1, hard: bool = False, dim: int = -1) -> torch.Tensor:\n",
    "    # more stable https://github.com/pytorch/pytorch/issues/41663\n",
    "    gumbel_dist = torch.distributions.gumbel.Gumbel(\n",
    "        torch.tensor(0.0, device=logits.device, dtype=logits.dtype),\n",
    "        torch.tensor(1.0, device=logits.device, dtype=logits.dtype),\n",
    "    )\n",
    "    gumbels = gumbel_dist.sample(logits.shape)\n",
    "\n",
    "    gumbels = (logits + gumbels) / tau  # ~Gumbel(logits,tau)\n",
    "    y_soft = gumbels.softmax(dim)\n",
    "\n",
    "    if hard:\n",
    "        # Straight through.\n",
    "        index = y_soft.max(dim, keepdim=True)[1]\n",
    "        y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(dim, index, 1.0)\n",
    "        ret = y_hard - y_soft.detach() + y_soft\n",
    "    else:\n",
    "        # Reparametrization trick.\n",
    "        ret = y_soft\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217561,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "sh78Xa0q9564"
   },
   "outputs": [],
   "source": [
    "def resize_attention_map(attentions, height, width, align_corners=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        attentions (`torch.Tensor`): attention map of shape [batch_size, groups, feat_height*feat_width]\n",
    "        height (`int`): height of the output attention map\n",
    "        width (`int`): width of the output attention map\n",
    "        align_corners (`bool`, *optional*): the `align_corner` argument for `nn.functional.interpolate`.\n",
    "\n",
    "    Returns:\n",
    "        `torch.Tensor`: resized attention map of shape [batch_size, groups, height, width]\n",
    "    \"\"\"\n",
    "\n",
    "    scale = (height * width // attentions.shape[2]) ** 0.5\n",
    "    if height > width:\n",
    "        feat_width = int(np.round(width / scale))\n",
    "        feat_height = attentions.shape[2] // feat_width\n",
    "    else:\n",
    "        feat_height = int(np.round(height / scale))\n",
    "        feat_width = attentions.shape[2] // feat_height\n",
    "\n",
    "    batch_size = attentions.shape[0]\n",
    "    groups = attentions.shape[1]  # number of group token\n",
    "    # [batch_size, groups, height*width, groups] -> [batch_size, groups, height, width]\n",
    "    attentions = attentions.reshape(batch_size, groups, feat_height, feat_width)\n",
    "    attentions = nn.functional.interpolate(\n",
    "        attentions, size=(height, width), mode=\"bilinear\", align_corners=align_corners\n",
    "    )\n",
    "    return attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217561,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "KyZtU92l98ix"
   },
   "outputs": [],
   "source": [
    "def get_grouping_from_attentions(attentions, hw_shape):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        attentions (`tuple(torch.FloatTensor)`: tuple of attention maps returned by `GroupViTVisionTransformer`\n",
    "        hw_shape (`tuple(int)`): height and width of the output attention map\n",
    "    Returns:\n",
    "        `torch.Tensor`: the attention map of shape [batch_size, groups, height, width]\n",
    "    \"\"\"\n",
    "\n",
    "    attn_maps = []\n",
    "    with torch.no_grad():\n",
    "        prev_attn_masks = None\n",
    "        for attn_masks in attentions:\n",
    "            # [batch_size, num_groups, height x width] -> [batch_size, height x width, num_groups]\n",
    "            attn_masks = attn_masks.permute(0, 2, 1).contiguous()\n",
    "            if prev_attn_masks is None:\n",
    "                prev_attn_masks = attn_masks\n",
    "            else:\n",
    "                prev_attn_masks = prev_attn_masks @ attn_masks\n",
    "            # [batch_size, heightxwidth, num_groups] -> [batch_size, num_groups, heightxwidth] -> [batch_size, num_groups, height, width]\n",
    "            cur_attn_map = resize_attention_map(prev_attn_masks.permute(0, 2, 1).contiguous(), *hw_shape)\n",
    "            attn_maps.append(cur_attn_map)\n",
    "\n",
    "    # [batch_size, num_groups, height, width]\n",
    "    final_grouping = attn_maps[-1]\n",
    "\n",
    "    return final_grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217561,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "sX37CrAn9_Mw"
   },
   "outputs": [],
   "source": [
    "class GroupViTCrossAttentionLayer(nn.Module):\n",
    "    def __init__(self, config: GroupViTVisionConfig):\n",
    "        super().__init__()\n",
    "        self.attn = GroupViTAttention(config)\n",
    "        self.norm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.mlp = GroupViTMLP(config)\n",
    "        self.norm_post = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, query, key):\n",
    "        x = query\n",
    "        x = x + self.attn(query, encoder_hidden_states=key)[0]\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        x = self.norm_post(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217561,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "LfMDcjbu-BFh"
   },
   "outputs": [],
   "source": [
    "class GroupViTAssignAttention(nn.Module):\n",
    "    def __init__(self, config: GroupViTVisionConfig):\n",
    "        super().__init__()\n",
    "        self.scale = config.hidden_size**-0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.proj = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.assign_eps = config.assign_eps\n",
    "\n",
    "    def get_attn(self, attn, gumbel=True, hard=True):\n",
    "        if gumbel and self.training:\n",
    "            attn = gumbel_softmax(attn, dim=-2, hard=hard)\n",
    "        else:\n",
    "            if hard:\n",
    "                attn = hard_softmax(attn, dim=-2)\n",
    "            else:\n",
    "                attn = nn.functional.softmax(attn, dim=-2)\n",
    "\n",
    "        return attn\n",
    "\n",
    "    def forward(self, query, key):\n",
    "        value = key\n",
    "        # [batch_size, query_length, channels]\n",
    "        query = self.q_proj(query)\n",
    "\n",
    "        # [batch_size, key_length, channels]\n",
    "        key = self.k_proj(key)\n",
    "\n",
    "        # [batch_size, key_length, channels]\n",
    "        value = self.v_proj(value)\n",
    "\n",
    "        # [batch_size, query_length, key_length]\n",
    "        raw_attn = (query @ key.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        attn = self.get_attn(raw_attn)\n",
    "        soft_attn = self.get_attn(raw_attn, gumbel=False, hard=False)\n",
    "\n",
    "        attn = attn / (attn.sum(dim=-1, keepdim=True) + self.assign_eps)\n",
    "\n",
    "        out = attn @ value\n",
    "\n",
    "        out = self.proj(out)\n",
    "\n",
    "        return out, soft_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217562,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "yOJBrOGt-GKD"
   },
   "outputs": [],
   "source": [
    "class GroupViTTokenAssign(nn.Module):\n",
    "    def __init__(self, config: GroupViTVisionConfig, num_group_token, num_output_group):\n",
    "        super().__init__()\n",
    "        self.num_output_group = num_output_group\n",
    "        # norm on group_tokens\n",
    "        self.norm_tokens = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        assign_mlp_ratio = (\n",
    "            config.assign_mlp_ratio\n",
    "            if isinstance(config.assign_mlp_ratio, collections.abc.Iterable)\n",
    "            else (config.assign_mlp_ratio, config.assign_mlp_ratio)\n",
    "        )\n",
    "        tokens_dim, channels_dim = [int(x * config.hidden_size) for x in assign_mlp_ratio]\n",
    "        self.mlp_inter = GroupViTMixerMLP(config, num_group_token, tokens_dim, num_output_group)\n",
    "        self.norm_post_tokens = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        # norm on x\n",
    "        self.norm_x = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.pre_assign_attn = GroupViTCrossAttentionLayer(config)\n",
    "\n",
    "        self.assign = GroupViTAssignAttention(config)\n",
    "        self.norm_new_x = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.mlp_channels = GroupViTMLP(config, config.hidden_size, channels_dim, config.hidden_size)\n",
    "\n",
    "    def project_group_token(self, group_tokens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            group_tokens (torch.Tensor): group tokens, [batch_size, num_group_tokens, channels]\n",
    "\n",
    "        Returns:\n",
    "            projected_group_tokens (torch.Tensor): [batch_size, num_output_groups, channels]\n",
    "        \"\"\"\n",
    "        # [B, num_output_groups, C] <- [B, num_group_tokens, C]\n",
    "        projected_group_tokens = self.mlp_inter(group_tokens)\n",
    "        projected_group_tokens = self.norm_post_tokens(projected_group_tokens)\n",
    "        return projected_group_tokens\n",
    "\n",
    "    def forward(self, image_tokens, group_tokens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_tokens (`torch.Tensor`): image tokens, of shape [batch_size, input_length, channels]\n",
    "            group_tokens (`torch.Tensor`): group tokens, [batch_size, num_group_tokens, channels]\n",
    "        \"\"\"\n",
    "\n",
    "        group_tokens = self.norm_tokens(group_tokens)\n",
    "        image_tokens = self.norm_x(image_tokens)\n",
    "        # [batch_size, num_output_groups, channels]\n",
    "        projected_group_tokens = self.project_group_token(group_tokens)\n",
    "        projected_group_tokens = self.pre_assign_attn(projected_group_tokens, image_tokens)\n",
    "        new_image_tokens, attention = self.assign(projected_group_tokens, image_tokens)\n",
    "        new_image_tokens += projected_group_tokens\n",
    "\n",
    "        new_image_tokens = new_image_tokens + self.mlp_channels(self.norm_new_x(new_image_tokens))\n",
    "\n",
    "        return new_image_tokens, attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217562,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "1eBLeUct-JCq"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GroupViTModelOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n",
    "            Contrastive loss for image-text similarity.\n",
    "        logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n",
    "            The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n",
    "            similarity scores.\n",
    "        logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n",
    "            The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n",
    "            similarity scores.\n",
    "        segmentation_logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels, logits_height, logits_width)`):\n",
    "            Classification scores for each pixel.\n",
    "\n",
    "            <Tip warning={true}>\n",
    "\n",
    "            The logits returned do not necessarily have the same size as the `pixel_values` passed as inputs. This is\n",
    "            to avoid doing two interpolations and lose some quality when a user needs to resize the logits to the\n",
    "            original image size as post-processing. You should always check your logits shape and resize as needed.\n",
    "\n",
    "            </Tip>\n",
    "\n",
    "        text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n",
    "            The text embeddings obtained by applying the projection layer to the pooled output of\n",
    "            [`GroupViTTextModel`].\n",
    "        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n",
    "            The image embeddings obtained by applying the projection layer to the pooled output of\n",
    "            [`GroupViTVisionModel`].\n",
    "        text_model_output (`BaseModelOutputWithPooling`):\n",
    "            The output of the [`GroupViTTextModel`].\n",
    "        vision_model_output (`BaseModelOutputWithPooling`):\n",
    "            The output of the [`GroupViTVisionModel`].\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits_per_image: torch.FloatTensor = None\n",
    "    logits_per_text: torch.FloatTensor = None\n",
    "    segmentation_logits: torch.FloatTensor = None\n",
    "    text_embeds: torch.FloatTensor = None\n",
    "    image_embeds: torch.FloatTensor = None\n",
    "    text_model_output: BaseModelOutputWithPooling = None\n",
    "    vision_model_output: BaseModelOutputWithPooling = None\n",
    "\n",
    "    def to_tuple(self) -> Tuple[Any]:\n",
    "        return tuple(\n",
    "            self[k] if k not in [\"text_model_output\", \"vision_model_output\"] else getattr(self, k).to_tuple()\n",
    "            for k in self.keys()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217562,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "8atGssmE-OgA"
   },
   "outputs": [],
   "source": [
    "class GroupViTPatchEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Image to Patch Embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int = 224,\n",
    "        patch_size: Union[int, Tuple[int, int]] = 16,\n",
    "        num_channels: int = 3,\n",
    "        embed_dim: int = 768,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        image_size = image_size if isinstance(image_size, collections.abc.Iterable) else (image_size, image_size)\n",
    "        patch_size = patch_size if isinstance(patch_size, collections.abc.Iterable) else (patch_size, patch_size)\n",
    "        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.projection = nn.Conv2d(num_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor, interpolate_pos_encoding: bool = False) -> torch.Tensor:\n",
    "        batch_size, num_channels, height, width = pixel_values.shape\n",
    "        if not interpolate_pos_encoding:\n",
    "            if height != self.image_size[0] or width != self.image_size[1]:\n",
    "                raise ValueError(\n",
    "                    f\"Input image size ({height}*{width}) doesn't match model\"\n",
    "                    f\" ({self.image_size[0]}*{self.image_size[1]}).\"\n",
    "                )\n",
    "        x = self.projection(pixel_values).flatten(2).transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217562,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "pvmCYxCX-Q5x"
   },
   "outputs": [],
   "source": [
    "class GroupViTVisionEmbeddings(nn.Module):\n",
    "    def __init__(self, config: GroupViTVisionConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embeddings = GroupViTPatchEmbeddings(\n",
    "            image_size=config.image_size,\n",
    "            patch_size=config.patch_size,\n",
    "            num_channels=config.num_channels,\n",
    "            embed_dim=config.hidden_size,\n",
    "        )\n",
    "        num_patches = self.patch_embeddings.num_patches\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches, config.hidden_size))\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.config = config\n",
    "\n",
    "    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher\n",
    "        resolution images.\n",
    "\n",
    "        Source:\n",
    "        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n",
    "        \"\"\"\n",
    "\n",
    "        npatch = embeddings.shape[1]\n",
    "        if npatch == self.position_embeddings.shape[1] and height == width:\n",
    "            return self.position_embeddings\n",
    "        patch_pos_embed = self.position_embeddings\n",
    "        num_original_pos_embed = patch_pos_embed.shape[1]\n",
    "        dim = embeddings.shape[-1]\n",
    "        feat_height = height // self.config.patch_size\n",
    "        feat_width = width // self.config.patch_size\n",
    "        # we add a small number to avoid floating point error in the interpolation\n",
    "        # see discussion at https://github.com/facebookresearch/dino/issues/8\n",
    "        feat_height, feat_width = feat_height + 0.1, feat_width + 0.1\n",
    "        original_height = original_width = math.sqrt(num_original_pos_embed)\n",
    "        reshaped_patch_pos_embed = patch_pos_embed.reshape(1, int(original_height), int(original_width), dim).permute(\n",
    "            0, 3, 1, 2\n",
    "        )\n",
    "        scale_factor = (feat_height / original_height, feat_width / original_width)\n",
    "        patch_pos_embed = nn.functional.interpolate(\n",
    "            reshaped_patch_pos_embed,\n",
    "            scale_factor=scale_factor,\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return patch_pos_embed\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor, interpolate_pos_encoding: bool = False) -> torch.Tensor:\n",
    "        batch_size, num_channels, height, width = pixel_values.shape\n",
    "        embeddings = self.patch_embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n",
    "\n",
    "        embeddings = self.layernorm(embeddings)\n",
    "\n",
    "        batch_size, seq_len, _ = embeddings.size()\n",
    "\n",
    "        # add positional encoding to each token\n",
    "        if interpolate_pos_encoding:\n",
    "            embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)\n",
    "        else:\n",
    "            embeddings = embeddings + self.position_embeddings\n",
    "\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217563,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "jZ_xSDus-ULA"
   },
   "outputs": [],
   "source": [
    "# Copied from transformers.models.clip.modeling_clip.CLIPTextEmbeddings with CLIP->GroupViT\n",
    "class GroupViTTextEmbeddings(nn.Module):\n",
    "    def __init__(self, config: GroupViTTextConfig):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(config.max_position_embeddings, embed_dim)\n",
    "\n",
    "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
    "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        seq_length = input_ids.shape[-1] if input_ids is not None else inputs_embeds.shape[-2]\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, :seq_length]\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.token_embedding(input_ids)\n",
    "\n",
    "        position_embeddings = self.position_embedding(position_ids)\n",
    "        embeddings = inputs_embeds + position_embeddings\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217563,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "WI0LJqF--anN"
   },
   "outputs": [],
   "source": [
    "class GroupViTStage(nn.Module):\n",
    "    \"\"\"This corresponds to the `GroupingLayer` class in the GroupViT implementation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: GroupViTVisionConfig,\n",
    "        depth: int,\n",
    "        num_prev_group_token: int,\n",
    "        num_group_token: int,\n",
    "        num_output_group: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.num_group_token = num_group_token\n",
    "        if num_group_token > 0:\n",
    "            self.group_token = nn.Parameter(torch.zeros(1, num_group_token, config.hidden_size))\n",
    "        else:\n",
    "            self.group_token = None\n",
    "        self.gradient_checkpointing = False\n",
    "        self.layers = nn.ModuleList([GroupViTEncoderLayer(config) for _ in range(depth)])\n",
    "\n",
    "        if num_group_token > 0:\n",
    "            self.downsample = GroupViTTokenAssign(\n",
    "                config=config,\n",
    "                num_group_token=num_group_token,\n",
    "                num_output_group=num_output_group,\n",
    "            )\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "        if num_prev_group_token > 0 and num_group_token > 0:\n",
    "            self.group_projector = nn.Sequential(\n",
    "                nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps),\n",
    "                GroupViTMixerMLP(config, num_prev_group_token, config.hidden_size // 2, num_group_token),\n",
    "            )\n",
    "        else:\n",
    "            self.group_projector = None\n",
    "\n",
    "    @property\n",
    "    def with_group_token(self):\n",
    "        return self.group_token is not None\n",
    "\n",
    "    def split_x(self, x):\n",
    "        if self.with_group_token:\n",
    "            return x[:, : -self.num_group_token], x[:, -self.num_group_token :]\n",
    "        else:\n",
    "            return x, None\n",
    "\n",
    "    def concat_x(self, x: torch.Tensor, group_token: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        if group_token is None:\n",
    "            return x\n",
    "        return torch.cat([x, group_token], dim=1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        prev_group_token: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.FloatTensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`): attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "                `(config.encoder_attention_heads,)`.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the grouping tensors of Grouping block.\n",
    "        \"\"\"\n",
    "        if self.with_group_token:\n",
    "            group_token = self.group_token.expand(hidden_states.size(0), -1, -1)\n",
    "            if self.group_projector is not None:\n",
    "                group_token = group_token + self.group_projector(prev_group_token)\n",
    "        else:\n",
    "            group_token = None\n",
    "\n",
    "        x = hidden_states\n",
    "\n",
    "        cat_x = self.concat_x(x, group_token)\n",
    "        for layer in self.layers:\n",
    "            layer_out = layer(cat_x, attention_mask=None, causal_attention_mask=None)\n",
    "            cat_x = layer_out[0]\n",
    "\n",
    "        x, group_token = self.split_x(cat_x)\n",
    "\n",
    "        attention = None\n",
    "        if self.downsample is not None:\n",
    "            x, attention = self.downsample(x, group_token)\n",
    "\n",
    "        outputs = (x, group_token)\n",
    "        if output_attentions:\n",
    "            outputs = outputs + (attention,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217563,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "lZNeThbG-bdS"
   },
   "outputs": [],
   "source": [
    "class GroupViTMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: GroupViTVisionConfig,\n",
    "        hidden_size: Optional[int] = None,\n",
    "        intermediate_size: Optional[int] = None,\n",
    "        output_size: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.activation_fn = ACT2FN[config.hidden_act]\n",
    "        hidden_size = hidden_size if hidden_size is not None else config.hidden_size\n",
    "        intermediate_size = intermediate_size if intermediate_size is not None else config.intermediate_size\n",
    "        output_size = output_size if output_size is not None else hidden_size\n",
    "        self.fc1 = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.fc2 = nn.Linear(intermediate_size, output_size)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.fc1(hidden_states)\n",
    "        hidden_states = self.activation_fn(hidden_states)\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217563,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "F5PfLQe2-dnX"
   },
   "outputs": [],
   "source": [
    "class GroupViTMixerMLP(GroupViTMLP):\n",
    "    def forward(self, x):\n",
    "        x = super().forward(x.transpose(1, 2))\n",
    "        return x.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217564,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "zIDEv8MG-fPE"
   },
   "outputs": [],
   "source": [
    "class GroupViTAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        if self.head_dim * self.num_heads != self.embed_dim:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n",
    "                f\" {self.num_heads}).\"\n",
    "            )\n",
    "        self.scale = self.head_dim**-0.5\n",
    "        self.dropout = config.attention_dropout\n",
    "\n",
    "        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
    "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        causal_attention_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
    "\n",
    "        bsz, tgt_len, embed_dim = hidden_states.size()\n",
    "        is_cross_attention = encoder_hidden_states is not None\n",
    "\n",
    "        # get query proj\n",
    "        query_states = self.q_proj(hidden_states) * self.scale\n",
    "        if is_cross_attention:\n",
    "            key_states = self._shape(self.k_proj(encoder_hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(encoder_hidden_states), -1, bsz)\n",
    "        else:\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "\n",
    "        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
    "        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
    "        key_states = key_states.view(*proj_shape)\n",
    "        value_states = value_states.view(*proj_shape)\n",
    "\n",
    "        src_len = key_states.size(1)\n",
    "        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
    "\n",
    "        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n",
    "                f\" {attn_weights.size()}\"\n",
    "            )\n",
    "\n",
    "        # apply the causal_attention_mask first\n",
    "        if causal_attention_mask is not None:\n",
    "            if causal_attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is\"\n",
    "                    f\" {causal_attention_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        if output_attentions:\n",
    "            # this operation is a bit akward, but it's required to\n",
    "            # make sure that attn_weights keeps its gradient.\n",
    "            # In order to do so, attn_weights have to reshaped\n",
    "            # twice and have to be reused in the following\n",
    "            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "        else:\n",
    "            attn_weights_reshaped = None\n",
    "\n",
    "        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "\n",
    "        attn_output = torch.bmm(attn_probs, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "        attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
    "\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights_reshaped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217564,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "MxxVauJA-ioM"
   },
   "outputs": [],
   "source": [
    "# Copied from transformers.models.clip.modeling_clip.CLIPEncoderLayer with CLIP->GroupViT\n",
    "class GroupViTEncoderLayer(nn.Module):\n",
    "    def __init__(self, config: GroupViTConfig):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.self_attn = GroupViTAttention(config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n",
    "        self.mlp = GroupViTMLP(config)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        causal_attention_mask: torch.Tensor,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.FloatTensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`): attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "                `(config.encoder_attention_heads,)`.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "        \"\"\"\n",
    "        residual = hidden_states\n",
    "\n",
    "        hidden_states = self.layer_norm1(hidden_states)\n",
    "        hidden_states, attn_weights = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            causal_attention_mask=causal_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.layer_norm2(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs += (attn_weights,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217564,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "iFAIWiAZ-k9h"
   },
   "outputs": [],
   "source": [
    "class GroupViTPreTrainedModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
    "    models.\n",
    "    \"\"\"\n",
    "\n",
    "    config_class = GroupViTConfig\n",
    "    base_model_prefix = \"groupvit\"\n",
    "    supports_gradient_checkpointing = True\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "\n",
    "        init_range = self.config.initializer_range\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=init_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "        factor = self.config.initializer_factor\n",
    "        if isinstance(module, GroupViTTextEmbeddings):\n",
    "            module.token_embedding.weight.data.normal_(mean=0.0, std=factor * 0.02)\n",
    "            module.position_embedding.weight.data.normal_(mean=0.0, std=factor * 0.02)\n",
    "        elif isinstance(module, GroupViTAttention):\n",
    "            factor = self.config.initializer_factor\n",
    "            in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n",
    "            out_proj_std = (module.embed_dim**-0.5) * factor\n",
    "            nn.init.normal_(module.q_proj.weight, std=in_proj_std)\n",
    "            nn.init.normal_(module.k_proj.weight, std=in_proj_std)\n",
    "            nn.init.normal_(module.v_proj.weight, std=in_proj_std)\n",
    "            nn.init.normal_(module.out_proj.weight, std=out_proj_std)\n",
    "        elif isinstance(module, GroupViTMLP):\n",
    "            factor = self.config.initializer_factor\n",
    "            in_proj_std = (\n",
    "                (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n",
    "            )\n",
    "            fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n",
    "            nn.init.normal_(module.fc1.weight, std=fc_std)\n",
    "            nn.init.normal_(module.fc2.weight, std=in_proj_std)\n",
    "\n",
    "    def _set_gradient_checkpointing(self, module, value=False):\n",
    "        if isinstance(module, (GroupViTTextEncoder, GroupViTVisionEncoder)):\n",
    "            module.gradient_checkpointing = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217564,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "jzru1jUe-wAJ"
   },
   "outputs": [],
   "source": [
    "GROUPVIT_START_DOCSTRING = r\"\"\"\n",
    "    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n",
    "    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n",
    "    behavior.\n",
    "\n",
    "    Parameters:\n",
    "        config ([`GroupViTConfig`]): Model configuration class with all the parameters of the model.\n",
    "            Initializing with a config file does not load the weights associated with the model, only the\n",
    "            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
    "\"\"\"\n",
    "\n",
    "GROUPVIT_TEXT_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n",
    "            it.\n",
    "\n",
    "            Indices can be obtained using [`CLIPTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
    "            [`PreTrainedTokenizer.__call__`] for details.\n",
    "\n",
    "            [What are input IDs?](../glossary#input-ids)\n",
    "        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "\n",
    "            [What are attention masks?](../glossary#attention-mask)\n",
    "        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n",
    "            config.max_position_embeddings - 1]`.\n",
    "\n",
    "            [What are position IDs?](../glossary#position-ids)\n",
    "        output_attentions (`bool`, *optional*):\n",
    "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
    "            tensors for more detail.\n",
    "        output_hidden_states (`bool`, *optional*):\n",
    "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
    "            more detail.\n",
    "        return_dict (`bool`, *optional*):\n",
    "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "\"\"\"\n",
    "\n",
    "GROUPVIT_VISION_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n",
    "            Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using\n",
    "            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details.\n",
    "        output_attentions (`bool`, *optional*):\n",
    "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
    "            tensors for more detail.\n",
    "        output_hidden_states (`bool`, *optional*):\n",
    "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
    "            more detail.\n",
    "        return_dict (`bool`, *optional*):\n",
    "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "\"\"\"\n",
    "\n",
    "GROUPVIT_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n",
    "            it.\n",
    "\n",
    "            Indices can be obtained using [`CLIPTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
    "            [`PreTrainedTokenizer.__call__`] for details.\n",
    "\n",
    "            [What are input IDs?](../glossary#input-ids)\n",
    "        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "\n",
    "            [What are attention masks?](../glossary#attention-mask)\n",
    "        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n",
    "            config.max_position_embeddings - 1]`.\n",
    "\n",
    "            [What are position IDs?](../glossary#position-ids)\n",
    "        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n",
    "            Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See\n",
    "            [`CLIPImageProcessor.__call__`] for details.\n",
    "        return_loss (`bool`, *optional*):\n",
    "            Whether or not to return the contrastive loss.\n",
    "        output_attentions (`bool`, *optional*):\n",
    "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
    "            tensors for more detail.\n",
    "        output_hidden_states (`bool`, *optional*):\n",
    "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
    "            more detail.\n",
    "        return_dict (`bool`, *optional*):\n",
    "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217564,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "diwTOUkc-yuM"
   },
   "outputs": [],
   "source": [
    "class GroupViTVisionEncoder(nn.Module):\n",
    "    def __init__(self, config: GroupViTVisionConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.stages = nn.ModuleList(\n",
    "            [\n",
    "                GroupViTStage(\n",
    "                    config=config,\n",
    "                    depth=config.depths[i],\n",
    "                    num_group_token=config.num_group_tokens[i],\n",
    "                    num_output_group=config.num_output_groups[i],\n",
    "                    num_prev_group_token=config.num_output_groups[i - 1] if i > 0 else 0,\n",
    "                )\n",
    "                for i in range(len(config.depths))\n",
    "            ]\n",
    "        )\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[tuple, BaseModelOutput]:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_groupings = () if output_attentions else None\n",
    "\n",
    "        group_tokens = None\n",
    "\n",
    "        for i, stage in enumerate(self.stages):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_outputs = stage(hidden_states, group_tokens, output_attentions)\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            group_tokens = layer_outputs[1]\n",
    "\n",
    "            if output_attentions and layer_outputs[2] is not None:\n",
    "                all_groupings = all_groupings + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, all_hidden_states, all_groupings] if v is not None)\n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=hidden_states, hidden_states=all_hidden_states, attentions=all_groupings\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217565,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "ErgnsQgL-zvh"
   },
   "outputs": [],
   "source": [
    "class GroupViTTextEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder consisting of `config.num_hidden_layers` self-attention layers. Each layer is a\n",
    "    [`GroupViTEncoderLayer`].\n",
    "\n",
    "    Args:\n",
    "        config: GroupViTTextConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: GroupViTTextConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layers = nn.ModuleList([GroupViTEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        inputs_embeds,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        causal_attention_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutput]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n",
    "                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n",
    "                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n",
    "                than the model's internal embedding lookup matrix.\n",
    "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            causal_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Causal mask for the text model. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            output_hidden_states (`bool`, *optional*):\n",
    "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
    "                for more detail.\n",
    "            return_dict (`bool`, *optional*):\n",
    "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        encoder_states = () if output_hidden_states else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "        for idx, encoder_layer in enumerate(self.layers):\n",
    "            if output_hidden_states:\n",
    "                encoder_states = encoder_states + (hidden_states,)\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(encoder_layer),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    causal_attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = encoder_layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    causal_attention_mask,\n",
    "                    output_attentions=output_attentions,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            encoder_states = encoder_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217566,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "fsgeLvGW_gB0"
   },
   "outputs": [],
   "source": [
    "# Copied from transformers.models.clip.modeling_clip.CLIPTextTransformer with CLIPText->GroupViTText, CLIPEncoder->GroupViTTextEncoder, CLIP_TEXT->GROUPVIT_TEXT\n",
    "class GroupViTTextTransformer(nn.Module):\n",
    "    def __init__(self, config: GroupViTTextConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        embed_dim = config.hidden_size\n",
    "        self.embeddings = GroupViTTextEmbeddings(config)\n",
    "        self.encoder = GroupViTTextEncoder(config)\n",
    "        self.final_layer_norm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(GROUPVIT_TEXT_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=GroupViTTextConfig)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if input_ids is None:\n",
    "            raise ValueError(\"You have to specify input_ids\")\n",
    "\n",
    "        input_shape = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, input_shape[-1])\n",
    "\n",
    "        hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n",
    "\n",
    "        bsz, seq_len = input_shape\n",
    "        # CLIP's text model uses causal mask, prepare it here.\n",
    "        # https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/clip/model.py#L324\n",
    "        causal_attention_mask = self._build_causal_attention_mask(bsz, seq_len, hidden_states.dtype).to(\n",
    "            hidden_states.device\n",
    "        )\n",
    "        # expand attention_mask\n",
    "        if attention_mask is not None:\n",
    "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "            attention_mask = _expand_mask(attention_mask, hidden_states.dtype)\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            inputs_embeds=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            causal_attention_mask=causal_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        last_hidden_state = encoder_outputs[0]\n",
    "        last_hidden_state = self.final_layer_norm(last_hidden_state)\n",
    "\n",
    "        # text_embeds.shape = [batch_size, sequence_length, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        # casting to torch.int for onnx compatibility: argmax doesn't support int64 inputs with opset 14\n",
    "        pooled_output = last_hidden_state[\n",
    "            torch.arange(last_hidden_state.shape[0], device=last_hidden_state.device),\n",
    "            input_ids.to(dtype=torch.int, device=last_hidden_state.device).argmax(dim=-1),\n",
    "        ]\n",
    "\n",
    "        if not return_dict:\n",
    "            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPooling(\n",
    "            last_hidden_state=last_hidden_state,\n",
    "            pooler_output=pooled_output,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def _build_causal_attention_mask(self, bsz, seq_len, dtype):\n",
    "        # lazily create causal attention mask, with full attention between the vision tokens\n",
    "        # pytorch uses additive attention mask; fill with -inf\n",
    "        mask = torch.empty(bsz, seq_len, seq_len, dtype=dtype)\n",
    "        mask.fill_(torch.tensor(torch.finfo(dtype).min))\n",
    "        mask.triu_(1)  # zero out the lower diagonal\n",
    "        mask = mask.unsqueeze(1)  # expand mask\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 1409,
     "status": "ok",
     "timestamp": 1677512218961,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "QAAhPken_ibc"
   },
   "outputs": [],
   "source": [
    "class GroupViTTextModel(GroupViTPreTrainedModel):\n",
    "    config_class = GroupViTTextConfig\n",
    "\n",
    "    def __init__(self, config: GroupViTTextConfig):\n",
    "        super().__init__(config)\n",
    "        self.text_model = GroupViTTextTransformer(config)\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self) -> nn.Module:\n",
    "        return self.text_model.embeddings.token_embedding\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.text_model.embeddings.token_embedding = value\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(GROUPVIT_TEXT_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=GroupViTTextConfig)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import CLIPTokenizer, GroupViTTextModel\n",
    "\n",
    "        >>> tokenizer = CLIPTokenizer.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "        >>> model = GroupViTTextModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "\n",
    "        >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n",
    "\n",
    "        >>> outputs = model(**inputs)\n",
    "        >>> last_hidden_state = outputs.last_hidden_state\n",
    "        >>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n",
    "        ```\"\"\"\n",
    "        return self.text_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1677512218961,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "pNJ4HozD_ls2"
   },
   "outputs": [],
   "source": [
    "class GroupViTVisionTransformer(nn.Module):########################\n",
    "    def __init__(self, config: GroupViTVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        embed_dim = config.hidden_size\n",
    "\n",
    "        self.embeddings = GroupViTVisionEmbeddings(config)\n",
    "        self.encoder = GroupViTVisionEncoder(config)\n",
    "        self.layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(GROUPVIT_VISION_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=GroupViTVisionConfig)\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if pixel_values is None:\n",
    "            raise ValueError(\"You have to specify pixel_values\")\n",
    "\n",
    "        hidden_states = self.embeddings(pixel_values)\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            hidden_states=hidden_states,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            output_attentions=output_attentions,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        last_hidden_state = encoder_outputs[0]\n",
    "\n",
    "        # normalize the last hidden state\n",
    "        last_hidden_state = self.layernorm(last_hidden_state)\n",
    "        pooled_output = last_hidden_state.mean(dim=1)\n",
    "\n",
    "        if not return_dict:\n",
    "            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPooling(\n",
    "            last_hidden_state=last_hidden_state,\n",
    "            pooler_output=pooled_output,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1677512218961,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "Tf5zCFfp_pS0"
   },
   "outputs": [],
   "source": [
    "class GroupViTVisionModel(GroupViTPreTrainedModel):\n",
    "    config_class = GroupViTVisionConfig\n",
    "    main_input_name = \"pixel_values\"\n",
    "\n",
    "    def __init__(self, config: GroupViTVisionConfig):\n",
    "        super().__init__(config)\n",
    "        self.vision_model = GroupViTVisionTransformer(config)\n",
    "\n",
    "        self.projection_dim = 128\n",
    "        self.projection_intermediate_dim = 4096\n",
    "        self.vision_embed_dim = config.hidden_size\n",
    "\n",
    "        self.visual_projection = nn.Sequential(\n",
    "            nn.Linear(self.vision_embed_dim, self.projection_intermediate_dim, bias=True),\n",
    "            nn.BatchNorm1d(self.projection_intermediate_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.projection_intermediate_dim, self.projection_dim, bias=True),\n",
    "        )\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self) -> GroupViTPatchEmbeddings:\n",
    "        return self.vision_model.embeddings.patch_embeddings\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(GROUPVIT_VISION_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=GroupViTVisionConfig)\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from PIL import Image\n",
    "        >>> import requests\n",
    "        >>> from transformers import AutoProcessor, GroupViTVisionModel\n",
    "\n",
    "        >>> processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "        >>> model = GroupViTVisionModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "\n",
    "        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "        >>> image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "        >>> inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "        >>> outputs = model(**inputs)\n",
    "        >>> last_hidden_state = outputs.last_hidden_state\n",
    "        >>> pooled_output = outputs.pooler_output  # pooled CLS states\n",
    "        ```\"\"\"\n",
    "\n",
    "        # print(type(pixel_values), type(output_attentions), type(output_hidden_states), type(return_dict))\n",
    "        # print(pixel_values.shape, output_attentions, output_hidden_states, return_dict)\n",
    "        # print('pixel_values=', pixel_values.shape)\n",
    "        output_attentions = True\n",
    "        output_hidden_states = False\n",
    "        return_dict = True\n",
    "        # print(pixel_values.shape, output_attentions, output_hidden_states, return_dict)\n",
    "        vision_outputs = self.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        # print(vision_outputs)\n",
    "\n",
    "        attentions = vision_outputs[2]\n",
    "            \n",
    "        # [batch_size_image, num_group, height, width]\n",
    "        grouping = get_grouping_from_attentions(attentions, pixel_values.shape[2:])\n",
    "        seg_logits = grouping\n",
    "\n",
    "        pooled_output = vision_outputs[1]  # pooled_output\n",
    "        image_features = self.visual_projection(pooled_output)\n",
    "\n",
    "        # print(image_features.shape)\n",
    "        return vision_outputs, seg_logits, image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1677512218962,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "Q3lIeXoH_slB"
   },
   "outputs": [],
   "source": [
    "@add_start_docstrings(GROUPVIT_START_DOCSTRING)\n",
    "class GroupViTModel(GroupViTPreTrainedModel):\n",
    "    config_class = GroupViTConfig\n",
    "\n",
    "    def __init__(self, config: GroupViTConfig):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # if not isinstance(config.text_config, GroupViTTextConfig):\n",
    "        #     raise ValueError(\n",
    "        #         \"config.text_config is expected to be of type GroupViTTextConfig but is of type\"\n",
    "        #         f\" {type(config.text_config)}.\"\n",
    "        #     )\n",
    "\n",
    "        if not isinstance(config.vision_config, GroupViTVisionConfig):\n",
    "            raise ValueError(\n",
    "                \"config.vision_config is expected to be of type GroupViTVisionConfig but is of type\"\n",
    "                f\" {type(config.vision_config)}.\"\n",
    "            )\n",
    "\n",
    "        # text_config = config.text_config\n",
    "        vision_config = config.vision_config\n",
    "\n",
    "        self.projection_dim = config.projection_dim\n",
    "        self.projection_intermediate_dim = config.projection_intermediate_dim\n",
    "        # self.text_embed_dim = text_config.hidden_size\n",
    "        self.vision_embed_dim = vision_config.hidden_size\n",
    "        print('hidden_size', vision_config.hidden_size)\n",
    "\n",
    "        # self.text_model = GroupViTTextTransformer(text_config)\n",
    "        self.vision_model = GroupViTVisionTransformer(vision_config)\n",
    "\n",
    "        self.visual_projection = nn.Sequential(\n",
    "            nn.Linear(self.vision_embed_dim, self.projection_intermediate_dim, bias=True),\n",
    "            nn.BatchNorm1d(self.projection_intermediate_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.projection_intermediate_dim, self.projection_dim, bias=True),\n",
    "        )\n",
    "        # self.text_projection = nn.Sequential(\n",
    "        #     nn.Linear(self.text_embed_dim, self.projection_intermediate_dim, bias=True),\n",
    "        #     nn.BatchNorm1d(self.projection_intermediate_dim),\n",
    "        #     nn.ReLU(inplace=True),\n",
    "        #     nn.Linear(self.projection_intermediate_dim, self.projection_dim, bias=True),\n",
    "        # )\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * self.config.logit_scale_init_value)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    # @add_start_docstrings_to_model_forward(GROUPVIT_TEXT_INPUTS_DOCSTRING)\n",
    "    # def get_text_features(\n",
    "    #     self,\n",
    "    #     input_ids: Optional[torch.Tensor] = None,\n",
    "    #     attention_mask: Optional[torch.Tensor] = None,\n",
    "    #     position_ids: Optional[torch.Tensor] = None,\n",
    "    #     output_attentions: Optional[bool] = None,\n",
    "    #     output_hidden_states: Optional[bool] = None,\n",
    "    #     return_dict: Optional[bool] = None,\n",
    "    # ) -> torch.FloatTensor:\n",
    "    #     r\"\"\"\n",
    "    #     Returns:\n",
    "    #         text_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The text embeddings obtained by\n",
    "    #         applying the projection layer to the pooled output of [`GroupViTTextModel`].\n",
    "\n",
    "    #     Examples:\n",
    "\n",
    "    #     ```python\n",
    "    #     >>> from transformers import CLIPTokenizer, GroupViTModel\n",
    "\n",
    "    #     >>> model = GroupViTModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "    #     >>> tokenizer = CLIPTokenizer.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "\n",
    "    #     >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n",
    "    #     >>> text_features = model.get_text_features(**inputs)\n",
    "    #     ```\"\"\"\n",
    "    #     # Use GROUPVIT model's config for some fields (if specified) instead of those of vision & text components.\n",
    "    #     output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "    #     output_hidden_states = (\n",
    "    #         output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "    #     )\n",
    "    #     return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "    #     text_outputs = self.text_model(\n",
    "    #         input_ids=input_ids,\n",
    "    #         attention_mask=attention_mask,\n",
    "    #         position_ids=position_ids,\n",
    "    #         output_attentions=output_attentions,\n",
    "    #         output_hidden_states=output_hidden_states,\n",
    "    #         return_dict=return_dict,\n",
    "    #     )\n",
    "\n",
    "    #     pooled_output = text_outputs[1]\n",
    "    #     text_features = self.text_projection(pooled_output)\n",
    "\n",
    "    #     return text_features\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(GROUPVIT_VISION_INPUTS_DOCSTRING)\n",
    "    def get_image_features(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> torch.FloatTensor:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "            image_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The image embeddings obtained by\n",
    "            applying the projection layer to the pooled output of [`GroupViTVisionModel`].\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from PIL import Image\n",
    "        >>> import requests\n",
    "        >>> from transformers import AutoProcessor, GroupViTModel\n",
    "\n",
    "        >>> model = GroupViTModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "        >>> processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "\n",
    "        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "        >>> image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "        >>> inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "        >>> image_features = model.get_image_features(**inputs)\n",
    "        ```\"\"\"\n",
    "        # Use GROUPVIT model's config for some fields (if specified) instead of those of vision & text components.\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        print(pixel_values.shape, output_attentions, output_hidden_states, return_dict)\n",
    "\n",
    "        vision_outputs = self.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = vision_outputs[1]  # pooled_output\n",
    "        print('01 ', pooled_output.shape)\n",
    "\n",
    "        image_features = self.visual_projection(pooled_output)\n",
    "        print('02 ', image_features.shape)\n",
    "\n",
    "        return image_features\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(GROUPVIT_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=GroupViTModelOutput, config_class=GroupViTConfig)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        return_loss: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_segmentation: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, GroupViTModelOutput]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from PIL import Image\n",
    "        >>> import requests\n",
    "        >>> from transformers import AutoProcessor, GroupViTModel\n",
    "\n",
    "        >>> model = GroupViTModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "        >>> processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "\n",
    "        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "        >>> image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "        >>> inputs = processor(\n",
    "        ...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True\n",
    "        ... )\n",
    "\n",
    "        >>> outputs = model(**inputs)\n",
    "        >>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "        >>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n",
    "        ```\"\"\"\n",
    "        # Use GROUPVIT model's config for some fields (if specified) instead of those of vision & text components.\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_segmentation = (\n",
    "            output_segmentation if output_segmentation is not None else self.config.output_segmentation\n",
    "        )\n",
    "        if output_segmentation:\n",
    "            output_attentions = True\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # print(type(pixel_values), type(output_attentions), type(output_hidden_states), type(return_dict))\n",
    "        print(pixel_values.shape, output_attentions, output_hidden_states, return_dict)\n",
    "\n",
    "        vision_outputs = self.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        # print(vision_outputs)\n",
    "\n",
    "        # text_outputs = self.text_model(\n",
    "        #     input_ids=input_ids,\n",
    "        #     attention_mask=attention_mask,\n",
    "        #     position_ids=position_ids,\n",
    "        #     output_attentions=output_attentions,\n",
    "        #     output_hidden_states=output_hidden_states,\n",
    "        #     return_dict=return_dict,\n",
    "        # )\n",
    "\n",
    "        image_embeds = vision_outputs[1]\n",
    "        image_embeds = self.visual_projection(image_embeds)\n",
    "\n",
    "        # text_embeds = text_outputs[1]\n",
    "        # text_embeds = self.text_projection(text_embeds)\n",
    "\n",
    "        # normalized features\n",
    "        image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n",
    "        # text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        # logit_scale = self.logit_scale.exp()\n",
    "        # logits_per_text = torch.matmul(text_embeds, image_embeds.t()) * logit_scale\n",
    "        # logits_per_image = logits_per_text.t()\n",
    "\n",
    "        seg_logits = None\n",
    "        if output_segmentation:\n",
    "            # grouped features\n",
    "            # [batch_size_image, num_group, hidden_size]\n",
    "            image_group_embeds = vision_outputs[0]\n",
    "            print('image_group_embeds_01', image_group_embeds.shape, type(image_group_embeds)) # torch.Size([1, 8, 384]) <class 'torch.Tensor'>\n",
    "\n",
    "            # [batch_size_image*num_group, hidden_size]\n",
    "            image_group_embeds = self.visual_projection(image_group_embeds.reshape(-1, image_group_embeds.shape[-1]))\n",
    "            print('image_group_embeds_02', image_group_embeds.shape, type(image_group_embeds)) # torch.Size([8, 256]) <class 'torch.Tensor'>\n",
    "\n",
    "            if output_hidden_states:\n",
    "                attentions = vision_outputs[3]\n",
    "                print('attentions_01', attentions.shape, type(attentions)) # *\n",
    "\n",
    "            else:\n",
    "                attentions = vision_outputs[2]\n",
    "                print('attentions_02', attentions[0].shape, type(attentions[0]), attentions[1].shape, type(attentions[1])) # torch.Size([1, 64, 196]) torch.Size([1, 8, 64]) <class 'torch.Tensor'>\n",
    "                \n",
    "            # [batch_size_image, num_group, height, width]\n",
    "            grouping = get_grouping_from_attentions(attentions, pixel_values.shape[2:])\n",
    "            print(pixel_values.shape)\n",
    "            print(pixel_values.shape[2:])\n",
    "            print('grouping_01', grouping.shape, type(grouping)) # torch.Size([1, 8, 224, 224]) <class 'torch.Tensor'>\n",
    "            seg_logits = grouping\n",
    "\n",
    "            # # normalized features\n",
    "            # image_group_embeds = image_group_embeds / image_group_embeds.norm(dim=-1, keepdim=True)\n",
    "            # print('image_group_embeds_03', image_group_embeds.shape, type(image_group_embeds)) # torch.Size([8, 256]) <class 'torch.Tensor'>\n",
    "\n",
    "            # # [batch_size_image x num_group, batch_size_text]\n",
    "            # logits_per_image_group = torch.matmul(image_group_embeds, text_embeds.t()) * logit_scale\n",
    "            # print('logits_per_image_group_01', logits_per_image_group.shape, type(logits_per_image_group)) # torch.Size([8, 3]) <class 'torch.Tensor'>\n",
    "\n",
    "            # # [batch_size_image, batch_size_text, num_group]\n",
    "            # logits_per_image_group = logits_per_image_group.reshape(\n",
    "            #     image_embeds.shape[0], -1, text_embeds.shape[0]\n",
    "            # ).permute(0, 2, 1)\n",
    "            # print('logits_per_image_group_02', logits_per_image_group.shape, type(logits_per_image_group)) # torch.Size([1, 3, 8]) <class 'torch.Tensor'>\n",
    "\n",
    "\n",
    "            # # [batch_size_image, batch_size_text, height x width]\n",
    "            # flatten_grouping = grouping.reshape(grouping.shape[0], grouping.shape[1], -1)\n",
    "            # print('flatten_grouping_01', flatten_grouping.shape, type(flatten_grouping)) # torch.Size([1, 8, 50176]) <class 'torch.Tensor'>\n",
    "\n",
    "\n",
    "            # # [batch_size_image, batch_size_text, height, width]\n",
    "            # seg_logits = torch.matmul(logits_per_image_group, flatten_grouping) * logit_scale\n",
    "            # print('seg_logits_01', seg_logits.shape, type(seg_logits)) # torch.Size([1, 3, 50176]) <class 'torch.Tensor'>\n",
    "\n",
    "            # seg_logits = seg_logits.reshape(\n",
    "            #     seg_logits.shape[0], seg_logits.shape[1], grouping.shape[2], grouping.shape[3]\n",
    "            # )\n",
    "            # print('seg_logits_02', seg_logits.shape, type(seg_logits)) # torch.Size([1, 3, 224, 224]) <class 'torch.Tensor'>\n",
    "\n",
    "        loss = None\n",
    "        if return_loss:\n",
    "            loss = groupvit_loss(logits_per_text)\n",
    "\n",
    "        if not return_dict:\n",
    "            if seg_logits is not None:\n",
    "                output = (\n",
    "                    logits_per_image,\n",
    "                    logits_per_text,\n",
    "                    seg_logits,\n",
    "                    text_embeds,\n",
    "                    image_embeds,\n",
    "                    text_outputs,\n",
    "                    vision_outputs,\n",
    "                )\n",
    "            else:\n",
    "                output = (logits_per_image, logits_per_text, text_embeds, image_embeds, text_outputs, vision_outputs)\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return GroupViTModelOutput(\n",
    "            loss=loss,\n",
    "            # logits_per_image=logits_per_image,\n",
    "            # logits_per_text=logits_per_text,\n",
    "            segmentation_logits=seg_logits,\n",
    "            # text_embeds=text_embeds,\n",
    "            image_embeds=image_embeds,\n",
    "            # text_model_output=text_outputs,\n",
    "            vision_model_output=vision_outputs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuwerq7N9s5S"
   },
   "source": [
    "### Set Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1677512218962,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "dvuxcmejkKt8",
    "outputId": "4e6311d9-6fd0-43ed-c40a-e5541e35f105"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(arch='resnet18', batch_size=512, bn_splits=8, cos=True, epochs=200, finetune_data_dir='/home/fine-tune_set/siim-acr-pneumothorax', knn_k=200, knn_t=0.1, lr=0.06, moco_dim=128, moco_k=4096, moco_m=0.99, moco_t=0.1, pretrain_data_dir='/home/unlabel_pre-training_set', results_dir='/home/230312_v0.1.02023-03-12-16-13-50-moco', resume='', schedule=[], seed=1, symmetric=False, wd=0.0005)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Train MoCo on CIFAR-10')\n",
    "\n",
    "parser.add_argument('-a', '--arch', default='resnet18')\n",
    "\n",
    "parser.add_argument('--finetune_data_dir', default=os.path.join(ROOT_PATH, 'fine-tune_set', 'siim-acr-pneumothorax'))\n",
    "parser.add_argument('--pretrain_data_dir', default=os.path.join(ROOT_PATH, 'unlabel_pre-training_set'))\n",
    "parser.add_argument('--seed', default=1)\n",
    "\n",
    "# lr: 0.06 for batch 512 (or 0.03 for batch 256)\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.06, type=float, metavar='LR', help='initial learning rate', dest='lr')\n",
    "parser.add_argument('--epochs', default=200, type=int, metavar='N', help='number of total epochs to run')\n",
    "parser.add_argument('--schedule', default=[120, 160], nargs='*', type=int, help='learning rate schedule (when to drop lr by 10x); does not take effect if --cos is on')\n",
    "parser.add_argument('--cos', action='store_true', help='use cosine lr schedule')\n",
    "\n",
    "parser.add_argument('--batch-size', default=512, type=int, metavar='N', help='mini-batch size')\n",
    "parser.add_argument('--wd', default=5e-4, type=float, metavar='W', help='weight decay')\n",
    "\n",
    "# moco specific configs:\n",
    "parser.add_argument('--moco-dim', default=128, type=int, help='feature dimension')\n",
    "parser.add_argument('--moco-k', default=4096, type=int, help='queue size; number of negative keys')\n",
    "parser.add_argument('--moco-m', default=0.99, type=float, help='moco momentum of updating key encoder')\n",
    "parser.add_argument('--moco-t', default=0.1, type=float, help='softmax temperature')\n",
    "\n",
    "parser.add_argument('--bn-splits', default=8, type=int, help='simulate multi-gpu behavior of BatchNorm in one gpu; 1 is SyncBatchNorm in multi-gpu')\n",
    "\n",
    "parser.add_argument('--symmetric', action='store_true', help='use a symmetric loss function that backprops to both crops')\n",
    "\n",
    "# knn monitor\n",
    "parser.add_argument('--knn-k', default=200, type=int, help='k in kNN monitor')\n",
    "parser.add_argument('--knn-t', default=0.1, type=float, help='softmax temperature in kNN monitor; could be different with moco-t')\n",
    "\n",
    "# utils\n",
    "parser.add_argument('--resume', default='', type=str, metavar='PATH', help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--results-dir', default='', type=str, metavar='PATH', help='path to cache (default: none)')\n",
    "\n",
    "'''\n",
    "args = parser.parse_args()  # running in command line\n",
    "'''\n",
    "args = parser.parse_args('')  # running in ipynb\n",
    "\n",
    "# set command line arguments here when running in ipynb\n",
    "args.epochs = 200\n",
    "args.cos = True\n",
    "args.schedule = []  # cos in use\n",
    "args.symmetric = False\n",
    "if args.results_dir == '':\n",
    "    args.results_dir = root_folder + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S-moco\")\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygQeHtsngrC8"
   },
   "source": [
    "### Dataset Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision.transforms as T\n",
    "# from transformers import AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def processor(tensor):\n",
    "#     processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "#     transform = T.ToPILImage()\n",
    "\n",
    "#     img = transform(tensor)\n",
    "#     inputs = processor(images=img, return_tensors=\"pt\")\n",
    "#     pixel_values = np.array(inputs[\"pixel_values\"])\n",
    "#     pixel_values = pixel_values.squeeze()\n",
    "#     pixel_values = torch.tensor(pixel_values)\n",
    "\n",
    "#     return pixel_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get from path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1677512218962,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "PvfVDZ8cdNqR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4999\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/unlabel_pre-training_set/images_001/imag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/unlabel_pre-training_set/images_001/imag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/unlabel_pre-training_set/images_001/imag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/unlabel_pre-training_set/images_001/imag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/unlabel_pre-training_set/images_001/imag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              images\n",
       "0  /home/unlabel_pre-training_set/images_001/imag...\n",
       "1  /home/unlabel_pre-training_set/images_001/imag...\n",
       "2  /home/unlabel_pre-training_set/images_001/imag...\n",
       "3  /home/unlabel_pre-training_set/images_001/imag...\n",
       "4  /home/unlabel_pre-training_set/images_001/imag..."
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = []\n",
    "i = 0\n",
    "\n",
    "for get_folder in os.listdir(args.pretrain_data_dir):\n",
    "    if get_folder.split('_')[0] == 'images':\n",
    "        for get_png in os.listdir(os.path.join(args.pretrain_data_dir, get_folder, 'images')):\n",
    "            images += [os.path.join(args.pretrain_data_dir, get_folder, 'images', get_png)]\n",
    "            i = i+1\n",
    "\n",
    "PathDF = pd.DataFrame({'images': images})\n",
    "print(i)\n",
    "PathDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_example(idx):\n",
    "    img = Image.open(PathDF['images'].iloc[idx])\n",
    "    print(np.array(img.convert('RGB')).shape)\n",
    "    \n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 1024, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_example(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set dataset & loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CXR_Dataset_pair(Dataset):\n",
    "    def __init__(self, path_df, transform=None):\n",
    "\n",
    "        self.path_df = path_df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.path_df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.transform is not None:\n",
    "            im_1 = self.transform(Image.open(self.path_df.iloc[idx]['images']).convert('RGB'))\n",
    "            im_2 = self.transform(Image.open(self.path_df.iloc[idx]['images']).convert('RGB'))\n",
    "        \n",
    "        return im_1, im_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179,
     "referenced_widgets": [
      "17032952ca804b558931b93c0fde1540",
      "cc091016bb8d423f80dddca00096ba0b",
      "6bbbd0d7f0f749939610ccc1aa47bfec",
      "eb9b54187a9b4404b93ce96cb5297a1e",
      "48854cfeccd84183a1819d703353a4fa",
      "7cdb20bd656249fe85674962d2a4ba7e",
      "20faf3fda02c4257b5ac68099de45581",
      "a4d8fcd96c2b44c1a21e6a0cf46d736d",
      "5bffaeb6f389499c958f4a12dca192e4",
      "d9070ee51e9c4ebe868777345a25a62d",
      "cf90228e48af46809d35402d94eb568e"
     ]
    },
    "executionInfo": {
     "elapsed": 23888,
     "status": "ok",
     "timestamp": 1677512242845,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "AoliFX5AnBJ0",
    "outputId": "978c2de8-f216-4ff2-b94f-ae3d2c15be6a"
   },
   "outputs": [],
   "source": [
    "train_data = CXR_Dataset_pair(PathDF, transform=train_transform)\n",
    "train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True, num_workers=0, pin_memory=True, drop_last=True)\n",
    "\n",
    "# memory_data = MRI_Dataset(train_df, transform=test_transform)\n",
    "# memory_loader = DataLoader(memory_data, batch_size=Config.valid_batch, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# test_data = MRI_Dataset(test_df, transform=test_transform)\n",
    "# test_loader = DataLoader(test_data, batch_size=Config.test_batch, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsAVAtRoiBbG"
   },
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define base encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SplitBatchNorm: simulate multi-gpu behavior of BatchNorm in one gpu by splitting alone the batch dimension\n",
    "# implementation adapted from https://github.com/davidcpage/cifar10-fast/blob/master/torch_backend.py\n",
    "# class SplitBatchNorm(nn.BatchNorm2d):\n",
    "#     def __init__(self, num_features, num_splits, **kw):\n",
    "#         super().__init__(num_features, **kw)\n",
    "#         self.num_splits = num_splits\n",
    "        \n",
    "#     def forward(self, input):\n",
    "#         N, C, H, W = input.shape\n",
    "#         if self.training or not self.track_running_stats:\n",
    "#             running_mean_split = self.running_mean.repeat(self.num_splits)\n",
    "#             running_var_split = self.running_var.repeat(self.num_splits)\n",
    "#             outcome = nn.functional.batch_norm(\n",
    "#                 input.view(-1, C * self.num_splits, H, W), running_mean_split, running_var_split, \n",
    "#                 self.weight.repeat(self.num_splits), self.bias.repeat(self.num_splits),\n",
    "#                 True, self.momentum, self.eps).view(N, C, H, W)\n",
    "#             self.running_mean.data.copy_(running_mean_split.view(self.num_splits, C).mean(dim=0))\n",
    "#             self.running_var.data.copy_(running_var_split.view(self.num_splits, C).mean(dim=0))\n",
    "#             return outcome\n",
    "#         else:\n",
    "#             return nn.functional.batch_norm(\n",
    "#                 input, self.running_mean, self.running_var, \n",
    "#                 self.weight, self.bias, False, self.momentum, self.eps)\n",
    "\n",
    "# class ModelBase(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Common CIFAR ResNet recipe.\n",
    "#     Comparing with ImageNet ResNet recipe, it:\n",
    "#     (i) replaces conv1 with kernel=3, str=1\n",
    "#     (ii) removes pool1\n",
    "#     \"\"\"\n",
    "#     def __init__(self, feature_dim=128, arch=None, bn_splits=16):\n",
    "#         super(ModelBase, self).__init__()\n",
    "\n",
    "#         # use split batchnorm\n",
    "#         norm_layer = partial(SplitBatchNorm, num_splits=bn_splits) if bn_splits > 1 else nn.BatchNorm2d\n",
    "#         resnet_arch = getattr(resnet, arch)\n",
    "#         net = resnet_arch(num_classes=feature_dim, norm_layer=norm_layer)\n",
    "\n",
    "#         self.net = []\n",
    "#         for name, module in net.named_children():\n",
    "#             if name == 'conv1':\n",
    "#                 module = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#             if isinstance(module, nn.MaxPool2d):\n",
    "#                 continue\n",
    "#             if isinstance(module, nn.Linear):\n",
    "#                 self.net.append(nn.Flatten(1))\n",
    "#             self.net.append(module)\n",
    "\n",
    "#         self.net = nn.Sequential(*self.net)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.net(x)\n",
    "#         # note: not normalized here\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate embedding for images\n",
    "# def embed_images(images, processor, model):\n",
    "#     inputs = processor(images=images)\n",
    "#     pixel_values = torch.tensor(np.array(inputs[\"pixel_values\"]))\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         embeddings = model.get_image_features(pixel_values=pixel_values)\n",
    "#     return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBase(nn.Module):\n",
    "    def __init__(self, feature_dim=128, arch=None, bn_splits=16):\n",
    "        super(ModelBase, self).__init__()\n",
    "\n",
    "        self.model = GroupViTVisionModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\", ignore_mismatched_sizes=True)\n",
    "\n",
    "    def forward(self, x, segmap=False):\n",
    "        vision_outputs, logits, image_features= self.model(x)\n",
    "\n",
    "        if segmap:\n",
    "            return image_features, logits\n",
    "        else:\n",
    "            return image_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define MoCo wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = torch.randn((1,3,244,244))\n",
    "# t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(t).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nvidia/groupvit-gcc-yfcc were not used when initializing GroupViTVisionModel: ['text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_projection.1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_projection.3.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_projection.3.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.final_layer_norm.weight', 'text_projection.1.running_mean', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_projection.0.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_projection.1.running_var', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_projection.1.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_projection.1.num_batches_tracked', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'logit_scale', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_projection.0.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.1.layer_norm1.weight']\n",
      "- This IS expected if you are initializing GroupViTVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GroupViTVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GroupViTVisionModel were not initialized from the model checkpoint at nvidia/groupvit-gcc-yfcc and are newly initialized because the shapes did not match:\n",
      "- visual_projection.3.weight: found shape torch.Size([256, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- visual_projection.3.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at nvidia/groupvit-gcc-yfcc were not used when initializing GroupViTVisionModel: ['text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_projection.1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_projection.3.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_projection.3.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.final_layer_norm.weight', 'text_projection.1.running_mean', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_projection.0.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_projection.1.running_var', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_projection.1.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_projection.1.num_batches_tracked', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'logit_scale', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_projection.0.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.1.layer_norm1.weight']\n",
      "- This IS expected if you are initializing GroupViTVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GroupViTVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GroupViTVisionModel were not initialized from the model checkpoint at nvidia/groupvit-gcc-yfcc and are newly initialized because the shapes did not match:\n",
      "- visual_projection.3.weight: found shape torch.Size([256, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- visual_projection.3.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelBase(\n",
      "  (model): GroupViTVisionModel(\n",
      "    (vision_model): GroupViTVisionTransformer(\n",
      "      (embeddings): GroupViTVisionEmbeddings(\n",
      "        (patch_embeddings): GroupViTPatchEmbeddings(\n",
      "          (projection): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (encoder): GroupViTVisionEncoder(\n",
      "        (stages): ModuleList(\n",
      "          (0): GroupViTStage(\n",
      "            (layers): ModuleList(\n",
      "              (0): GroupViTEncoderLayer(\n",
      "                (self_attn): GroupViTAttention(\n",
      "                  (k_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                )\n",
      "                (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): GroupViTMLP(\n",
      "                  (activation_fn): GELUActivation()\n",
      "                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                )\n",
      "                (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "              (1): GroupViTEncoderLayer(\n",
      "                (self_attn): GroupViTAttention(\n",
      "                  (k_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                )\n",
      "                (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): GroupViTMLP(\n",
      "                  (activation_fn): GELUActivation()\n",
      "                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                )\n",
      "                (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "              (2): GroupViTEncoderLayer(\n",
      "                (self_attn): GroupViTAttention(\n",
      "                  (k_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                )\n",
      "                (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): GroupViTMLP(\n",
      "                  (activation_fn): GELUActivation()\n",
      "                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                )\n",
      "                (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "              (3): GroupViTEncoderLayer(\n",
      "                (self_attn): GroupViTAttention(\n",
      "                  (k_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                )\n",
      "                (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): GroupViTMLP(\n",
      "                  (activation_fn): GELUActivation()\n",
      "                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                )\n",
      "                (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "              (4): GroupViTEncoderLayer(\n",
      "                (self_attn): GroupViTAttention(\n",
      "                  (k_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                )\n",
      "                (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): GroupViTMLP(\n",
      "                  (activation_fn): GELUActivation()\n",
      "                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                )\n",
      "                (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "              (5): GroupViTEncoderLayer(\n",
      "                (self_attn): GroupViTAttention(\n",
      "                  (k_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                )\n",
      "                (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): GroupViTMLP(\n",
      "                  (activation_fn): GELUActivation()\n",
      "                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                )\n",
      "                (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (downsample): GroupViTTokenAssign(\n",
      "              (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp_inter): GroupViTMixerMLP(\n",
      "                (activation_fn): GELUActivation()\n",
      "                (fc1): Linear(in_features=64, out_features=192, bias=True)\n",
      "                (fc2): Linear(in_features=192, out_features=64, bias=True)\n",
      "              )\n",
      "              (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (pre_assign_attn): GroupViTCrossAttentionLayer(\n",
      "                (attn): GroupViTAttention(\n",
      "                  (k_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                )\n",
      "                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): GroupViTMLP(\n",
      "                  (activation_fn): GELUActivation()\n",
      "                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                )\n",
      "                (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "              (assign): GroupViTAssignAttention(\n",
      "                (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (k_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              )\n",
      "              (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp_channels): GroupViTMLP(\n",
      "                (activation_fn): GELUActivation()\n",
      "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): GroupViTStage(\n",
      "            (layers): ModuleList(\n",
      "              (0): GroupViTEncoderLayer(\n",
      "                (self_attn): GroupViTAttention(\n",
      "                  (k_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                )\n",
      "                (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): GroupViTMLP(\n",
      "                  (activation_fn): GELUActivation()\n",
      "                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                )\n",
      "                (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "              (1): GroupViTEncoderLayer(\n",
      "                (self_attn): GroupViTAttention(\n",
      "                  (k_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                )\n",
      "                (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): GroupViTMLP(\n",
      "                  (activation_fn): GELUActivation()\n",
      "                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                )\n",
      "                (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "              (2): GroupViTEncoderLayer(\n",
      "                (self_attn): GroupViTAttention(\n",
      "                  (k_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                )\n",
      "                (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): GroupViTMLP(\n",
      "                  (activation_fn): GELUActivation()\n",
      "                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                )\n",
      "                (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (downsample): GroupViTTokenAssign(\n",
      "              (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp_inter): GroupViTMixerMLP(\n",
      "                (activation_fn): GELUActivation()\n",
      "                (fc1): Linear(in_features=8, out_features=192, bias=True)\n",
      "                (fc2): Linear(in_features=192, out_features=8, bias=True)\n",
      "              )\n",
      "              (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (pre_assign_attn): GroupViTCrossAttentionLayer(\n",
      "                (attn): GroupViTAttention(\n",
      "                  (k_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                )\n",
      "                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): GroupViTMLP(\n",
      "                  (activation_fn): GELUActivation()\n",
      "                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                )\n",
      "                (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "              (assign): GroupViTAssignAttention(\n",
      "                (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (k_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              )\n",
      "              (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp_channels): GroupViTMLP(\n",
      "                (activation_fn): GELUActivation()\n",
      "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (group_projector): Sequential(\n",
      "              (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (1): GroupViTMixerMLP(\n",
      "                (activation_fn): GELUActivation()\n",
      "                (fc1): Linear(in_features=64, out_features=192, bias=True)\n",
      "                (fc2): Linear(in_features=192, out_features=8, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): GroupViTStage(\n",
      "            (layers): ModuleList(\n",
      "              (0): GroupViTEncoderLayer(\n",
      "                (self_attn): GroupViTAttention(\n",
      "                  (k_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                )\n",
      "                (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): GroupViTMLP(\n",
      "                  (activation_fn): GELUActivation()\n",
      "                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                )\n",
      "                (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "              (1): GroupViTEncoderLayer(\n",
      "                (self_attn): GroupViTAttention(\n",
      "                  (k_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                )\n",
      "                (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): GroupViTMLP(\n",
      "                  (activation_fn): GELUActivation()\n",
      "                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                )\n",
      "                (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "              (2): GroupViTEncoderLayer(\n",
      "                (self_attn): GroupViTAttention(\n",
      "                  (k_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                )\n",
      "                (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): GroupViTMLP(\n",
      "                  (activation_fn): GELUActivation()\n",
      "                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                )\n",
      "                (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (visual_projection): Sequential(\n",
      "      (0): Linear(in_features=384, out_features=4096, bias=True)\n",
      "      (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Linear(in_features=4096, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ModelMoCo(nn.Module):\n",
    "    def __init__(self, dim=128, K=4096, m=0.99, T=0.1, arch='resnet18', bn_splits=8, symmetric=True):\n",
    "        super(ModelMoCo, self).__init__()\n",
    "\n",
    "        self.K = K\n",
    "        self.m = m\n",
    "        self.T = T\n",
    "        self.symmetric = symmetric\n",
    "\n",
    "        # create the encoders\n",
    "        self.encoder_q = ModelBase(feature_dim=dim, arch=arch, bn_splits=bn_splits)\n",
    "        self.encoder_k = ModelBase(feature_dim=dim, arch=arch, bn_splits=bn_splits)\n",
    "\n",
    "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
    "            param_k.data.copy_(param_q.data)  # initialize\n",
    "            param_k.requires_grad = False  # not update by gradient\n",
    "\n",
    "        # create the queue\n",
    "        self.register_buffer(\"queue\", torch.randn(dim, K))\n",
    "        self.queue = nn.functional.normalize(self.queue, dim=0)\n",
    "\n",
    "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _momentum_update_key_encoder(self):\n",
    "        \"\"\"\n",
    "        Momentum update of the key encoder\n",
    "        \"\"\"\n",
    "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
    "            param_k.data = param_k.data * self.m + param_q.data * (1. - self.m)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _dequeue_and_enqueue(self, keys):\n",
    "        batch_size = keys.shape[0]\n",
    "\n",
    "        ptr = int(self.queue_ptr)\n",
    "        assert self.K % batch_size == 0  # for simplicity\n",
    "\n",
    "        # replace the keys at ptr (dequeue and enqueue)\n",
    "        self.queue[:, ptr:ptr + batch_size] = keys.t()  # transpose\n",
    "        ptr = (ptr + batch_size) % self.K  # move pointer\n",
    "\n",
    "        self.queue_ptr[0] = ptr\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _batch_shuffle_single_gpu(self, x):\n",
    "        \"\"\"\n",
    "        Batch shuffle, for making use of BatchNorm.\n",
    "        \"\"\"\n",
    "        # random shuffle index\n",
    "        idx_shuffle = torch.randperm(x.shape[0]).cuda()\n",
    "\n",
    "        # index for restoring\n",
    "        idx_unshuffle = torch.argsort(idx_shuffle)\n",
    "\n",
    "        return x[idx_shuffle], idx_unshuffle\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _batch_unshuffle_single_gpu(self, x, idx_unshuffle):\n",
    "        \"\"\"\n",
    "        Undo batch shuffle.\n",
    "        \"\"\"\n",
    "        return x[idx_unshuffle]\n",
    "\n",
    "    def contrastive_loss(self, im_q, im_k):\n",
    "        # compute query features\n",
    "        q = self.encoder_q(im_q)  # queries: NxC\n",
    "        q = nn.functional.normalize(q, dim=1)  # already normalized\n",
    "\n",
    "        # compute key features\n",
    "        with torch.no_grad():  # no gradient to keys\n",
    "            # shuffle for making use of BN\n",
    "            im_k_, idx_unshuffle = self._batch_shuffle_single_gpu(im_k)\n",
    "\n",
    "            k = self.encoder_k(im_k_)  # keys: NxC\n",
    "            k = nn.functional.normalize(k, dim=1)  # already normalized\n",
    "\n",
    "            # undo shuffle\n",
    "            k = self._batch_unshuffle_single_gpu(k, idx_unshuffle)\n",
    "\n",
    "        # compute logits\n",
    "        # Einstein sum is more intuitive\n",
    "        # positive logits: Nx1\n",
    "        l_pos = torch.einsum('nc,nc->n', [q, k]).unsqueeze(-1)\n",
    "        # negative logits: NxK\n",
    "        l_neg = torch.einsum('nc,ck->nk', [q, self.queue.clone().detach()])\n",
    "\n",
    "        # logits: Nx(1+K)\n",
    "        logits = torch.cat([l_pos, l_neg], dim=1)\n",
    "\n",
    "        # apply temperature\n",
    "        logits /= self.T\n",
    "\n",
    "        # labels: positive key indicators\n",
    "        labels = torch.zeros(logits.shape[0], dtype=torch.long).cuda()\n",
    "        \n",
    "        loss = nn.CrossEntropyLoss().cuda()(logits, labels)\n",
    "\n",
    "        return loss, q, k\n",
    "\n",
    "    def forward(self, im1, im2):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            im_q: a batch of query images\n",
    "            im_k: a batch of key images\n",
    "        Output:\n",
    "            loss\n",
    "        \"\"\"\n",
    "\n",
    "        # update the key encoder\n",
    "        with torch.no_grad():  # no gradient to keys\n",
    "            self._momentum_update_key_encoder()\n",
    "\n",
    "        # compute loss\n",
    "        if self.symmetric:  # asymmetric loss\n",
    "            loss_12, q1, k2 = self.contrastive_loss(im1, im2)\n",
    "            loss_21, q2, k1 = self.contrastive_loss(im2, im1)\n",
    "            loss = loss_12 + loss_21\n",
    "            k = torch.cat([k1, k2], dim=0)\n",
    "        else:  # asymmetric loss\n",
    "            loss, q, k = self.contrastive_loss(im1, im2)\n",
    "\n",
    "        self._dequeue_and_enqueue(k)\n",
    "\n",
    "        return loss\n",
    "\n",
    "# create model\n",
    "model = ModelMoCo(\n",
    "        dim=args.moco_dim,\n",
    "        K=args.moco_k,\n",
    "        m=args.moco_m,\n",
    "        T=args.moco_t,\n",
    "        arch=args.arch,\n",
    "        bn_splits=args.bn_splits,\n",
    "        symmetric=args.symmetric,\n",
    "    ).cuda()\n",
    "print(model.encoder_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "### for_multi_GPU\n",
    "model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YXcpXBwi8KV"
   },
   "source": [
    "### Define train/test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define train & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for one epoch\n",
    "def train(net, data_loader, train_optimizer, epoch, args):\n",
    "    net.train()\n",
    "    adjust_learning_rate(optimizer, epoch, args)\n",
    "\n",
    "    total_loss, total_num, train_bar = 0.0, 0, tqdm(data_loader)\n",
    "    for im_1, im_2 in train_bar:\n",
    "\n",
    "        # print(im_1.shape, type(im_1), type(im_1[0][0][0][0]), im_1[0][0][0][0], im_1)\n",
    "\n",
    "        \n",
    "        im_1, im_2 = im_1.cuda(non_blocking=True), im_2.cuda(non_blocking=True)\n",
    "\n",
    "        loss = net(im_1, im_2) # torch.Size([512, 3, 32, 32])*2\n",
    "        \n",
    "#         print(loss)\n",
    "        loss = loss.mean()\n",
    "        \n",
    "        train_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        train_optimizer.step()\n",
    "\n",
    "        total_num += data_loader.batch_size\n",
    "        total_loss += loss.item() * data_loader.batch_size\n",
    "        train_bar.set_description('Train Epoch: [{}/{}], lr: {:.6f}, Loss: {:.4f}'.format(epoch, args.epochs, optimizer.param_groups[0]['lr'], total_loss / total_num))\n",
    "\n",
    "    return total_loss / total_num\n",
    "\n",
    "# lr scheduler for training\n",
    "def adjust_learning_rate(optimizer, epoch, args):\n",
    "    \"\"\"Decay the learning rate based on schedule\"\"\"\n",
    "    lr = args.lr\n",
    "    if args.cos:  # cosine lr schedule\n",
    "        lr *= 0.5 * (1. + math.cos(math.pi * epoch / args.epochs))\n",
    "    else:  # stepwise lr schedule\n",
    "        for milestone in args.schedule:\n",
    "            lr *= 0.1 if epoch >= milestone else 1.\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test using a knn monitor\n",
    "def test(net, memory_data_loader, test_data_loader, epoch, args):\n",
    "    net.eval()\n",
    "    classes = len(memory_data_loader.dataset.classes)\n",
    "    total_top1, total_top5, total_num, feature_bank = 0.0, 0.0, 0, []\n",
    "    with torch.no_grad():\n",
    "        # generate feature bank\n",
    "        for data, target in tqdm(memory_data_loader, desc='Feature extracting'):\n",
    "            feature = net(data.cuda(non_blocking=True))\n",
    "            feature = F.normalize(feature, dim=1)\n",
    "            feature_bank.append(feature)\n",
    "        # [D, N]\n",
    "        feature_bank = torch.cat(feature_bank, dim=0).t().contiguous()\n",
    "        # [N]\n",
    "        feature_labels = torch.tensor(memory_data_loader.dataset.targets, device=feature_bank.device)\n",
    "        # loop test data to predict the label by weighted knn search\n",
    "        test_bar = tqdm(test_data_loader)\n",
    "        for data, target in test_bar:\n",
    "            data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n",
    "            feature = net(data)\n",
    "            feature = F.normalize(feature, dim=1)\n",
    "            \n",
    "            pred_labels = knn_predict(feature, feature_bank, feature_labels, classes, args.knn_k, args.knn_t)\n",
    "\n",
    "            total_num += data.size(0)\n",
    "            total_top1 += (pred_labels[:, 0] == target).float().sum().item()\n",
    "            test_bar.set_description('Test Epoch: [{}/{}] Acc@1:{:.2f}%'.format(epoch, args.epochs, total_top1 / total_num * 100))\n",
    "\n",
    "    return total_top1 / total_num * 100\n",
    "\n",
    "# knn monitor as in InstDisc https://arxiv.org/abs/1805.01978\n",
    "# implementation follows http://github.com/zhirongw/lemniscate.pytorch and https://github.com/leftthomas/SimCLR\n",
    "def knn_predict(feature, feature_bank, feature_labels, classes, knn_k, knn_t):\n",
    "    # compute cos similarity between each feature vector and feature bank ---> [B, N]\n",
    "    sim_matrix = torch.mm(feature, feature_bank)\n",
    "    # [B, K]\n",
    "    sim_weight, sim_indices = sim_matrix.topk(k=knn_k, dim=-1)\n",
    "    # [B, K]\n",
    "    sim_labels = torch.gather(feature_labels.expand(feature.size(0), -1), dim=-1, index=sim_indices)\n",
    "    sim_weight = (sim_weight / knn_t).exp()\n",
    "\n",
    "    # counts for each class\n",
    "    one_hot_label = torch.zeros(feature.size(0) * knn_k, classes, device=sim_labels.device)\n",
    "    # [B*K, C]\n",
    "    one_hot_label = one_hot_label.scatter(dim=-1, index=sim_labels.view(-1, 1), value=1.0)\n",
    "    # weighted score ---> [B, C]\n",
    "    pred_scores = torch.sum(one_hot_label.view(feature.size(0), -1, classes) * sim_weight.unsqueeze(dim=-1), dim=1)\n",
    "\n",
    "    pred_labels = pred_scores.argsort(dim=-1, descending=True)\n",
    "    return pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "def ade_palette():\n",
    "    return [[  0,   0,   0], [  0,   0, 255], [  0, 255,   0], [255,   0,   0],\n",
    "            [255, 255, 255], [255, 255,   0], [255,   0, 255], [  0, 255, 255]]\n",
    "\n",
    "def get_input_image(get_id, processor):\n",
    "    img_path = PathDF['images'].iloc[get_id]\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    return inputs[\"pixel_values\"].cuda(), image, img_path.split('/')[-1]\n",
    "\n",
    "def get_color_seg(logits, image):\n",
    "    # First, rescale logits to original image size\n",
    "    logits = nn.functional.interpolate(logits.detach().cpu(),\n",
    "                    size=image.size[::-1], # (height, width)\n",
    "                    mode='bilinear',\n",
    "                    align_corners=False)\n",
    "\n",
    "    # Second, apply argmax on the class dimension\n",
    "    seg = logits.argmax(dim=1)[0]\n",
    "    color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
    "    palette = np.array(ade_palette())\n",
    "    for label, color in enumerate(palette):\n",
    "        color_seg[seg == label, :] = color\n",
    "    # Convert to BGR\n",
    "    color_seg = color_seg[..., ::-1]\n",
    "\n",
    "    return color_seg\n",
    "\n",
    "def plot(image, color_seg, text):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(8,4))\n",
    "    ax[0].imshow(np.array(image).astype(np.uint8))\n",
    "    ax[0].set_title(\"Image\")\n",
    "    ax[1].imshow(np.array(color_seg).astype(np.uint8))\n",
    "    ax[1].set_title(\"Mask\")\n",
    "    img = np.array(image) * 0.7 + color_seg * 0.3\n",
    "    img = img.astype(np.uint8)\n",
    "    ax[2].imshow(img)\n",
    "    ax[2].set_title(text)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "\n",
    "def looker(idx, processor):\n",
    "    input, image, name = get_input_image(idx, processor)\n",
    "    _, logits = model.ModelMoCo.encoder_q(input, segmap=True)\n",
    "    color_seg = get_color_seg(logits, image)\n",
    "\n",
    "    plot(image, color_seg, 'trained')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86lHkiKox3KO"
   },
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"<ipython-input-52-27952717f5de>\", line 121, in forward\n    loss, q, k = self.contrastive_loss(im1, im2)\n  File \"<ipython-input-52-27952717f5de>\", line 67, in contrastive_loss\n    q = self.encoder_q(im_q)  # queries: NxC\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"<ipython-input-49-8b97e0fea7af>\", line 8, in forward\n    vision_outputs, logits, image_features= self.model(x)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"<ipython-input-36-9f6e10184f5d>\", line 76, in forward\n    grouping = get_grouping_from_attentions(attentions, pixel_values.shape[2:])\n  File \"<ipython-input-16-b5933bb10fb3>\", line 21, in get_grouping_from_attentions\n    cur_attn_map = resize_attention_map(prev_attn_masks.permute(0, 2, 1).contiguous(), *hw_shape)\n  File \"<ipython-input-15-9c740063e817>\", line 26, in resize_attention_map\n    attentions, size=(height, width), mode=\"bilinear\", align_corners=align_corners\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\", line 3709, in interpolate\n    return torch._C._nn.upsample_bilinear2d(input, output_size, align_corners, scale_factors)\nRuntimeError: CUDA out of memory. Tried to allocate 3.06 GiB (GPU 0; 23.70 GiB total capacity; 18.37 GiB already allocated; 1.58 GiB free; 18.50 GiB reserved in total by PyTorch)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-e34d27855784>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mlooker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-394fe3ca69cd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, data_loader, train_optimizer, epoch, args)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mim_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# torch.Size([512, 3, 32, 32])*2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#         print(loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"<ipython-input-52-27952717f5de>\", line 121, in forward\n    loss, q, k = self.contrastive_loss(im1, im2)\n  File \"<ipython-input-52-27952717f5de>\", line 67, in contrastive_loss\n    q = self.encoder_q(im_q)  # queries: NxC\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"<ipython-input-49-8b97e0fea7af>\", line 8, in forward\n    vision_outputs, logits, image_features= self.model(x)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"<ipython-input-36-9f6e10184f5d>\", line 76, in forward\n    grouping = get_grouping_from_attentions(attentions, pixel_values.shape[2:])\n  File \"<ipython-input-16-b5933bb10fb3>\", line 21, in get_grouping_from_attentions\n    cur_attn_map = resize_attention_map(prev_attn_masks.permute(0, 2, 1).contiguous(), *hw_shape)\n  File \"<ipython-input-15-9c740063e817>\", line 26, in resize_attention_map\n    attentions, size=(height, width), mode=\"bilinear\", align_corners=align_corners\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\", line 3709, in interpolate\n    return torch._C._nn.upsample_bilinear2d(input, output_size, align_corners, scale_factors)\nRuntimeError: CUDA out of memory. Tried to allocate 3.06 GiB (GPU 0; 23.70 GiB total capacity; 18.37 GiB already allocated; 1.58 GiB free; 18.50 GiB reserved in total by PyTorch)\n"
     ]
    }
   ],
   "source": [
    "# define optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.wd, momentum=0.9)\n",
    "\n",
    "# load model if resume\n",
    "epoch_start = 1\n",
    "if args.resume is not '':\n",
    "    checkpoint = torch.load(args.resume)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    epoch_start = checkpoint['epoch'] + 1\n",
    "    print('Loaded from: {}'.format(args.resume))\n",
    "\n",
    "# logging\n",
    "results = {'train_loss': [], 'test_acc@1': []}\n",
    "if not os.path.exists(args.results_dir):\n",
    "    os.mkdir(args.results_dir)\n",
    "# dump args\n",
    "with open(args.results_dir + '/args.json', 'w') as fid:\n",
    "    json.dump(args.__dict__, fid, indent=2)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(epoch_start, args.epochs + 1):\n",
    "    train_loss = train(model, train_loader, optimizer, epoch, args)\n",
    "    results['train_loss'].append(train_loss)\n",
    "    looker(5, processor)\n",
    "    # test_acc_1 = test(model.encoder_q, memory_loader, test_loader, epoch, args)\n",
    "    results['test_acc@1'].append(0.5)\n",
    "    # save statistics\n",
    "    data_frame = pd.DataFrame(data=results, index=range(epoch_start, epoch + 1))\n",
    "    data_frame.to_csv(args.results_dir + '/log.csv', index_label='epoch')\n",
    "    # save model\n",
    "    torch.save({'epoch': epoch, 'state_dict': model.state_dict(), 'optimizer' : optimizer.state_dict(),}, args.results_dir + '/model_last.pth')\n",
    "#     torch.save({'epoch': epoch, 'encoder_q_state_dict': model.encoder_q.state_dict(), 'optimizer' : optimizer.state_dict(),}, args.results_dir + '/encoder_q_model_last.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_with_pretrain_weight(model, weight_path):\n",
    "    checkpoint = torch.load(os.path.abspath(os.path.join(ROOT_PATH, weight_path, 'model_last.pth')))\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    return model.encoder_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ = get_model_with_pretrain_weight(model, '../lab_05/230307_v0.0.62023-03-07-13-03-04-moco')\n",
    "# model__ = GroupViTVisionModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\", ignore_mismatched_sizes=True)\n",
    "# processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "\n",
    "# for get_id in range(PathDF.shape[0]):\n",
    "\n",
    "#     input, image, name = get_input_image(get_id, processor)\n",
    "#     _, logits = model_(input)\n",
    "#     color_seg = get_color_seg(logits, image)\n",
    "\n",
    "#     plot(image, color_seg, 'trained')\n",
    "\n",
    "#     inputs__ = processor(images=image, return_tensors=\"pt\")\n",
    "#     _, logits__, _ = model__(**inputs__)\n",
    "#     color_seg = get_color_seg(logits__, image)\n",
    "\n",
    "#     plot(image, color_seg, 'untrain')\n",
    "\n",
    "\n",
    "# print('\\n\\nSuccessfully Completed!!!\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = []\n",
    "# i = 0\n",
    "\n",
    "# for get_folder in os.listdir(args.finetune_data_dir):\n",
    "#     if get_folder == 'png_images':\n",
    "#         for get_file in os.listdir(os.path.join(args.finetune_data_dir, 'png_images')):\n",
    "#             if get_file in os.listdir(os.path.join(args.finetune_data_dir, 'png_masks')):\n",
    "#                 images += [os.path.join(args.finetune_data_dir, 'png_images', get_file)]\n",
    "#                 i = i+1\n",
    "\n",
    "# PathDF = pd.DataFrame({'images': images})\n",
    "# print(i)\n",
    "# PathDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_example(idx):\n",
    "#     img = Image.open(PathDF['images'].iloc[idx])\n",
    "#     print(np.array(img.convert('RGB')).shape)\n",
    "    \n",
    "#     plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_example(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ = get_model_with_pretrain_weight(model, '../lab_05/230307_v0.0.62023-03-07-13-03-04-moco')\n",
    "# processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "\n",
    "# path = os.path.join(args.finetune_data_dir, 'step_set_')\n",
    "# checkpath(path)\n",
    "\n",
    "# for get_id in range(PathDF.shape[0]):\n",
    "\n",
    "#     input, image, name = get_input_image(get_id, processor)\n",
    "#     _, logits = model_(input)\n",
    "#     color_seg = get_color_seg(logits, image)\n",
    "\n",
    "#     kokonopath = os.path.join(path, 'logits_' + name)\n",
    "#     cv2.imwrite(kokonopath, np.array(color_seg).astype(np.uint8))\n",
    "#     print(kokonopath)\n",
    "\n",
    "# print('\\n\\nSuccessfully Completed!!!\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/facebookresearch/moco/blob/colab-notebook/colab/moco_cifar10_demo.ipynb",
     "timestamp": 1677256490794
    }
   ]
  },
  "environment": {
   "name": "pytorch-gpu.1-4.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06768cfdf690415e9affdf2a74e59a30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "17032952ca804b558931b93c0fde1540": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cc091016bb8d423f80dddca00096ba0b",
       "IPY_MODEL_6bbbd0d7f0f749939610ccc1aa47bfec",
       "IPY_MODEL_eb9b54187a9b4404b93ce96cb5297a1e"
      ],
      "layout": "IPY_MODEL_48854cfeccd84183a1819d703353a4fa"
     }
    },
    "1d94bafa0c204861869e3c8656f88338": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20faf3fda02c4257b5ac68099de45581": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "23596ead25e842da821eee9281ddd44b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce5a59adc10e4ad3bbd5ee949704a493",
      "placeholder": "​",
      "style": "IPY_MODEL_2e92aabde375495f880ae9752b55e057",
      "value": " 4.64k/4.64k [00:00&lt;00:00, 115kB/s]"
     }
    },
    "2e92aabde375495f880ae9752b55e057": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "31ea43544fcd44a19f86c920afaa12ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3deab35d86db420c963d21fde4f4f770": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1d94bafa0c204861869e3c8656f88338",
      "placeholder": "​",
      "style": "IPY_MODEL_06768cfdf690415e9affdf2a74e59a30",
      "value": "Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"
     }
    },
    "3e187d79da6f477180cac4b95d949388": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "48854cfeccd84183a1819d703353a4fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5528a5ca02bc4cde9fa1bad142519e35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_79cc1507b29d4125ae5e3c704b2b8b75",
      "placeholder": "​",
      "style": "IPY_MODEL_31ea43544fcd44a19f86c920afaa12ab",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "55606bf393b940278f76baf65170eeab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5bffaeb6f389499c958f4a12dca192e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5d806cd2fe7e4424b7fb90371815f4d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3deab35d86db420c963d21fde4f4f770",
       "IPY_MODEL_e3214b7f823d4a12bed90f36b6cd3bbe",
       "IPY_MODEL_6bec28962d4740aabe8e9896e00134b7"
      ],
      "layout": "IPY_MODEL_5da754b21ef34dc9a7eff14bdc3a34bc"
     }
    },
    "5da754b21ef34dc9a7eff14bdc3a34bc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6bbbd0d7f0f749939610ccc1aa47bfec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4d8fcd96c2b44c1a21e6a0cf46d736d",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5bffaeb6f389499c958f4a12dca192e4",
      "value": 170498071
     }
    },
    "6bec28962d4740aabe8e9896e00134b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d083420df95d42ecb16cedd5037dc2a3",
      "placeholder": "​",
      "style": "IPY_MODEL_55606bf393b940278f76baf65170eeab",
      "value": " 223M/223M [00:02&lt;00:00, 86.7MB/s]"
     }
    },
    "73487b45ea444f8e81410a9627c85b40": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79cc1507b29d4125ae5e3c704b2b8b75": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7cdb20bd656249fe85674962d2a4ba7e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9cf2f5d1a7894a73861ce8bd69675ed1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a4d8fcd96c2b44c1a21e6a0cf46d736d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aeabbf27ff5e4ef5a0c3de652c86d632": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5528a5ca02bc4cde9fa1bad142519e35",
       "IPY_MODEL_c3b291f042fe422aa3a2b22f1208d0ca",
       "IPY_MODEL_23596ead25e842da821eee9281ddd44b"
      ],
      "layout": "IPY_MODEL_d74ac37d81974a4780581b554f3e7d23"
     }
    },
    "c3b291f042fe422aa3a2b22f1208d0ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73487b45ea444f8e81410a9627c85b40",
      "max": 4642,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3e187d79da6f477180cac4b95d949388",
      "value": 4642
     }
    },
    "cc091016bb8d423f80dddca00096ba0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7cdb20bd656249fe85674962d2a4ba7e",
      "placeholder": "​",
      "style": "IPY_MODEL_20faf3fda02c4257b5ac68099de45581",
      "value": "100%"
     }
    },
    "ce5a59adc10e4ad3bbd5ee949704a493": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf90228e48af46809d35402d94eb568e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d083420df95d42ecb16cedd5037dc2a3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d74ac37d81974a4780581b554f3e7d23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9070ee51e9c4ebe868777345a25a62d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3214b7f823d4a12bed90f36b6cd3bbe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f48ad91e88b340258d9fbe66bc58696b",
      "max": 223137427,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9cf2f5d1a7894a73861ce8bd69675ed1",
      "value": 223137427
     }
    },
    "eb9b54187a9b4404b93ce96cb5297a1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d9070ee51e9c4ebe868777345a25a62d",
      "placeholder": "​",
      "style": "IPY_MODEL_cf90228e48af46809d35402d94eb568e",
      "value": " 170498071/170498071 [00:14&lt;00:00, 14802420.28it/s]"
     }
    },
    "f48ad91e88b340258d9fbe66bc58696b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
