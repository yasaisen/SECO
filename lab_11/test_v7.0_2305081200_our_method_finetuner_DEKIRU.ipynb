{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MXcbCW8f7Eh"
   },
   "source": [
    "### Some Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# !pip install segmentation_models_pytorch\n",
    "# !pip uninstall opencv-python\n",
    "# !pip install opencv-contrib-python\n",
    "# !pip install scikit-learn\n",
    "# !pip install albumentations\n",
    "# !pip install transformers\n",
    "# !apt-get update\n",
    "# !apt-get install ffmpeg\n",
    "# !apt-get install libsm6\n",
    "# !apt-get install libxext6\n",
    "# !apt-get install unzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Mar 25 17:29:16 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.76       Driver Version: 515.76       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:1F:00.0 Off |                  N/A |\n",
      "| 37%   69C    P0   140W / 200W |      0MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# ### for_multi_GPU\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5122,
     "status": "ok",
     "timestamp": 1677512171549,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "CvAFrPlS4bLU",
    "outputId": "2900b98f-10a0-48ae-ae6a-0aff7f787c20"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pdb\n",
    "import time\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader, Dataset, sampler\n",
    "from matplotlib import pyplot as plt\n",
    "from albumentations import (HorizontalFlip, ShiftScaleRotate, Normalize, Resize, Compose, GaussNoise)\n",
    "# from albumentations.torch import ToTensorV2\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_rtoBGhgSae"
   },
   "source": [
    "### Set Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30443,
     "status": "ok",
     "timestamp": 1677512201987,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "UKCi74HiH1A9",
    "outputId": "779d592f-9214-477e-c922-233d7845cb7c"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# ROOT_PATH = '/home/yasaisen/Desktop/11_research/11_research_main/lab_08'\n",
    "ROOT_PATH = '/home'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_submission_path = '../input/siim-acr-pneumothorax-segmentation/sample_submission.csv'\n",
    "# sample_submission_path = '/content/drive/My Drive/11_research_main/lab_01/CXR_Dataset/stage_2_sample_submission.csv'\n",
    "\n",
    "train_rle_path = os.path.join(ROOT_PATH, 'train-rle.csv')\n",
    "data_folder = os.path.join(ROOT_PATH, 'step_set_segmix_v6.0')\n",
    "test_data_folder = os.path.join(ROOT_PATH, 'test_png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1677512201988,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "Uh-pffjlIDDw"
   },
   "outputs": [],
   "source": [
    "def checkpath(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2263,
     "status": "ok",
     "timestamp": 1677512204231,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "SRDsG25wIKf-"
   },
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "Version = '230312_v0.1.0'\n",
    "\n",
    "root_folder = os.path.abspath(os.path.join(ROOT_PATH, Version))\n",
    "\n",
    "# model_DIR = os.path.abspath(os.path.join(root_folder, 'model'))\n",
    "# checkpath(root_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupVit Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2463,
     "status": "ok",
     "timestamp": 1677512217559,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "1tdXYOBI7W7Q"
   },
   "outputs": [],
   "source": [
    "import collections.abc\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "\n",
    "# from ...activations import ACT2FN\n",
    "from transformers.activations import ACT2FN\n",
    "\n",
    "# from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n",
    "from transformers.modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n",
    "# from ...modeling_utils import PreTrainedModel\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "# from ...utils import (\n",
    "#     ModelOutput,\n",
    "#     add_start_docstrings,\n",
    "#     add_start_docstrings_to_model_forward,\n",
    "#     logging,\n",
    "#     replace_return_docstrings,\n",
    "# )\n",
    "from transformers.utils import (\n",
    "    ModelOutput,\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "# from .configuration_groupvit import GroupViTConfig, GroupViTTextConfig, GroupViTVisionConfig\n",
    "from transformers.models.groupvit.configuration_groupvit import GroupViTConfig, GroupViTTextConfig #, GroupViTVisionConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from typing import TYPE_CHECKING, Any, Mapping, Optional, Union\n",
    "\n",
    "# from ...configuration_utils import PretrainedConfig\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "# from ...onnx import OnnxConfig\n",
    "from transformers.onnx import OnnxConfig\n",
    "# from ...utils import logging\n",
    "from transformers.utils import logging\n",
    "\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    # from ...processing_utils import ProcessorMixin\n",
    "    from transformers.processing_utils import ProcessorMixin\n",
    "    # from ...utils import TensorType\n",
    "    from transformers.utils import TensorType\n",
    "\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "GROUPVIT_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n",
    "    \"nvidia/groupvit-gcc-yfcc\": \"https://huggingface.co/nvidia/groupvit-gcc-yfcc/resolve/main/config.json\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupViTVisionConfig(PretrainedConfig):\n",
    "    r\"\"\"\n",
    "    This is the configuration class to store the configuration of a [`GroupViTVisionModel`]. It is used to instantiate\n",
    "    an GroupViT model according to the specified arguments, defining the model architecture. Instantiating a\n",
    "    configuration with the defaults will yield a similar configuration to that of the GroupViT\n",
    "    [nvidia/groupvit-gcc-yfcc](https://huggingface.co/nvidia/groupvit-gcc-yfcc) architecture.\n",
    "\n",
    "    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n",
    "    documentation from [`PretrainedConfig`] for more information.\n",
    "\n",
    "    Args:\n",
    "        hidden_size (`int`, *optional*, defaults to 384):\n",
    "            Dimensionality of the encoder layers and the pooler layer.\n",
    "        intermediate_size (`int`, *optional*, defaults to 1536):\n",
    "            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n",
    "        depths (`List[int]`, *optional*, defaults to [6, 3, 3]):\n",
    "            The number of layers in each encoder block.\n",
    "        num_group_tokens (`List[int]`, *optional*, defaults to [64, 8, 0]):\n",
    "            The number of group tokens for each stage.\n",
    "        num_output_groups (`List[int]`, *optional*, defaults to [64, 8, 8]):\n",
    "            The number of output groups for each stage, 0 means no group.\n",
    "        num_attention_heads (`int`, *optional*, defaults to 6):\n",
    "            Number of attention heads for each attention layer in the Transformer encoder.\n",
    "        image_size (`int`, *optional*, defaults to 224):\n",
    "            The size (resolution) of each image.\n",
    "        patch_size (`int`, *optional*, defaults to 16):\n",
    "            The size (resolution) of each patch.\n",
    "        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n",
    "            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n",
    "            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` ``\"quick_gelu\"` are supported.\n",
    "        layer_norm_eps (`float`, *optional*, defaults to 1e-5):\n",
    "            The epsilon used by the layer normalization layers.\n",
    "        dropout (`float`, *optional*, defaults to 0.0):\n",
    "            The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.\n",
    "        attention_dropout (`float`, *optional*, defaults to 0.0):\n",
    "            The dropout ratio for the attention probabilities.\n",
    "        initializer_range (`float`, *optional*, defaults to 0.02):\n",
    "            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
    "        initializer_factor (`float`, *optional*, defaults to 1.0):\n",
    "            A factor for initializing all weight matrices (should be kept to 1, used internally for initialization\n",
    "            testing).\n",
    "\n",
    "    Example:\n",
    "\n",
    "    ```python\n",
    "    >>> from transformers import GroupViTVisionConfig, GroupViTVisionModel\n",
    "\n",
    "    >>> # Initializing a GroupViTVisionModel with nvidia/groupvit-gcc-yfcc style configuration\n",
    "    >>> configuration = GroupViTVisionConfig()\n",
    "\n",
    "    >>> model = GroupViTVisionModel(configuration)\n",
    "\n",
    "    >>> # Accessing the model configuration\n",
    "    >>> configuration = model.config\n",
    "    ```\"\"\"\n",
    "\n",
    "    model_type = \"groupvit_vision_model\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=384,\n",
    "        intermediate_size=1536,\n",
    "        depths=[6, 3, 3],\n",
    "        num_hidden_layers=12,\n",
    "        num_group_tokens=[64, 8, 0],\n",
    "        num_output_groups=[64, 8, 8],\n",
    "        num_attention_heads=6,\n",
    "        image_size=1024,\n",
    "        patch_size=16,\n",
    "        num_channels=3,\n",
    "        hidden_act=\"gelu\",\n",
    "        layer_norm_eps=1e-5,\n",
    "        dropout=0.0,\n",
    "        attention_dropout=0.0,\n",
    "        initializer_range=0.02,\n",
    "        initializer_factor=1.0,\n",
    "        assign_eps=1.0,\n",
    "        assign_mlp_ratio=[0.5, 4],\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.depths = depths\n",
    "        if num_hidden_layers != sum(depths):\n",
    "            logger.warning(\n",
    "                f\"Manually setting num_hidden_layers to {num_hidden_layers}, but we expect num_hidden_layers =\"\n",
    "                f\" sum(depth) = {sum(depths)}\"\n",
    "            )\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_group_tokens = num_group_tokens\n",
    "        self.num_output_groups = num_output_groups\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_channels = num_channels\n",
    "        self.hidden_act = hidden_act\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.dropout = dropout\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.initializer_range = initializer_range\n",
    "        self.initializer_factor = initializer_factor\n",
    "        self.assign_eps = assign_eps\n",
    "        self.assign_mlp_ratio = assign_mlp_ratio\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n",
    "        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
    "\n",
    "        # get the vision config dict if we are loading from GroupViTConfig\n",
    "        if config_dict.get(\"model_type\") == \"groupvit\":\n",
    "            config_dict = config_dict[\"vision_config\"]\n",
    "\n",
    "        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n",
    "            logger.warning(\n",
    "                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n",
    "                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n",
    "            )\n",
    "\n",
    "        return cls.from_dict(config_dict, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217559,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "M97ik37v9rbD"
   },
   "outputs": [],
   "source": [
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "_CHECKPOINT_FOR_DOC = \"nvidia/groupvit-gcc-yfcc\"\n",
    "\n",
    "GROUPVIT_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
    "    \"nvidia/groupvit-gcc-yfcc\",\n",
    "    # See all GroupViT models at https://huggingface.co/models?filter=groupvit\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217560,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "TfjSCoDn9utr"
   },
   "outputs": [],
   "source": [
    "# Copied from transformers.models.bart.modeling_bart._expand_mask\n",
    "def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n",
    "    \"\"\"\n",
    "    bsz, src_len = mask.size()\n",
    "    tgt_len = tgt_len if tgt_len is not None else src_len\n",
    "\n",
    "    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
    "\n",
    "    inverted_mask = 1.0 - expanded_mask\n",
    "\n",
    "    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217560,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "6PRDe_fu9xoT"
   },
   "outputs": [],
   "source": [
    "# contrastive loss function, adapted from\n",
    "# https://sachinruk.github.io/blog/pytorch/pytorch%20lightning/loss%20function/gpu/2021/03/07/CLIP.html\n",
    "def contrastive_loss(logits: torch.Tensor) -> torch.Tensor:\n",
    "    return nn.functional.cross_entropy(logits, torch.arange(len(logits), device=logits.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217560,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "NTvFTrtG9z1z"
   },
   "outputs": [],
   "source": [
    "# Copied from transformers.models.clip.modeling_clip.clip_loss with clip->groupvit\n",
    "def groupvit_loss(similarity: torch.Tensor) -> torch.Tensor:\n",
    "    caption_loss = contrastive_loss(similarity)\n",
    "    image_loss = contrastive_loss(similarity.t())\n",
    "    return (caption_loss + image_loss) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217560,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "iOvAJP0L91ns"
   },
   "outputs": [],
   "source": [
    "def hard_softmax(logits: torch.Tensor, dim: int):\n",
    "    y_soft = logits.softmax(dim)\n",
    "    # Straight through.\n",
    "    index = y_soft.max(dim, keepdim=True)[1]\n",
    "    y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(dim, index, 1.0)\n",
    "    ret = y_hard - y_soft.detach() + y_soft\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217560,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "6-b2H_fE93b1"
   },
   "outputs": [],
   "source": [
    "def gumbel_softmax(logits: torch.Tensor, tau: float = 1, hard: bool = False, dim: int = -1) -> torch.Tensor:\n",
    "    # more stable https://github.com/pytorch/pytorch/issues/41663\n",
    "    gumbel_dist = torch.distributions.gumbel.Gumbel(\n",
    "        torch.tensor(0.0, device=logits.device, dtype=logits.dtype),\n",
    "        torch.tensor(1.0, device=logits.device, dtype=logits.dtype),\n",
    "    )\n",
    "    gumbels = gumbel_dist.sample(logits.shape)\n",
    "\n",
    "    gumbels = (logits + gumbels) / tau  # ~Gumbel(logits,tau)\n",
    "    y_soft = gumbels.softmax(dim)\n",
    "\n",
    "    if hard:\n",
    "        # Straight through.\n",
    "        index = y_soft.max(dim, keepdim=True)[1]\n",
    "        y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(dim, index, 1.0)\n",
    "        ret = y_hard - y_soft.detach() + y_soft\n",
    "    else:\n",
    "        # Reparametrization trick.\n",
    "        ret = y_soft\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217561,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "sh78Xa0q9564"
   },
   "outputs": [],
   "source": [
    "def resize_attention_map(attentions, height, width, align_corners=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        attentions (`torch.Tensor`): attention map of shape [batch_size, groups, feat_height*feat_width]\n",
    "        height (`int`): height of the output attention map\n",
    "        width (`int`): width of the output attention map\n",
    "        align_corners (`bool`, *optional*): the `align_corner` argument for `nn.functional.interpolate`.\n",
    "\n",
    "    Returns:\n",
    "        `torch.Tensor`: resized attention map of shape [batch_size, groups, height, width]\n",
    "    \"\"\"\n",
    "\n",
    "    scale = (height * width // attentions.shape[2]) ** 0.5\n",
    "    if height > width:\n",
    "        feat_width = int(np.round(width / scale))\n",
    "        feat_height = attentions.shape[2] // feat_width\n",
    "    else:\n",
    "        feat_height = int(np.round(height / scale))\n",
    "        feat_width = attentions.shape[2] // feat_height\n",
    "\n",
    "    batch_size = attentions.shape[0]\n",
    "    groups = attentions.shape[1]  # number of group token\n",
    "    # [batch_size, groups, height*width, groups] -> [batch_size, groups, height, width]\n",
    "    attentions = attentions.reshape(batch_size, groups, feat_height, feat_width)\n",
    "    attentions = nn.functional.interpolate(\n",
    "        attentions, size=(height, width), mode=\"bilinear\", align_corners=align_corners\n",
    "    )\n",
    "    return attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217561,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "KyZtU92l98ix"
   },
   "outputs": [],
   "source": [
    "def get_grouping_from_attentions(attentions, hw_shape):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        attentions (`tuple(torch.FloatTensor)`: tuple of attention maps returned by `GroupViTVisionTransformer`\n",
    "        hw_shape (`tuple(int)`): height and width of the output attention map\n",
    "    Returns:\n",
    "        `torch.Tensor`: the attention map of shape [batch_size, groups, height, width]\n",
    "    \"\"\"\n",
    "\n",
    "    attn_maps = []\n",
    "    with torch.no_grad():\n",
    "        prev_attn_masks = None\n",
    "        for attn_masks in attentions:\n",
    "            # [batch_size, num_groups, height x width] -> [batch_size, height x width, num_groups]\n",
    "            attn_masks = attn_masks.permute(0, 2, 1).contiguous()\n",
    "            if prev_attn_masks is None:\n",
    "                prev_attn_masks = attn_masks\n",
    "            else:\n",
    "                prev_attn_masks = prev_attn_masks @ attn_masks\n",
    "            # [batch_size, heightxwidth, num_groups] -> [batch_size, num_groups, heightxwidth] -> [batch_size, num_groups, height, width]\n",
    "            cur_attn_map = resize_attention_map(prev_attn_masks.permute(0, 2, 1).contiguous(), *hw_shape)\n",
    "            attn_maps.append(cur_attn_map)\n",
    "\n",
    "    # [batch_size, num_groups, height, width]\n",
    "    final_grouping = attn_maps[-1]\n",
    "\n",
    "    return final_grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217561,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "sX37CrAn9_Mw"
   },
   "outputs": [],
   "source": [
    "class GroupViTCrossAttentionLayer(nn.Module):\n",
    "    def __init__(self, config: GroupViTVisionConfig):\n",
    "        super().__init__()\n",
    "        self.attn = GroupViTAttention(config)\n",
    "        self.norm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.mlp = GroupViTMLP(config)\n",
    "        self.norm_post = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, query, key):\n",
    "        x = query\n",
    "        x = x + self.attn(query, encoder_hidden_states=key)[0]\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        x = self.norm_post(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217561,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "LfMDcjbu-BFh"
   },
   "outputs": [],
   "source": [
    "class GroupViTAssignAttention(nn.Module):\n",
    "    def __init__(self, config: GroupViTVisionConfig):\n",
    "        super().__init__()\n",
    "        self.scale = config.hidden_size**-0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.proj = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.assign_eps = config.assign_eps\n",
    "\n",
    "    def get_attn(self, attn, gumbel=True, hard=True):\n",
    "        if gumbel and self.training:\n",
    "            attn = gumbel_softmax(attn, dim=-2, hard=hard)\n",
    "        else:\n",
    "            if hard:\n",
    "                attn = hard_softmax(attn, dim=-2)\n",
    "            else:\n",
    "                attn = nn.functional.softmax(attn, dim=-2)\n",
    "\n",
    "        return attn\n",
    "\n",
    "    def forward(self, query, key):\n",
    "        value = key\n",
    "        # [batch_size, query_length, channels]\n",
    "        query = self.q_proj(query)\n",
    "\n",
    "        # [batch_size, key_length, channels]\n",
    "        key = self.k_proj(key)\n",
    "\n",
    "        # [batch_size, key_length, channels]\n",
    "        value = self.v_proj(value)\n",
    "\n",
    "        # [batch_size, query_length, key_length]\n",
    "        raw_attn = (query @ key.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        attn = self.get_attn(raw_attn)\n",
    "        soft_attn = self.get_attn(raw_attn, gumbel=False, hard=False)\n",
    "\n",
    "        attn = attn / (attn.sum(dim=-1, keepdim=True) + self.assign_eps)\n",
    "\n",
    "        out = attn @ value\n",
    "\n",
    "        out = self.proj(out)\n",
    "\n",
    "        return out, soft_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217562,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "yOJBrOGt-GKD"
   },
   "outputs": [],
   "source": [
    "class GroupViTTokenAssign(nn.Module):\n",
    "    def __init__(self, config: GroupViTVisionConfig, num_group_token, num_output_group):\n",
    "        super().__init__()\n",
    "        self.num_output_group = num_output_group\n",
    "        # norm on group_tokens\n",
    "        self.norm_tokens = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        assign_mlp_ratio = (\n",
    "            config.assign_mlp_ratio\n",
    "            if isinstance(config.assign_mlp_ratio, collections.abc.Iterable)\n",
    "            else (config.assign_mlp_ratio, config.assign_mlp_ratio)\n",
    "        )\n",
    "        tokens_dim, channels_dim = [int(x * config.hidden_size) for x in assign_mlp_ratio]\n",
    "        self.mlp_inter = GroupViTMixerMLP(config, num_group_token, tokens_dim, num_output_group)\n",
    "        self.norm_post_tokens = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        # norm on x\n",
    "        self.norm_x = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.pre_assign_attn = GroupViTCrossAttentionLayer(config)\n",
    "\n",
    "        self.assign = GroupViTAssignAttention(config)\n",
    "        self.norm_new_x = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.mlp_channels = GroupViTMLP(config, config.hidden_size, channels_dim, config.hidden_size)\n",
    "\n",
    "    def project_group_token(self, group_tokens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            group_tokens (torch.Tensor): group tokens, [batch_size, num_group_tokens, channels]\n",
    "\n",
    "        Returns:\n",
    "            projected_group_tokens (torch.Tensor): [batch_size, num_output_groups, channels]\n",
    "        \"\"\"\n",
    "        # [B, num_output_groups, C] <- [B, num_group_tokens, C]\n",
    "        projected_group_tokens = self.mlp_inter(group_tokens)\n",
    "        projected_group_tokens = self.norm_post_tokens(projected_group_tokens)\n",
    "        return projected_group_tokens\n",
    "\n",
    "    def forward(self, image_tokens, group_tokens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_tokens (`torch.Tensor`): image tokens, of shape [batch_size, input_length, channels]\n",
    "            group_tokens (`torch.Tensor`): group tokens, [batch_size, num_group_tokens, channels]\n",
    "        \"\"\"\n",
    "\n",
    "        group_tokens = self.norm_tokens(group_tokens)\n",
    "        image_tokens = self.norm_x(image_tokens)\n",
    "        # [batch_size, num_output_groups, channels]\n",
    "        projected_group_tokens = self.project_group_token(group_tokens)\n",
    "        projected_group_tokens = self.pre_assign_attn(projected_group_tokens, image_tokens)\n",
    "        new_image_tokens, attention = self.assign(projected_group_tokens, image_tokens)\n",
    "        new_image_tokens += projected_group_tokens\n",
    "\n",
    "        new_image_tokens = new_image_tokens + self.mlp_channels(self.norm_new_x(new_image_tokens))\n",
    "\n",
    "        return new_image_tokens, attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217562,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "1eBLeUct-JCq"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GroupViTModelOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n",
    "            Contrastive loss for image-text similarity.\n",
    "        logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n",
    "            The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n",
    "            similarity scores.\n",
    "        logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n",
    "            The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n",
    "            similarity scores.\n",
    "        segmentation_logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels, logits_height, logits_width)`):\n",
    "            Classification scores for each pixel.\n",
    "\n",
    "            <Tip warning={true}>\n",
    "\n",
    "            The logits returned do not necessarily have the same size as the `pixel_values` passed as inputs. This is\n",
    "            to avoid doing two interpolations and lose some quality when a user needs to resize the logits to the\n",
    "            original image size as post-processing. You should always check your logits shape and resize as needed.\n",
    "\n",
    "            </Tip>\n",
    "\n",
    "        text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n",
    "            The text embeddings obtained by applying the projection layer to the pooled output of\n",
    "            [`GroupViTTextModel`].\n",
    "        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n",
    "            The image embeddings obtained by applying the projection layer to the pooled output of\n",
    "            [`GroupViTVisionModel`].\n",
    "        text_model_output (`BaseModelOutputWithPooling`):\n",
    "            The output of the [`GroupViTTextModel`].\n",
    "        vision_model_output (`BaseModelOutputWithPooling`):\n",
    "            The output of the [`GroupViTVisionModel`].\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits_per_image: torch.FloatTensor = None\n",
    "    logits_per_text: torch.FloatTensor = None\n",
    "    segmentation_logits: torch.FloatTensor = None\n",
    "    text_embeds: torch.FloatTensor = None\n",
    "    image_embeds: torch.FloatTensor = None\n",
    "    text_model_output: BaseModelOutputWithPooling = None\n",
    "    vision_model_output: BaseModelOutputWithPooling = None\n",
    "\n",
    "    def to_tuple(self) -> Tuple[Any]:\n",
    "        return tuple(\n",
    "            self[k] if k not in [\"text_model_output\", \"vision_model_output\"] else getattr(self, k).to_tuple()\n",
    "            for k in self.keys()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217562,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "8atGssmE-OgA"
   },
   "outputs": [],
   "source": [
    "class GroupViTPatchEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Image to Patch Embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int = 224,\n",
    "        patch_size: Union[int, Tuple[int, int]] = 16,\n",
    "        num_channels: int = 3,\n",
    "        embed_dim: int = 768,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        image_size = image_size if isinstance(image_size, collections.abc.Iterable) else (image_size, image_size)\n",
    "        patch_size = patch_size if isinstance(patch_size, collections.abc.Iterable) else (patch_size, patch_size)\n",
    "        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.projection = nn.Conv2d(num_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor, interpolate_pos_encoding: bool = False) -> torch.Tensor:\n",
    "        batch_size, num_channels, height, width = pixel_values.shape\n",
    "        if not interpolate_pos_encoding:\n",
    "            if height != self.image_size[0] or width != self.image_size[1]:\n",
    "                raise ValueError(\n",
    "                    f\"Input image size ({height}*{width}) doesn't match model\"\n",
    "                    f\" ({self.image_size[0]}*{self.image_size[1]}).\"\n",
    "                )\n",
    "        x = self.projection(pixel_values).flatten(2).transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217562,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "pvmCYxCX-Q5x"
   },
   "outputs": [],
   "source": [
    "class GroupViTVisionEmbeddings(nn.Module):\n",
    "    def __init__(self, config: GroupViTVisionConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embeddings = GroupViTPatchEmbeddings(\n",
    "            image_size=config.image_size,\n",
    "            patch_size=config.patch_size,\n",
    "            num_channels=config.num_channels,\n",
    "            embed_dim=config.hidden_size,\n",
    "        )\n",
    "        num_patches = self.patch_embeddings.num_patches\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches, config.hidden_size))\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.config = config\n",
    "\n",
    "    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher\n",
    "        resolution images.\n",
    "\n",
    "        Source:\n",
    "        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n",
    "        \"\"\"\n",
    "\n",
    "        npatch = embeddings.shape[1]\n",
    "        if npatch == self.position_embeddings.shape[1] and height == width:\n",
    "            return self.position_embeddings\n",
    "        patch_pos_embed = self.position_embeddings\n",
    "        num_original_pos_embed = patch_pos_embed.shape[1]\n",
    "        dim = embeddings.shape[-1]\n",
    "        feat_height = height // self.config.patch_size\n",
    "        feat_width = width // self.config.patch_size\n",
    "        # we add a small number to avoid floating point error in the interpolation\n",
    "        # see discussion at https://github.com/facebookresearch/dino/issues/8\n",
    "        feat_height, feat_width = feat_height + 0.1, feat_width + 0.1\n",
    "        original_height = original_width = math.sqrt(num_original_pos_embed)\n",
    "        reshaped_patch_pos_embed = patch_pos_embed.reshape(1, int(original_height), int(original_width), dim).permute(\n",
    "            0, 3, 1, 2\n",
    "        )\n",
    "        scale_factor = (feat_height / original_height, feat_width / original_width)\n",
    "        patch_pos_embed = nn.functional.interpolate(\n",
    "            reshaped_patch_pos_embed,\n",
    "            scale_factor=scale_factor,\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return patch_pos_embed\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor, interpolate_pos_encoding: bool = False) -> torch.Tensor:\n",
    "        batch_size, num_channels, height, width = pixel_values.shape\n",
    "        embeddings = self.patch_embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n",
    "\n",
    "        embeddings = self.layernorm(embeddings)\n",
    "\n",
    "        batch_size, seq_len, _ = embeddings.size()\n",
    "\n",
    "        # add positional encoding to each token\n",
    "        if interpolate_pos_encoding:\n",
    "            embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)\n",
    "        else:\n",
    "            embeddings = embeddings + self.position_embeddings\n",
    "\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217563,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "jZ_xSDus-ULA"
   },
   "outputs": [],
   "source": [
    "# Copied from transformers.models.clip.modeling_clip.CLIPTextEmbeddings with CLIP->GroupViT\n",
    "class GroupViTTextEmbeddings(nn.Module):\n",
    "    def __init__(self, config: GroupViTTextConfig):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(config.max_position_embeddings, embed_dim)\n",
    "\n",
    "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
    "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        seq_length = input_ids.shape[-1] if input_ids is not None else inputs_embeds.shape[-2]\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, :seq_length]\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.token_embedding(input_ids)\n",
    "\n",
    "        position_embeddings = self.position_embedding(position_ids)\n",
    "        embeddings = inputs_embeds + position_embeddings\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217563,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "WI0LJqF--anN"
   },
   "outputs": [],
   "source": [
    "class GroupViTStage(nn.Module):\n",
    "    \"\"\"This corresponds to the `GroupingLayer` class in the GroupViT implementation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: GroupViTVisionConfig,\n",
    "        depth: int,\n",
    "        num_prev_group_token: int,\n",
    "        num_group_token: int,\n",
    "        num_output_group: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.num_group_token = num_group_token\n",
    "        if num_group_token > 0:\n",
    "            self.group_token = nn.Parameter(torch.zeros(1, num_group_token, config.hidden_size))\n",
    "        else:\n",
    "            self.group_token = None\n",
    "        self.gradient_checkpointing = False\n",
    "        self.layers = nn.ModuleList([GroupViTEncoderLayer(config) for _ in range(depth)])\n",
    "\n",
    "        if num_group_token > 0:\n",
    "            self.downsample = GroupViTTokenAssign(\n",
    "                config=config,\n",
    "                num_group_token=num_group_token,\n",
    "                num_output_group=num_output_group,\n",
    "            )\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "        if num_prev_group_token > 0 and num_group_token > 0:\n",
    "            self.group_projector = nn.Sequential(\n",
    "                nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps),\n",
    "                GroupViTMixerMLP(config, num_prev_group_token, config.hidden_size // 2, num_group_token),\n",
    "            )\n",
    "        else:\n",
    "            self.group_projector = None\n",
    "\n",
    "    @property\n",
    "    def with_group_token(self):\n",
    "        return self.group_token is not None\n",
    "\n",
    "    def split_x(self, x):\n",
    "        if self.with_group_token:\n",
    "            return x[:, : -self.num_group_token], x[:, -self.num_group_token :]\n",
    "        else:\n",
    "            return x, None\n",
    "\n",
    "    def concat_x(self, x: torch.Tensor, group_token: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        if group_token is None:\n",
    "            return x\n",
    "        return torch.cat([x, group_token], dim=1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        prev_group_token: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.FloatTensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`): attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "                `(config.encoder_attention_heads,)`.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the grouping tensors of Grouping block.\n",
    "        \"\"\"\n",
    "        if self.with_group_token:\n",
    "            group_token = self.group_token.expand(hidden_states.size(0), -1, -1)\n",
    "            if self.group_projector is not None:\n",
    "                group_token = group_token + self.group_projector(prev_group_token)\n",
    "        else:\n",
    "            group_token = None\n",
    "\n",
    "        x = hidden_states\n",
    "\n",
    "        cat_x = self.concat_x(x, group_token)\n",
    "        for layer in self.layers:\n",
    "            layer_out = layer(cat_x, attention_mask=None, causal_attention_mask=None)\n",
    "            cat_x = layer_out[0]\n",
    "\n",
    "        x, group_token = self.split_x(cat_x)\n",
    "\n",
    "        attention = None\n",
    "        if self.downsample is not None:\n",
    "            x, attention = self.downsample(x, group_token)\n",
    "\n",
    "        outputs = (x, group_token)\n",
    "        if output_attentions:\n",
    "            outputs = outputs + (attention,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217563,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "lZNeThbG-bdS"
   },
   "outputs": [],
   "source": [
    "class GroupViTMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: GroupViTVisionConfig,\n",
    "        hidden_size: Optional[int] = None,\n",
    "        intermediate_size: Optional[int] = None,\n",
    "        output_size: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.activation_fn = ACT2FN[config.hidden_act]\n",
    "        hidden_size = hidden_size if hidden_size is not None else config.hidden_size\n",
    "        intermediate_size = intermediate_size if intermediate_size is not None else config.intermediate_size\n",
    "        output_size = output_size if output_size is not None else hidden_size\n",
    "        self.fc1 = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.fc2 = nn.Linear(intermediate_size, output_size)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.fc1(hidden_states)\n",
    "        hidden_states = self.activation_fn(hidden_states)\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217563,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "F5PfLQe2-dnX"
   },
   "outputs": [],
   "source": [
    "class GroupViTMixerMLP(GroupViTMLP):\n",
    "    def forward(self, x):\n",
    "        x = super().forward(x.transpose(1, 2))\n",
    "        return x.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217564,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "zIDEv8MG-fPE"
   },
   "outputs": [],
   "source": [
    "class GroupViTAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        if self.head_dim * self.num_heads != self.embed_dim:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n",
    "                f\" {self.num_heads}).\"\n",
    "            )\n",
    "        self.scale = self.head_dim**-0.5\n",
    "        self.dropout = config.attention_dropout\n",
    "\n",
    "        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
    "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        causal_attention_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
    "\n",
    "        bsz, tgt_len, embed_dim = hidden_states.size()\n",
    "        is_cross_attention = encoder_hidden_states is not None\n",
    "\n",
    "        # get query proj\n",
    "        query_states = self.q_proj(hidden_states) * self.scale\n",
    "        if is_cross_attention:\n",
    "            key_states = self._shape(self.k_proj(encoder_hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(encoder_hidden_states), -1, bsz)\n",
    "        else:\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "\n",
    "        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
    "        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
    "        key_states = key_states.view(*proj_shape)\n",
    "        value_states = value_states.view(*proj_shape)\n",
    "\n",
    "        src_len = key_states.size(1)\n",
    "        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
    "\n",
    "        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n",
    "                f\" {attn_weights.size()}\"\n",
    "            )\n",
    "\n",
    "        # apply the causal_attention_mask first\n",
    "        if causal_attention_mask is not None:\n",
    "            if causal_attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is\"\n",
    "                    f\" {causal_attention_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        if output_attentions:\n",
    "            # this operation is a bit akward, but it's required to\n",
    "            # make sure that attn_weights keeps its gradient.\n",
    "            # In order to do so, attn_weights have to reshaped\n",
    "            # twice and have to be reused in the following\n",
    "            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "        else:\n",
    "            attn_weights_reshaped = None\n",
    "\n",
    "        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "\n",
    "        attn_output = torch.bmm(attn_probs, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "        attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
    "\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights_reshaped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217564,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "MxxVauJA-ioM"
   },
   "outputs": [],
   "source": [
    "# Copied from transformers.models.clip.modeling_clip.CLIPEncoderLayer with CLIP->GroupViT\n",
    "class GroupViTEncoderLayer(nn.Module):\n",
    "    def __init__(self, config: GroupViTConfig):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.self_attn = GroupViTAttention(config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n",
    "        self.mlp = GroupViTMLP(config)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        causal_attention_mask: torch.Tensor,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.FloatTensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`): attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "                `(config.encoder_attention_heads,)`.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "        \"\"\"\n",
    "        residual = hidden_states\n",
    "\n",
    "        hidden_states = self.layer_norm1(hidden_states)\n",
    "        hidden_states, attn_weights = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            causal_attention_mask=causal_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.layer_norm2(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs += (attn_weights,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217564,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "iFAIWiAZ-k9h"
   },
   "outputs": [],
   "source": [
    "class GroupViTPreTrainedModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
    "    models.\n",
    "    \"\"\"\n",
    "\n",
    "    config_class = GroupViTConfig\n",
    "    base_model_prefix = \"groupvit\"\n",
    "    supports_gradient_checkpointing = True\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "\n",
    "        init_range = self.config.initializer_range\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=init_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "        factor = self.config.initializer_factor\n",
    "        if isinstance(module, GroupViTTextEmbeddings):\n",
    "            module.token_embedding.weight.data.normal_(mean=0.0, std=factor * 0.02)\n",
    "            module.position_embedding.weight.data.normal_(mean=0.0, std=factor * 0.02)\n",
    "        elif isinstance(module, GroupViTAttention):\n",
    "            factor = self.config.initializer_factor\n",
    "            in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n",
    "            out_proj_std = (module.embed_dim**-0.5) * factor\n",
    "            nn.init.normal_(module.q_proj.weight, std=in_proj_std)\n",
    "            nn.init.normal_(module.k_proj.weight, std=in_proj_std)\n",
    "            nn.init.normal_(module.v_proj.weight, std=in_proj_std)\n",
    "            nn.init.normal_(module.out_proj.weight, std=out_proj_std)\n",
    "        elif isinstance(module, GroupViTMLP):\n",
    "            factor = self.config.initializer_factor\n",
    "            in_proj_std = (\n",
    "                (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n",
    "            )\n",
    "            fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n",
    "            nn.init.normal_(module.fc1.weight, std=fc_std)\n",
    "            nn.init.normal_(module.fc2.weight, std=in_proj_std)\n",
    "\n",
    "    def _set_gradient_checkpointing(self, module, value=False):\n",
    "        if isinstance(module, (GroupViTTextEncoder, GroupViTVisionEncoder)):\n",
    "            module.gradient_checkpointing = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217564,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "jzru1jUe-wAJ"
   },
   "outputs": [],
   "source": [
    "GROUPVIT_START_DOCSTRING = r\"\"\"\n",
    "    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n",
    "    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n",
    "    behavior.\n",
    "\n",
    "    Parameters:\n",
    "        config ([`GroupViTConfig`]): Model configuration class with all the parameters of the model.\n",
    "            Initializing with a config file does not load the weights associated with the model, only the\n",
    "            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
    "\"\"\"\n",
    "\n",
    "GROUPVIT_TEXT_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n",
    "            it.\n",
    "\n",
    "            Indices can be obtained using [`CLIPTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
    "            [`PreTrainedTokenizer.__call__`] for details.\n",
    "\n",
    "            [What are input IDs?](../glossary#input-ids)\n",
    "        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "\n",
    "            [What are attention masks?](../glossary#attention-mask)\n",
    "        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n",
    "            config.max_position_embeddings - 1]`.\n",
    "\n",
    "            [What are position IDs?](../glossary#position-ids)\n",
    "        output_attentions (`bool`, *optional*):\n",
    "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
    "            tensors for more detail.\n",
    "        output_hidden_states (`bool`, *optional*):\n",
    "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
    "            more detail.\n",
    "        return_dict (`bool`, *optional*):\n",
    "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "\"\"\"\n",
    "\n",
    "GROUPVIT_VISION_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n",
    "            Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using\n",
    "            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details.\n",
    "        output_attentions (`bool`, *optional*):\n",
    "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
    "            tensors for more detail.\n",
    "        output_hidden_states (`bool`, *optional*):\n",
    "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
    "            more detail.\n",
    "        return_dict (`bool`, *optional*):\n",
    "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "\"\"\"\n",
    "\n",
    "GROUPVIT_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n",
    "            it.\n",
    "\n",
    "            Indices can be obtained using [`CLIPTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
    "            [`PreTrainedTokenizer.__call__`] for details.\n",
    "\n",
    "            [What are input IDs?](../glossary#input-ids)\n",
    "        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "\n",
    "            [What are attention masks?](../glossary#attention-mask)\n",
    "        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n",
    "            config.max_position_embeddings - 1]`.\n",
    "\n",
    "            [What are position IDs?](../glossary#position-ids)\n",
    "        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n",
    "            Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See\n",
    "            [`CLIPImageProcessor.__call__`] for details.\n",
    "        return_loss (`bool`, *optional*):\n",
    "            Whether or not to return the contrastive loss.\n",
    "        output_attentions (`bool`, *optional*):\n",
    "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
    "            tensors for more detail.\n",
    "        output_hidden_states (`bool`, *optional*):\n",
    "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
    "            more detail.\n",
    "        return_dict (`bool`, *optional*):\n",
    "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217564,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "diwTOUkc-yuM"
   },
   "outputs": [],
   "source": [
    "class GroupViTVisionEncoder(nn.Module):\n",
    "    def __init__(self, config: GroupViTVisionConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.stages = nn.ModuleList(\n",
    "            [\n",
    "                GroupViTStage(\n",
    "                    config=config,\n",
    "                    depth=config.depths[i],\n",
    "                    num_group_token=config.num_group_tokens[i],\n",
    "                    num_output_group=config.num_output_groups[i],\n",
    "                    num_prev_group_token=config.num_output_groups[i - 1] if i > 0 else 0,\n",
    "                )\n",
    "                for i in range(len(config.depths))\n",
    "            ]\n",
    "        )\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[tuple, BaseModelOutput]:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_groupings = () if output_attentions else None\n",
    "\n",
    "        group_tokens = None\n",
    "\n",
    "        for i, stage in enumerate(self.stages):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_outputs = stage(hidden_states, group_tokens, output_attentions)\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            group_tokens = layer_outputs[1]\n",
    "\n",
    "            if output_attentions and layer_outputs[2] is not None:\n",
    "                all_groupings = all_groupings + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, all_hidden_states, all_groupings] if v is not None)\n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=hidden_states, hidden_states=all_hidden_states, attentions=all_groupings\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217565,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "ErgnsQgL-zvh"
   },
   "outputs": [],
   "source": [
    "class GroupViTTextEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder consisting of `config.num_hidden_layers` self-attention layers. Each layer is a\n",
    "    [`GroupViTEncoderLayer`].\n",
    "\n",
    "    Args:\n",
    "        config: GroupViTTextConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: GroupViTTextConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layers = nn.ModuleList([GroupViTEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        inputs_embeds,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        causal_attention_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutput]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n",
    "                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n",
    "                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n",
    "                than the model's internal embedding lookup matrix.\n",
    "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            causal_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Causal mask for the text model. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            output_hidden_states (`bool`, *optional*):\n",
    "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
    "                for more detail.\n",
    "            return_dict (`bool`, *optional*):\n",
    "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        encoder_states = () if output_hidden_states else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "        for idx, encoder_layer in enumerate(self.layers):\n",
    "            if output_hidden_states:\n",
    "                encoder_states = encoder_states + (hidden_states,)\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(encoder_layer),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    causal_attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = encoder_layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    causal_attention_mask,\n",
    "                    output_attentions=output_attentions,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            encoder_states = encoder_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217566,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "fsgeLvGW_gB0"
   },
   "outputs": [],
   "source": [
    "# Copied from transformers.models.clip.modeling_clip.CLIPTextTransformer with CLIPText->GroupViTText, CLIPEncoder->GroupViTTextEncoder, CLIP_TEXT->GROUPVIT_TEXT\n",
    "class GroupViTTextTransformer(nn.Module):\n",
    "    def __init__(self, config: GroupViTTextConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        embed_dim = config.hidden_size\n",
    "        self.embeddings = GroupViTTextEmbeddings(config)\n",
    "        self.encoder = GroupViTTextEncoder(config)\n",
    "        self.final_layer_norm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(GROUPVIT_TEXT_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=GroupViTTextConfig)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if input_ids is None:\n",
    "            raise ValueError(\"You have to specify input_ids\")\n",
    "\n",
    "        input_shape = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, input_shape[-1])\n",
    "\n",
    "        hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n",
    "\n",
    "        bsz, seq_len = input_shape\n",
    "        # CLIP's text model uses causal mask, prepare it here.\n",
    "        # https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/clip/model.py#L324\n",
    "        causal_attention_mask = self._build_causal_attention_mask(bsz, seq_len, hidden_states.dtype).to(\n",
    "            hidden_states.device\n",
    "        )\n",
    "        # expand attention_mask\n",
    "        if attention_mask is not None:\n",
    "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "            attention_mask = _expand_mask(attention_mask, hidden_states.dtype)\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            inputs_embeds=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            causal_attention_mask=causal_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        last_hidden_state = encoder_outputs[0]\n",
    "        last_hidden_state = self.final_layer_norm(last_hidden_state)\n",
    "\n",
    "        # text_embeds.shape = [batch_size, sequence_length, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        # casting to torch.int for onnx compatibility: argmax doesn't support int64 inputs with opset 14\n",
    "        pooled_output = last_hidden_state[\n",
    "            torch.arange(last_hidden_state.shape[0], device=last_hidden_state.device),\n",
    "            input_ids.to(dtype=torch.int, device=last_hidden_state.device).argmax(dim=-1),\n",
    "        ]\n",
    "\n",
    "        if not return_dict:\n",
    "            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPooling(\n",
    "            last_hidden_state=last_hidden_state,\n",
    "            pooler_output=pooled_output,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def _build_causal_attention_mask(self, bsz, seq_len, dtype):\n",
    "        # lazily create causal attention mask, with full attention between the vision tokens\n",
    "        # pytorch uses additive attention mask; fill with -inf\n",
    "        mask = torch.empty(bsz, seq_len, seq_len, dtype=dtype)\n",
    "        mask.fill_(torch.tensor(torch.finfo(dtype).min))\n",
    "        mask.triu_(1)  # zero out the lower diagonal\n",
    "        mask = mask.unsqueeze(1)  # expand mask\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 1409,
     "status": "ok",
     "timestamp": 1677512218961,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "QAAhPken_ibc"
   },
   "outputs": [],
   "source": [
    "class GroupViTTextModel(GroupViTPreTrainedModel):\n",
    "    config_class = GroupViTTextConfig\n",
    "\n",
    "    def __init__(self, config: GroupViTTextConfig):\n",
    "        super().__init__(config)\n",
    "        self.text_model = GroupViTTextTransformer(config)\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self) -> nn.Module:\n",
    "        return self.text_model.embeddings.token_embedding\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.text_model.embeddings.token_embedding = value\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(GROUPVIT_TEXT_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=GroupViTTextConfig)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import CLIPTokenizer, GroupViTTextModel\n",
    "\n",
    "        >>> tokenizer = CLIPTokenizer.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "        >>> model = GroupViTTextModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "\n",
    "        >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n",
    "\n",
    "        >>> outputs = model(**inputs)\n",
    "        >>> last_hidden_state = outputs.last_hidden_state\n",
    "        >>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n",
    "        ```\"\"\"\n",
    "        return self.text_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1677512218961,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "pNJ4HozD_ls2"
   },
   "outputs": [],
   "source": [
    "class GroupViTVisionTransformer(nn.Module):########################\n",
    "    def __init__(self, config: GroupViTVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        embed_dim = config.hidden_size\n",
    "\n",
    "        self.embeddings = GroupViTVisionEmbeddings(config)\n",
    "        self.encoder = GroupViTVisionEncoder(config)\n",
    "        self.layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(GROUPVIT_VISION_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=GroupViTVisionConfig)\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if pixel_values is None:\n",
    "            raise ValueError(\"You have to specify pixel_values\")\n",
    "\n",
    "        hidden_states = self.embeddings(pixel_values)\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            hidden_states=hidden_states,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            output_attentions=output_attentions,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        last_hidden_state = encoder_outputs[0]\n",
    "\n",
    "        # normalize the last hidden state\n",
    "        last_hidden_state = self.layernorm(last_hidden_state)\n",
    "        pooled_output = last_hidden_state.mean(dim=1)\n",
    "\n",
    "        if not return_dict:\n",
    "            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPooling(\n",
    "            last_hidden_state=last_hidden_state,\n",
    "            pooler_output=pooled_output,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1677512218961,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "Tf5zCFfp_pS0"
   },
   "outputs": [],
   "source": [
    "class GroupViTVisionModel(GroupViTPreTrainedModel):\n",
    "    config_class = GroupViTVisionConfig\n",
    "    main_input_name = \"pixel_values\"\n",
    "\n",
    "    def __init__(self, config: GroupViTVisionConfig, projection_dim=128):\n",
    "        super().__init__(config)\n",
    "        self.vision_model = GroupViTVisionTransformer(config)\n",
    "\n",
    "        self.projection_dim = projection_dim\n",
    "        self.projection_intermediate_dim = 4096\n",
    "        self.vision_embed_dim = config.hidden_size\n",
    "\n",
    "        self.visual_projection = nn.Sequential(\n",
    "            nn.Linear(self.vision_embed_dim, self.projection_intermediate_dim, bias=True),\n",
    "            nn.BatchNorm1d(self.projection_intermediate_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.projection_intermediate_dim, self.projection_dim, bias=True),\n",
    "        )\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self) -> GroupViTPatchEmbeddings:\n",
    "        return self.vision_model.embeddings.patch_embeddings\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(GROUPVIT_VISION_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=GroupViTVisionConfig)\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from PIL import Image\n",
    "        >>> import requests\n",
    "        >>> from transformers import AutoProcessor, GroupViTVisionModel\n",
    "\n",
    "        >>> processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "        >>> model = GroupViTVisionModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "\n",
    "        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "        >>> image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "        >>> inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "        >>> outputs = model(**inputs)\n",
    "        >>> last_hidden_state = outputs.last_hidden_state\n",
    "        >>> pooled_output = outputs.pooler_output  # pooled CLS states\n",
    "        ```\"\"\"\n",
    "\n",
    "        # print(type(pixel_values), type(output_attentions), type(output_hidden_states), type(return_dict))\n",
    "        # print(pixel_values.shape, output_attentions, output_hidden_states, return_dict)\n",
    "        # print('pixel_values=', pixel_values.shape)\n",
    "        output_attentions = True\n",
    "        output_hidden_states = False\n",
    "        return_dict = True\n",
    "        # print(pixel_values.shape, output_attentions, output_hidden_states, return_dict)\n",
    "        vision_outputs = self.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        # print(vision_outputs)\n",
    "\n",
    "        attentions = vision_outputs[2]\n",
    "            \n",
    "        # [batch_size_image, num_group, height, width]\n",
    "        grouping = get_grouping_from_attentions(attentions, pixel_values.shape[2:])\n",
    "        seg_logits = grouping\n",
    "\n",
    "        pooled_output = vision_outputs[1]  # pooled_output\n",
    "        image_features = self.visual_projection(pooled_output)\n",
    "\n",
    "        # print(image_features.shape)\n",
    "        return vision_outputs, seg_logits, image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1677512218962,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "Q3lIeXoH_slB"
   },
   "outputs": [],
   "source": [
    "@add_start_docstrings(GROUPVIT_START_DOCSTRING)\n",
    "class GroupViTModel(GroupViTPreTrainedModel):\n",
    "    config_class = GroupViTConfig\n",
    "\n",
    "    def __init__(self, config: GroupViTConfig):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # if not isinstance(config.text_config, GroupViTTextConfig):\n",
    "        #     raise ValueError(\n",
    "        #         \"config.text_config is expected to be of type GroupViTTextConfig but is of type\"\n",
    "        #         f\" {type(config.text_config)}.\"\n",
    "        #     )\n",
    "\n",
    "        if not isinstance(config.vision_config, GroupViTVisionConfig):\n",
    "            raise ValueError(\n",
    "                \"config.vision_config is expected to be of type GroupViTVisionConfig but is of type\"\n",
    "                f\" {type(config.vision_config)}.\"\n",
    "            )\n",
    "\n",
    "        # text_config = config.text_config\n",
    "        vision_config = config.vision_config\n",
    "\n",
    "        self.projection_dim = config.projection_dim\n",
    "        self.projection_intermediate_dim = config.projection_intermediate_dim\n",
    "        # self.text_embed_dim = text_config.hidden_size\n",
    "        self.vision_embed_dim = vision_config.hidden_size\n",
    "        print('hidden_size', vision_config.hidden_size)\n",
    "\n",
    "        # self.text_model = GroupViTTextTransformer(text_config)\n",
    "        self.vision_model = GroupViTVisionTransformer(vision_config)\n",
    "\n",
    "        self.visual_projection = nn.Sequential(\n",
    "            nn.Linear(self.vision_embed_dim, self.projection_intermediate_dim, bias=True),\n",
    "            nn.BatchNorm1d(self.projection_intermediate_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.projection_intermediate_dim, self.projection_dim, bias=True),\n",
    "        )\n",
    "        # self.text_projection = nn.Sequential(\n",
    "        #     nn.Linear(self.text_embed_dim, self.projection_intermediate_dim, bias=True),\n",
    "        #     nn.BatchNorm1d(self.projection_intermediate_dim),\n",
    "        #     nn.ReLU(inplace=True),\n",
    "        #     nn.Linear(self.projection_intermediate_dim, self.projection_dim, bias=True),\n",
    "        # )\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * self.config.logit_scale_init_value)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    # @add_start_docstrings_to_model_forward(GROUPVIT_TEXT_INPUTS_DOCSTRING)\n",
    "    # def get_text_features(\n",
    "    #     self,\n",
    "    #     input_ids: Optional[torch.Tensor] = None,\n",
    "    #     attention_mask: Optional[torch.Tensor] = None,\n",
    "    #     position_ids: Optional[torch.Tensor] = None,\n",
    "    #     output_attentions: Optional[bool] = None,\n",
    "    #     output_hidden_states: Optional[bool] = None,\n",
    "    #     return_dict: Optional[bool] = None,\n",
    "    # ) -> torch.FloatTensor:\n",
    "    #     r\"\"\"\n",
    "    #     Returns:\n",
    "    #         text_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The text embeddings obtained by\n",
    "    #         applying the projection layer to the pooled output of [`GroupViTTextModel`].\n",
    "\n",
    "    #     Examples:\n",
    "\n",
    "    #     ```python\n",
    "    #     >>> from transformers import CLIPTokenizer, GroupViTModel\n",
    "\n",
    "    #     >>> model = GroupViTModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "    #     >>> tokenizer = CLIPTokenizer.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "\n",
    "    #     >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n",
    "    #     >>> text_features = model.get_text_features(**inputs)\n",
    "    #     ```\"\"\"\n",
    "    #     # Use GROUPVIT model's config for some fields (if specified) instead of those of vision & text components.\n",
    "    #     output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "    #     output_hidden_states = (\n",
    "    #         output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "    #     )\n",
    "    #     return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "    #     text_outputs = self.text_model(\n",
    "    #         input_ids=input_ids,\n",
    "    #         attention_mask=attention_mask,\n",
    "    #         position_ids=position_ids,\n",
    "    #         output_attentions=output_attentions,\n",
    "    #         output_hidden_states=output_hidden_states,\n",
    "    #         return_dict=return_dict,\n",
    "    #     )\n",
    "\n",
    "    #     pooled_output = text_outputs[1]\n",
    "    #     text_features = self.text_projection(pooled_output)\n",
    "\n",
    "    #     return text_features\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(GROUPVIT_VISION_INPUTS_DOCSTRING)\n",
    "    def get_image_features(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> torch.FloatTensor:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "            image_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The image embeddings obtained by\n",
    "            applying the projection layer to the pooled output of [`GroupViTVisionModel`].\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from PIL import Image\n",
    "        >>> import requests\n",
    "        >>> from transformers import AutoProcessor, GroupViTModel\n",
    "\n",
    "        >>> model = GroupViTModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "        >>> processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "\n",
    "        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "        >>> image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "        >>> inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "        >>> image_features = model.get_image_features(**inputs)\n",
    "        ```\"\"\"\n",
    "        # Use GROUPVIT model's config for some fields (if specified) instead of those of vision & text components.\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        print(pixel_values.shape, output_attentions, output_hidden_states, return_dict)\n",
    "\n",
    "        vision_outputs = self.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = vision_outputs[1]  # pooled_output\n",
    "        print('01 ', pooled_output.shape)\n",
    "\n",
    "        image_features = self.visual_projection(pooled_output)\n",
    "        print('02 ', image_features.shape)\n",
    "\n",
    "        return image_features\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(GROUPVIT_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=GroupViTModelOutput, config_class=GroupViTConfig)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        return_loss: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_segmentation: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, GroupViTModelOutput]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from PIL import Image\n",
    "        >>> import requests\n",
    "        >>> from transformers import AutoProcessor, GroupViTModel\n",
    "\n",
    "        >>> model = GroupViTModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "        >>> processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "\n",
    "        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "        >>> image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "        >>> inputs = processor(\n",
    "        ...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True\n",
    "        ... )\n",
    "\n",
    "        >>> outputs = model(**inputs)\n",
    "        >>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "        >>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n",
    "        ```\"\"\"\n",
    "        # Use GROUPVIT model's config for some fields (if specified) instead of those of vision & text components.\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_segmentation = (\n",
    "            output_segmentation if output_segmentation is not None else self.config.output_segmentation\n",
    "        )\n",
    "        if output_segmentation:\n",
    "            output_attentions = True\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # print(type(pixel_values), type(output_attentions), type(output_hidden_states), type(return_dict))\n",
    "        print(pixel_values.shape, output_attentions, output_hidden_states, return_dict)\n",
    "\n",
    "        vision_outputs = self.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        # print(vision_outputs)\n",
    "\n",
    "        # text_outputs = self.text_model(\n",
    "        #     input_ids=input_ids,\n",
    "        #     attention_mask=attention_mask,\n",
    "        #     position_ids=position_ids,\n",
    "        #     output_attentions=output_attentions,\n",
    "        #     output_hidden_states=output_hidden_states,\n",
    "        #     return_dict=return_dict,\n",
    "        # )\n",
    "\n",
    "        image_embeds = vision_outputs[1]\n",
    "        image_embeds = self.visual_projection(image_embeds)\n",
    "\n",
    "        # text_embeds = text_outputs[1]\n",
    "        # text_embeds = self.text_projection(text_embeds)\n",
    "\n",
    "        # normalized features\n",
    "        image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n",
    "        # text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        # logit_scale = self.logit_scale.exp()\n",
    "        # logits_per_text = torch.matmul(text_embeds, image_embeds.t()) * logit_scale\n",
    "        # logits_per_image = logits_per_text.t()\n",
    "\n",
    "        seg_logits = None\n",
    "        if output_segmentation:\n",
    "            # grouped features\n",
    "            # [batch_size_image, num_group, hidden_size]\n",
    "            image_group_embeds = vision_outputs[0]\n",
    "            print('image_group_embeds_01', image_group_embeds.shape, type(image_group_embeds)) # torch.Size([1, 8, 384]) <class 'torch.Tensor'>\n",
    "\n",
    "            # [batch_size_image*num_group, hidden_size]\n",
    "            image_group_embeds = self.visual_projection(image_group_embeds.reshape(-1, image_group_embeds.shape[-1]))\n",
    "            print('image_group_embeds_02', image_group_embeds.shape, type(image_group_embeds)) # torch.Size([8, 256]) <class 'torch.Tensor'>\n",
    "\n",
    "            if output_hidden_states:\n",
    "                attentions = vision_outputs[3]\n",
    "                print('attentions_01', attentions.shape, type(attentions)) # *\n",
    "\n",
    "            else:\n",
    "                attentions = vision_outputs[2]\n",
    "                print('attentions_02', attentions[0].shape, type(attentions[0]), attentions[1].shape, type(attentions[1])) # torch.Size([1, 64, 196]) torch.Size([1, 8, 64]) <class 'torch.Tensor'>\n",
    "                \n",
    "            # [batch_size_image, num_group, height, width]\n",
    "            grouping = get_grouping_from_attentions(attentions, pixel_values.shape[2:])\n",
    "            print(pixel_values.shape)\n",
    "            print(pixel_values.shape[2:])\n",
    "            print('grouping_01', grouping.shape, type(grouping)) # torch.Size([1, 8, 224, 224]) <class 'torch.Tensor'>\n",
    "            seg_logits = grouping\n",
    "\n",
    "            # # normalized features\n",
    "            # image_group_embeds = image_group_embeds / image_group_embeds.norm(dim=-1, keepdim=True)\n",
    "            # print('image_group_embeds_03', image_group_embeds.shape, type(image_group_embeds)) # torch.Size([8, 256]) <class 'torch.Tensor'>\n",
    "\n",
    "            # # [batch_size_image x num_group, batch_size_text]\n",
    "            # logits_per_image_group = torch.matmul(image_group_embeds, text_embeds.t()) * logit_scale\n",
    "            # print('logits_per_image_group_01', logits_per_image_group.shape, type(logits_per_image_group)) # torch.Size([8, 3]) <class 'torch.Tensor'>\n",
    "\n",
    "            # # [batch_size_image, batch_size_text, num_group]\n",
    "            # logits_per_image_group = logits_per_image_group.reshape(\n",
    "            #     image_embeds.shape[0], -1, text_embeds.shape[0]\n",
    "            # ).permute(0, 2, 1)\n",
    "            # print('logits_per_image_group_02', logits_per_image_group.shape, type(logits_per_image_group)) # torch.Size([1, 3, 8]) <class 'torch.Tensor'>\n",
    "\n",
    "\n",
    "            # # [batch_size_image, batch_size_text, height x width]\n",
    "            # flatten_grouping = grouping.reshape(grouping.shape[0], grouping.shape[1], -1)\n",
    "            # print('flatten_grouping_01', flatten_grouping.shape, type(flatten_grouping)) # torch.Size([1, 8, 50176]) <class 'torch.Tensor'>\n",
    "\n",
    "\n",
    "            # # [batch_size_image, batch_size_text, height, width]\n",
    "            # seg_logits = torch.matmul(logits_per_image_group, flatten_grouping) * logit_scale\n",
    "            # print('seg_logits_01', seg_logits.shape, type(seg_logits)) # torch.Size([1, 3, 50176]) <class 'torch.Tensor'>\n",
    "\n",
    "            # seg_logits = seg_logits.reshape(\n",
    "            #     seg_logits.shape[0], seg_logits.shape[1], grouping.shape[2], grouping.shape[3]\n",
    "            # )\n",
    "            # print('seg_logits_02', seg_logits.shape, type(seg_logits)) # torch.Size([1, 3, 224, 224]) <class 'torch.Tensor'>\n",
    "\n",
    "        loss = None\n",
    "        if return_loss:\n",
    "            loss = groupvit_loss(logits_per_text)\n",
    "\n",
    "        if not return_dict:\n",
    "            if seg_logits is not None:\n",
    "                output = (\n",
    "                    logits_per_image,\n",
    "                    logits_per_text,\n",
    "                    seg_logits,\n",
    "                    text_embeds,\n",
    "                    image_embeds,\n",
    "                    text_outputs,\n",
    "                    vision_outputs,\n",
    "                )\n",
    "            else:\n",
    "                output = (logits_per_image, logits_per_text, text_embeds, image_embeds, text_outputs, vision_outputs)\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return GroupViTModelOutput(\n",
    "            loss=loss,\n",
    "            # logits_per_image=logits_per_image,\n",
    "            # logits_per_text=logits_per_text,\n",
    "            segmentation_logits=seg_logits,\n",
    "            # text_embeds=text_embeds,\n",
    "            image_embeds=image_embeds,\n",
    "            # text_model_output=text_outputs,\n",
    "            vision_model_output=vision_outputs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuwerq7N9s5S"
   },
   "source": [
    "### Set Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1677512218962,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "dvuxcmejkKt8",
    "outputId": "4e6311d9-6fd0-43ed-c40a-e5541e35f105"
   },
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description='Train MoCo on CIFAR-10')\n",
    "\n",
    "# parser.add_argument('--finetune_data_dir', default=os.path.join(ROOT_PATH, '../lab_06', 'fine-tune_set', 'siim-acr-pneumothorax'))\n",
    "# parser.add_argument('--pretrain_data_dir', default=os.path.join(ROOT_PATH, '../lab_05', 'unlabel_pre-training_set'))\n",
    "\n",
    "# parser.add_argument('--image_size', default=256, type=int)\n",
    "\n",
    "# parser.add_argument('--lr', '--learning-rate', default=0.06, type=float, metavar='LR', help='initial learning rate', dest='lr')\n",
    "# parser.add_argument('--epochs', default=200, type=int, metavar='N', help='number of total epochs to run')\n",
    "# parser.add_argument('--batch-size', default=4, type=int, metavar='N', help='mini-batch size')\n",
    "\n",
    "# '''\n",
    "# args = parser.parse_args()  # running in command line\n",
    "# '''\n",
    "# args = parser.parse_args('')  # running in ipynb\n",
    "\n",
    "\n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygQeHtsngrC8"
   },
   "source": [
    "### Dataset Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_length_decode(rle, height=1024, width=1024, fill_value=1):\n",
    "    component = np.zeros((height, width), np.float32)\n",
    "    component = component.reshape(-1)\n",
    "    rle = np.array([int(s) for s in rle.strip().split(' ')])\n",
    "    rle = rle.reshape(-1, 2)\n",
    "    start = 0\n",
    "    for index, length in rle:\n",
    "        start = start+index\n",
    "        end = start+length\n",
    "        component[start: end] = fill_value\n",
    "        start = end\n",
    "    component = component.reshape(width, height).T\n",
    "    return component\n",
    "\n",
    "def run_length_encode(component):\n",
    "    component = component.T.flatten()\n",
    "    start = np.where(component[1:] > component[:-1])[0]+1\n",
    "    end = np.where(component[:-1] > component[1:])[0]+1\n",
    "    length = end-start\n",
    "    rle = []\n",
    "    for i in range(len(length)):\n",
    "        if i == 0:\n",
    "            rle.extend([start[0], length[0]])\n",
    "        else:\n",
    "            rle.extend([start[i]-end[i-1], length[i]])\n",
    "    rle = ' '.join([str(r) for r in rle])\n",
    "    return rle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(phase, size, mean, std):\n",
    "    list_transforms = []\n",
    "    if phase == \"train\":\n",
    "        list_transforms.extend(\n",
    "            [\n",
    "#                 HorizontalFlip(),\n",
    "                ShiftScaleRotate(\n",
    "                    shift_limit=0,  # no resizing\n",
    "                    scale_limit=0.1,\n",
    "                    rotate_limit=10, # rotate\n",
    "                    p=0.5,\n",
    "                    border_mode=cv2.BORDER_CONSTANT\n",
    "                ),\n",
    "#                 GaussNoise(),\n",
    "            ]\n",
    "        )\n",
    "    list_transforms.extend(\n",
    "        [\n",
    "            Resize(size, size),\n",
    "            Normalize(mean=mean, std=std, p=1),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    list_trfms = Compose(list_transforms)\n",
    "    return list_trfms\n",
    "\n",
    "def provider(\n",
    "    fold,\n",
    "    total_folds,\n",
    "    data_folder,\n",
    "    df_path,\n",
    "    phase,\n",
    "    size,\n",
    "    mean=None,\n",
    "    std=None,\n",
    "    batch_size=8,\n",
    "    num_workers=4,\n",
    "):\n",
    "    df_all = pd.read_csv(df_path)\n",
    "    df = df_all.drop_duplicates('ImageId')\n",
    "    df_with_mask = df[df[\" EncodedPixels\"] != \" -1\"]\n",
    "    df_with_mask['has_mask'] = 1\n",
    "    df_without_mask = df[df[\" EncodedPixels\"] == \" -1\"]\n",
    "    df_without_mask['has_mask'] = 0\n",
    "    df_without_mask_sampled = df_without_mask.sample(len(df_with_mask), random_state=69) # random state is imp\n",
    "    df = pd.concat([df_with_mask, df_without_mask_sampled])\n",
    "    \n",
    "    #NOTE: equal number of positive and negative cases are chosen.\n",
    "    \n",
    "    kfold = StratifiedKFold(total_folds, shuffle=True, random_state=69)\n",
    "    train_idx, val_idx = list(kfold.split(df[\"ImageId\"], df[\"has_mask\"]))[fold]\n",
    "    train_df, val_df = df.iloc[train_idx], df.iloc[val_idx]\n",
    "    df = train_df if phase == \"train\" else val_df\n",
    "    # NOTE: total_folds=5 -> train/val : 80%/20%\n",
    "    \n",
    "    fnames = df['ImageId'].values\n",
    "    \n",
    "    image_dataset = SIIMDataset(df_all, fnames, data_folder, size, mean, std, phase)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        image_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,\n",
    "        generator=torch.Generator(device='cuda')\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get from path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader = provider(\n",
    "#     fold=0,\n",
    "#     total_folds=5,\n",
    "#     data_folder=data_folder,\n",
    "#     df_path=train_rle_path,\n",
    "#     phase=\"train\",\n",
    "#     size=512,\n",
    "#     mean = (0.485, 0.456, 0.406),\n",
    "#     std = (0.229, 0.224, 0.225),\n",
    "#     batch_size=16,\n",
    "#     num_workers=4,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(dataloader)) # get a batch from the dataloader\n",
    "# images, masks = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot some random images in the `batch`\n",
    "# idx = random.choice(range(16))\n",
    "# plt.imshow(np.asarray(images[idx]).transpose(1, 2, 0))\n",
    "# plt.imshow(masks[idx][0], alpha=0.2, cmap='Reds')\n",
    "# plt.show()\n",
    "# if len(np.unique(masks[idx][0])) == 1: # only zeros\n",
    "#     print('Chosen image has no ground truth mask, rerun the cell')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set dataset & loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SIIMDataset(Dataset):\n",
    "    def __init__(self, df, fnames, data_folder, size, mean, std, phase):\n",
    "        self.df = df\n",
    "        self.root = data_folder\n",
    "        self.size = size\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.phase = phase\n",
    "        self.transforms = get_transforms(phase, size, mean, std)\n",
    "        self.gb = self.df.groupby('ImageId')\n",
    "        self.fnames = fnames\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.fnames[idx]\n",
    "        df = self.gb.get_group(image_id)\n",
    "        annotations = df[' EncodedPixels'].tolist()\n",
    "        image_path = os.path.join(self.root, 'segmix_' + image_id + \".png\")\n",
    "        image = cv2.imread(image_path)\n",
    "        mask = np.zeros([1024, 1024])\n",
    "        if annotations[0] != ' -1':\n",
    "            for rle in annotations:\n",
    "                mask += run_length_decode(rle)\n",
    "        mask = (mask >= 1).astype('float32') # for overlap cases\n",
    "        augmented = self.transforms(image=image, mask=mask)\n",
    "        image = augmented['image']\n",
    "        mask = augmented['mask']\n",
    "        return image, mask.unsqueeze(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsAVAtRoiBbG"
   },
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define base encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(\"resnet34\", encoder_weights=\"imagenet\", activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ModelBase(configuration=configuration)\n",
    "\n",
    "# t = torch.randn((32,3,512,512))\n",
    "# print(t.shape)\n",
    "# print(model(t).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define MoCo wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YXcpXBwi8KV"
   },
   "source": [
    "### Define train/test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(input, target):\n",
    "    input = torch.sigmoid(input)\n",
    "    smooth = 1.0\n",
    "    iflat = input.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    return ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if not (target.size() == input.size()):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n",
    "                             .format(target.size(), input.size()))\n",
    "        max_val = (-input).clamp(min=0)\n",
    "        loss = input - input * target + max_val + \\\n",
    "            ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class MixedLoss(nn.Module):\n",
    "    def __init__(self, alpha, gamma):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.focal = FocalLoss(gamma)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        loss = self.alpha*self.focal(input, target) - torch.log(dice_loss(input, target))\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define train & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, threshold):\n",
    "    X_p = np.copy(X)\n",
    "    preds = (X_p > threshold).astype('uint8')\n",
    "    return preds\n",
    "\n",
    "def metric(probability, truth, threshold=0.5, reduction='none'):\n",
    "    '''Calculates dice of positive and negative images seperately'''\n",
    "    '''probability and truth must be torch tensors'''\n",
    "    batch_size = len(truth)\n",
    "    with torch.no_grad():\n",
    "        probability = probability.view(batch_size, -1)\n",
    "        truth = truth.view(batch_size, -1)\n",
    "        assert(probability.shape == truth.shape)\n",
    "\n",
    "        p = (probability > threshold).float()\n",
    "        t = (truth > 0.5).float()\n",
    "\n",
    "        t_sum = t.sum(-1)\n",
    "        p_sum = p.sum(-1)\n",
    "        neg_index = torch.nonzero(t_sum == 0)\n",
    "        pos_index = torch.nonzero(t_sum >= 1)\n",
    "\n",
    "        dice_neg = (p_sum == 0).float()\n",
    "        dice_pos = 2 * (p*t).sum(-1)/((p+t).sum(-1))\n",
    "\n",
    "        dice_neg = dice_neg[neg_index]\n",
    "        dice_pos = dice_pos[pos_index]\n",
    "        dice = torch.cat([dice_pos, dice_neg])\n",
    "\n",
    "#         dice_neg = np.nan_to_num(dice_neg.mean().item(), 0)\n",
    "#         dice_pos = np.nan_to_num(dice_pos.mean().item(), 0)\n",
    "#         dice = dice.mean().item()\n",
    "\n",
    "        num_neg = len(neg_index)\n",
    "        num_pos = len(pos_index)\n",
    "\n",
    "    return dice, dice_neg, dice_pos, num_neg, num_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Meter:\n",
    "    '''A meter to keep track of iou and dice scores throughout an epoch'''\n",
    "    def __init__(self, phase, epoch):\n",
    "        self.base_threshold = 0.5 # <<<<<<<<<<< here's the threshold\n",
    "        self.base_dice_scores = []\n",
    "        self.dice_neg_scores = []\n",
    "        self.dice_pos_scores = []\n",
    "        self.iou_scores = []\n",
    "\n",
    "    def update(self, targets, outputs):\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        dice, dice_neg, dice_pos, _, _ = metric(probs, targets, self.base_threshold)\n",
    "        self.base_dice_scores.extend(dice)\n",
    "        self.dice_pos_scores.extend(dice_pos)\n",
    "        self.dice_neg_scores.extend(dice_neg)\n",
    "        preds = predict(probs, self.base_threshold)\n",
    "        iou = compute_iou_batch(preds, targets, classes=[1])\n",
    "        self.iou_scores.append(iou)\n",
    "\n",
    "    def get_metrics(self):\n",
    "        dice = np.nanmean(self.base_dice_scores)\n",
    "        dice_neg = np.nanmean(self.dice_neg_scores)\n",
    "        dice_pos = np.nanmean(self.dice_pos_scores)\n",
    "        dices = [dice, dice_neg, dice_pos]\n",
    "        iou = np.nanmean(self.iou_scores)\n",
    "        return dices, iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_log(phase, epoch, epoch_loss, meter, start):\n",
    "    '''logging the metrics at the end of an epoch'''\n",
    "    dices, iou = meter.get_metrics()\n",
    "    dice, dice_neg, dice_pos = dices\n",
    "    print(\"Loss: %0.4f | dice: %0.4f | dice_neg: %0.4f | dice_pos: %0.4f | IoU: %0.4f\" % (epoch_loss, dice, dice_neg, dice_pos, iou))\n",
    "    return dice, iou\n",
    "\n",
    "def compute_ious(pred, label, classes, ignore_index=255, only_present=True):\n",
    "    '''computes iou for one ground truth mask and predicted mask'''\n",
    "    pred[label == ignore_index] = 0\n",
    "    ious = []\n",
    "    for c in classes:\n",
    "        label_c = label == c\n",
    "        if only_present and np.sum(label_c) == 0:\n",
    "            ious.append(np.nan)\n",
    "            continue\n",
    "        pred_c = pred == c\n",
    "        intersection = np.logical_and(pred_c, label_c).sum()\n",
    "        union = np.logical_or(pred_c, label_c).sum()\n",
    "        if union != 0:\n",
    "            ious.append(intersection / union)\n",
    "    return ious if ious else [1]\n",
    "\n",
    "\n",
    "def compute_iou_batch(outputs, labels, classes=None):\n",
    "    '''computes mean iou for a batch of ground truth masks and predicted masks'''\n",
    "    ious = []\n",
    "    preds = np.copy(outputs) # copy is imp\n",
    "    labels = np.array(labels) # tensor to np\n",
    "    for pred, label in zip(preds, labels):\n",
    "        ious.append(np.nanmean(compute_ious(pred, label, classes)))\n",
    "    iou = np.nanmean(ious)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    '''This class takes care of training and validation of our model'''\n",
    "    def __init__(self, model):\n",
    "        self.fold = 1\n",
    "        self.total_folds = 5\n",
    "        self.num_workers = 6\n",
    "        self.batch_size = {\"train\": 8, \"val\": 4}\n",
    "        self.accumulation_steps = 32 // self.batch_size['train']\n",
    "        self.lr = 5e-4\n",
    "        self.num_epochs = 100\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.phases = [\"train\", \"val\"]\n",
    "        self.device = torch.device(\"cuda:0\")\n",
    "        torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "        self.net = model\n",
    "        self.criterion = MixedLoss(10.0, 2.0)\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode=\"min\", patience=3, verbose=True)\n",
    "        self.net = self.net.to(self.device)\n",
    "        cudnn.benchmark = True\n",
    "        self.dataloaders = {\n",
    "            phase: provider(\n",
    "                fold=1,\n",
    "                total_folds=5,\n",
    "                data_folder=data_folder,\n",
    "                df_path=train_rle_path,\n",
    "                phase=phase,\n",
    "                size=1024,\n",
    "                mean=(0.485, 0.456, 0.406),\n",
    "                std=(0.229, 0.224, 0.225),\n",
    "                batch_size=self.batch_size[phase],\n",
    "                num_workers=self.num_workers,\n",
    "            )\n",
    "            for phase in self.phases\n",
    "        }\n",
    "        self.losses = {phase: [] for phase in self.phases}\n",
    "        self.iou_scores = {phase: [] for phase in self.phases}\n",
    "        self.dice_scores = {phase: [] for phase in self.phases}\n",
    "        \n",
    "    def forward(self, images, targets):\n",
    "        images = images.to(self.device)\n",
    "        masks = targets.to(self.device)\n",
    "        outputs = self.net(images)\n",
    "        loss = self.criterion(outputs, masks)\n",
    "        return loss, outputs\n",
    "\n",
    "    def iterate(self, epoch, phase):\n",
    "        meter = Meter(phase, epoch)\n",
    "        start = time.strftime(\"%H:%M:%S\")\n",
    "        print(f\"Starting epoch: {epoch} | phase: {phase} | ⏰: {start}\")\n",
    "        batch_size = self.batch_size[phase]\n",
    "        self.net.train(phase == \"train\")\n",
    "        dataloader = self.dataloaders[phase]\n",
    "        running_loss = 0.0\n",
    "        total_batches = len(dataloader)\n",
    "#         tk0 = tqdm(dataloader, total=total_batches)\n",
    "        self.optimizer.zero_grad()\n",
    "        for itr, batch in enumerate(dataloader):\n",
    "            images, targets = batch\n",
    "            loss, outputs = self.forward(images, targets)\n",
    "            loss = loss / self.accumulation_steps\n",
    "            if phase == \"train\":\n",
    "                loss.backward()\n",
    "                if (itr + 1 ) % self.accumulation_steps == 0:\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "            running_loss += loss.item()\n",
    "            outputs = outputs.detach().cpu()\n",
    "            meter.update(targets, outputs)\n",
    "#             tk0.set_postfix(loss=(running_loss / ((itr + 1))))\n",
    "        epoch_loss = (running_loss * self.accumulation_steps) / total_batches\n",
    "        dice, iou = epoch_log(phase, epoch, epoch_loss, meter, start)\n",
    "        self.losses[phase].append(epoch_loss)\n",
    "        self.dice_scores[phase].append(dice)\n",
    "        self.iou_scores[phase].append(iou)\n",
    "        torch.cuda.empty_cache()\n",
    "        return epoch_loss\n",
    "\n",
    "    def start(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.iterate(epoch, \"train\")\n",
    "            state = {\n",
    "                \"epoch\": epoch,\n",
    "                \"best_loss\": self.best_loss,\n",
    "                \"state_dict\": self.net.state_dict(),\n",
    "                \"optimizer\": self.optimizer.state_dict(),\n",
    "            }\n",
    "            val_loss = self.iterate(epoch, \"val\")\n",
    "            self.scheduler.step(val_loss)\n",
    "            if val_loss < self.best_loss:\n",
    "                print(\"******** New optimal found, saving state ********\")\n",
    "                state[\"best_loss\"] = self.best_loss = val_loss\n",
    "                torch.save(state, \"./model.pth\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86lHkiKox3KO"
   },
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0 | phase: train | ⏰: 17:29:25\n",
      "Loss: 3.6010 | dice: 0.1417 | dice_neg: 0.0321 | dice_pos: 0.2514 | IoU: 0.1609\n",
      "Starting epoch: 0 | phase: val | ⏰: 17:37:02\n",
      "Loss: 3.2117 | dice: 0.2173 | dice_neg: 0.1324 | dice_pos: 0.3022 | IoU: 0.1965\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 1 | phase: train | ⏰: 17:38:08\n",
      "Loss: 2.1148 | dice: 0.2288 | dice_neg: 0.1445 | dice_pos: 0.3132 | IoU: 0.2098\n",
      "Starting epoch: 1 | phase: val | ⏰: 17:45:23\n",
      "Loss: 2.6236 | dice: 0.2638 | dice_neg: 0.2269 | dice_pos: 0.3006 | IoU: 0.2059\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 2 | phase: train | ⏰: 17:46:29\n",
      "Loss: 1.6878 | dice: 0.3301 | dice_neg: 0.3269 | dice_pos: 0.3334 | IoU: 0.2338\n",
      "Starting epoch: 2 | phase: val | ⏰: 17:53:49\n",
      "Loss: 2.3403 | dice: 0.3166 | dice_neg: 0.2857 | dice_pos: 0.3474 | IoU: 0.2398\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 3 | phase: train | ⏰: 17:54:54\n",
      "Loss: 1.6390 | dice: 0.3242 | dice_neg: 0.3174 | dice_pos: 0.3309 | IoU: 0.2323\n",
      "Starting epoch: 3 | phase: val | ⏰: 18:02:20\n",
      "Loss: 2.6077 | dice: 0.3858 | dice_neg: 0.5210 | dice_pos: 0.2506 | IoU: 0.1743\n",
      "\n",
      "Starting epoch: 4 | phase: train | ⏰: 18:03:25\n",
      "Loss: 1.3856 | dice: 0.4214 | dice_neg: 0.4924 | dice_pos: 0.3503 | IoU: 0.2546\n",
      "Starting epoch: 4 | phase: val | ⏰: 18:10:49\n",
      "Loss: 2.1289 | dice: 0.4525 | dice_neg: 0.5378 | dice_pos: 0.3672 | IoU: 0.2718\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 5 | phase: train | ⏰: 18:11:57\n",
      "Loss: 1.3097 | dice: 0.4481 | dice_neg: 0.5276 | dice_pos: 0.3686 | IoU: 0.2698\n",
      "Starting epoch: 5 | phase: val | ⏰: 18:19:24\n",
      "Loss: 2.2247 | dice: 0.5866 | dice_neg: 0.8592 | dice_pos: 0.3140 | IoU: 0.2294\n",
      "\n",
      "Starting epoch: 6 | phase: train | ⏰: 18:20:32\n",
      "Loss: 1.3322 | dice: 0.4424 | dice_neg: 0.5181 | dice_pos: 0.3667 | IoU: 0.2690\n",
      "Starting epoch: 6 | phase: val | ⏰: 18:28:01\n",
      "Loss: 2.1362 | dice: 0.5189 | dice_neg: 0.6681 | dice_pos: 0.3697 | IoU: 0.2687\n",
      "\n",
      "Starting epoch: 7 | phase: train | ⏰: 18:29:07\n",
      "Loss: 1.2984 | dice: 0.4704 | dice_neg: 0.5575 | dice_pos: 0.3832 | IoU: 0.2801\n",
      "Starting epoch: 7 | phase: val | ⏰: 18:36:28\n",
      "Loss: 2.2473 | dice: 0.5875 | dice_neg: 0.8761 | dice_pos: 0.2990 | IoU: 0.2238\n",
      "\n",
      "Starting epoch: 8 | phase: train | ⏰: 18:37:35\n",
      "Loss: 1.1075 | dice: 0.5274 | dice_neg: 0.6458 | dice_pos: 0.4089 | IoU: 0.3077\n",
      "Starting epoch: 8 | phase: val | ⏰: 18:44:59\n",
      "Loss: 2.1989 | dice: 0.5891 | dice_neg: 0.8109 | dice_pos: 0.3673 | IoU: 0.2738\n",
      "Epoch     9: reducing learning rate of group 0 to 5.0000e-05.\n",
      "\n",
      "Starting epoch: 9 | phase: train | ⏰: 18:46:04\n",
      "Loss: 0.9573 | dice: 0.5801 | dice_neg: 0.7236 | dice_pos: 0.4365 | IoU: 0.3332\n",
      "Starting epoch: 9 | phase: val | ⏰: 18:53:32\n",
      "Loss: 2.0000 | dice: 0.6125 | dice_neg: 0.8761 | dice_pos: 0.3490 | IoU: 0.2575\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 10 | phase: train | ⏰: 18:54:37\n",
      "Loss: 1.0091 | dice: 0.5562 | dice_neg: 0.6590 | dice_pos: 0.4535 | IoU: 0.3441\n",
      "Starting epoch: 10 | phase: val | ⏰: 19:02:01\n",
      "Loss: 1.8816 | dice: 0.6058 | dice_neg: 0.8340 | dice_pos: 0.3775 | IoU: 0.2707\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 11 | phase: train | ⏰: 19:03:07\n",
      "Loss: 0.9351 | dice: 0.6039 | dice_neg: 0.7478 | dice_pos: 0.4600 | IoU: 0.3514\n",
      "Starting epoch: 11 | phase: val | ⏰: 19:10:34\n",
      "Loss: 1.9569 | dice: 0.6108 | dice_neg: 0.8235 | dice_pos: 0.3982 | IoU: 0.2912\n",
      "\n",
      "Starting epoch: 12 | phase: train | ⏰: 19:11:39\n",
      "Loss: 0.9150 | dice: 0.6327 | dice_neg: 0.7877 | dice_pos: 0.4777 | IoU: 0.3684\n",
      "Starting epoch: 12 | phase: val | ⏰: 19:19:01\n",
      "Loss: 1.7664 | dice: 0.6127 | dice_neg: 0.8025 | dice_pos: 0.4229 | IoU: 0.3107\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 13 | phase: train | ⏰: 19:20:07\n",
      "Loss: 0.8635 | dice: 0.6508 | dice_neg: 0.8245 | dice_pos: 0.4770 | IoU: 0.3683\n",
      "Starting epoch: 13 | phase: val | ⏰: 19:27:33\n",
      "Loss: 1.7908 | dice: 0.6257 | dice_neg: 0.8340 | dice_pos: 0.4173 | IoU: 0.3144\n",
      "\n",
      "Starting epoch: 14 | phase: train | ⏰: 19:28:39\n",
      "Loss: 0.8741 | dice: 0.6380 | dice_neg: 0.7882 | dice_pos: 0.4878 | IoU: 0.3770\n",
      "Starting epoch: 14 | phase: val | ⏰: 19:36:03\n",
      "Loss: 1.8255 | dice: 0.6296 | dice_neg: 0.8256 | dice_pos: 0.4336 | IoU: 0.3307\n",
      "\n",
      "Starting epoch: 15 | phase: train | ⏰: 19:37:09\n",
      "Loss: 0.8107 | dice: 0.6516 | dice_neg: 0.8098 | dice_pos: 0.4934 | IoU: 0.3802\n",
      "Starting epoch: 15 | phase: val | ⏰: 19:44:35\n",
      "Loss: 1.7234 | dice: 0.6043 | dice_neg: 0.7563 | dice_pos: 0.4522 | IoU: 0.3308\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 16 | phase: train | ⏰: 19:45:41\n",
      "Loss: 0.8337 | dice: 0.6484 | dice_neg: 0.7909 | dice_pos: 0.5060 | IoU: 0.3922\n",
      "Starting epoch: 16 | phase: val | ⏰: 19:53:04\n",
      "Loss: 1.5898 | dice: 0.6155 | dice_neg: 0.7962 | dice_pos: 0.4348 | IoU: 0.3340\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 17 | phase: train | ⏰: 19:54:14\n",
      "Loss: 0.8437 | dice: 0.6666 | dice_neg: 0.8276 | dice_pos: 0.5055 | IoU: 0.3905\n",
      "Starting epoch: 17 | phase: val | ⏰: 20:01:37\n",
      "Loss: 1.9062 | dice: 0.6347 | dice_neg: 0.8866 | dice_pos: 0.3829 | IoU: 0.2922\n",
      "\n",
      "Starting epoch: 18 | phase: train | ⏰: 20:02:44\n",
      "Loss: 0.8243 | dice: 0.6606 | dice_neg: 0.8114 | dice_pos: 0.5099 | IoU: 0.3978\n",
      "Starting epoch: 18 | phase: val | ⏰: 20:10:08\n",
      "Loss: 1.7526 | dice: 0.5765 | dice_neg: 0.7017 | dice_pos: 0.4513 | IoU: 0.3378\n",
      "\n",
      "Starting epoch: 19 | phase: train | ⏰: 20:11:15\n",
      "Loss: 0.8511 | dice: 0.6537 | dice_neg: 0.7982 | dice_pos: 0.5093 | IoU: 0.3934\n",
      "Starting epoch: 19 | phase: val | ⏰: 20:18:38\n",
      "Loss: 1.8975 | dice: 0.6236 | dice_neg: 0.8235 | dice_pos: 0.4237 | IoU: 0.3135\n",
      "\n",
      "Starting epoch: 20 | phase: train | ⏰: 20:19:43\n",
      "Loss: 0.7736 | dice: 0.6714 | dice_neg: 0.8245 | dice_pos: 0.5183 | IoU: 0.4044\n",
      "Starting epoch: 20 | phase: val | ⏰: 20:27:03\n",
      "Loss: 1.5791 | dice: 0.6367 | dice_neg: 0.8466 | dice_pos: 0.4267 | IoU: 0.3372\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 21 | phase: train | ⏰: 20:28:09\n",
      "Loss: 0.7605 | dice: 0.6859 | dice_neg: 0.8518 | dice_pos: 0.5199 | IoU: 0.4055\n",
      "Starting epoch: 21 | phase: val | ⏰: 20:35:37\n",
      "Loss: 1.7001 | dice: 0.6438 | dice_neg: 0.8824 | dice_pos: 0.4052 | IoU: 0.3025\n",
      "\n",
      "Starting epoch: 22 | phase: train | ⏰: 20:36:43\n",
      "Loss: 0.7759 | dice: 0.6786 | dice_neg: 0.8313 | dice_pos: 0.5258 | IoU: 0.4128\n",
      "Starting epoch: 22 | phase: val | ⏰: 20:44:05\n",
      "Loss: 1.7864 | dice: 0.6285 | dice_neg: 0.8172 | dice_pos: 0.4398 | IoU: 0.3272\n",
      "\n",
      "Starting epoch: 23 | phase: train | ⏰: 20:45:11\n",
      "Loss: 0.7346 | dice: 0.6875 | dice_neg: 0.8466 | dice_pos: 0.5285 | IoU: 0.4143\n",
      "Starting epoch: 23 | phase: val | ⏰: 20:52:40\n",
      "Loss: 1.7988 | dice: 0.6507 | dice_neg: 0.9097 | dice_pos: 0.3918 | IoU: 0.2969\n",
      "\n",
      "Starting epoch: 24 | phase: train | ⏰: 20:53:49\n",
      "Loss: 0.7675 | dice: 0.6799 | dice_neg: 0.8313 | dice_pos: 0.5284 | IoU: 0.4085\n",
      "Starting epoch: 24 | phase: val | ⏰: 21:01:16\n",
      "Loss: 1.6526 | dice: 0.6513 | dice_neg: 0.8887 | dice_pos: 0.4140 | IoU: 0.3182\n",
      "Epoch    25: reducing learning rate of group 0 to 5.0000e-06.\n",
      "\n",
      "Starting epoch: 25 | phase: train | ⏰: 21:02:22\n",
      "Loss: 0.7336 | dice: 0.7046 | dice_neg: 0.8760 | dice_pos: 0.5333 | IoU: 0.4169\n",
      "Starting epoch: 25 | phase: val | ⏰: 21:09:47\n",
      "Loss: 1.6553 | dice: 0.6524 | dice_neg: 0.8845 | dice_pos: 0.4204 | IoU: 0.3221\n",
      "\n",
      "Starting epoch: 26 | phase: train | ⏰: 21:10:51\n",
      "Loss: 0.7052 | dice: 0.7058 | dice_neg: 0.8728 | dice_pos: 0.5388 | IoU: 0.4255\n",
      "Starting epoch: 26 | phase: val | ⏰: 21:18:16\n",
      "Loss: 1.6096 | dice: 0.6561 | dice_neg: 0.8697 | dice_pos: 0.4425 | IoU: 0.3456\n",
      "\n",
      "Starting epoch: 27 | phase: train | ⏰: 21:19:21\n",
      "Loss: 0.6807 | dice: 0.7077 | dice_neg: 0.8728 | dice_pos: 0.5425 | IoU: 0.4307\n",
      "Starting epoch: 27 | phase: val | ⏰: 21:26:46\n",
      "Loss: 1.6804 | dice: 0.6561 | dice_neg: 0.8950 | dice_pos: 0.4172 | IoU: 0.3232\n",
      "\n",
      "Starting epoch: 28 | phase: train | ⏰: 21:27:50\n",
      "Loss: 0.6946 | dice: 0.7107 | dice_neg: 0.8807 | dice_pos: 0.5407 | IoU: 0.4288\n",
      "Starting epoch: 28 | phase: val | ⏰: 21:35:11\n",
      "Loss: 1.6855 | dice: 0.6603 | dice_neg: 0.8887 | dice_pos: 0.4319 | IoU: 0.3266\n",
      "Epoch    29: reducing learning rate of group 0 to 5.0000e-07.\n",
      "\n",
      "Starting epoch: 29 | phase: train | ⏰: 21:36:18\n",
      "Loss: 0.7218 | dice: 0.7118 | dice_neg: 0.8791 | dice_pos: 0.5444 | IoU: 0.4281\n",
      "Starting epoch: 29 | phase: val | ⏰: 21:43:40\n",
      "Loss: 1.7743 | dice: 0.6577 | dice_neg: 0.8866 | dice_pos: 0.4288 | IoU: 0.3225\n",
      "\n",
      "Starting epoch: 30 | phase: train | ⏰: 21:44:47\n",
      "Loss: 0.7031 | dice: 0.7107 | dice_neg: 0.8765 | dice_pos: 0.5449 | IoU: 0.4324\n",
      "Starting epoch: 30 | phase: val | ⏰: 21:52:02\n",
      "Loss: 1.6060 | dice: 0.6584 | dice_neg: 0.8887 | dice_pos: 0.4282 | IoU: 0.3180\n",
      "\n",
      "Starting epoch: 31 | phase: train | ⏰: 21:53:09\n",
      "Loss: 0.7264 | dice: 0.7157 | dice_neg: 0.8875 | dice_pos: 0.5438 | IoU: 0.4284\n",
      "Starting epoch: 31 | phase: val | ⏰: 22:00:22\n",
      "Loss: 1.7142 | dice: 0.6572 | dice_neg: 0.9034 | dice_pos: 0.4111 | IoU: 0.3101\n",
      "\n",
      "Starting epoch: 32 | phase: train | ⏰: 22:01:27\n",
      "Loss: 0.6944 | dice: 0.7199 | dice_neg: 0.8902 | dice_pos: 0.5495 | IoU: 0.4365\n",
      "Starting epoch: 32 | phase: val | ⏰: 22:08:55\n",
      "Loss: 1.7150 | dice: 0.6606 | dice_neg: 0.8971 | dice_pos: 0.4241 | IoU: 0.3210\n",
      "Epoch    33: reducing learning rate of group 0 to 5.0000e-08.\n",
      "\n",
      "Starting epoch: 33 | phase: train | ⏰: 22:10:02\n",
      "Loss: 0.6466 | dice: 0.7177 | dice_neg: 0.8870 | dice_pos: 0.5484 | IoU: 0.4322\n",
      "Starting epoch: 33 | phase: val | ⏰: 22:17:36\n",
      "Loss: 1.7010 | dice: 0.6593 | dice_neg: 0.8992 | dice_pos: 0.4195 | IoU: 0.3202\n",
      "\n",
      "Starting epoch: 34 | phase: train | ⏰: 22:18:39\n",
      "Loss: 0.6471 | dice: 0.7226 | dice_neg: 0.8933 | dice_pos: 0.5519 | IoU: 0.4327\n",
      "Starting epoch: 34 | phase: val | ⏰: 22:26:06\n",
      "Loss: 1.4488 | dice: 0.6588 | dice_neg: 0.8908 | dice_pos: 0.4268 | IoU: 0.3303\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 35 | phase: train | ⏰: 22:27:10\n",
      "Loss: 0.6306 | dice: 0.7210 | dice_neg: 0.8970 | dice_pos: 0.5450 | IoU: 0.4298\n",
      "Starting epoch: 35 | phase: val | ⏰: 22:34:38\n",
      "Loss: 1.8832 | dice: 0.6585 | dice_neg: 0.9013 | dice_pos: 0.4157 | IoU: 0.3080\n",
      "\n",
      "Starting epoch: 36 | phase: train | ⏰: 22:35:45\n",
      "Loss: 0.6679 | dice: 0.7138 | dice_neg: 0.8833 | dice_pos: 0.5443 | IoU: 0.4278\n",
      "Starting epoch: 36 | phase: val | ⏰: 22:43:18\n",
      "Loss: 1.9076 | dice: 0.6566 | dice_neg: 0.9055 | dice_pos: 0.4077 | IoU: 0.3047\n",
      "\n",
      "Starting epoch: 37 | phase: train | ⏰: 22:44:20\n",
      "Loss: 0.6639 | dice: 0.7179 | dice_neg: 0.8896 | dice_pos: 0.5461 | IoU: 0.4322\n",
      "Starting epoch: 37 | phase: val | ⏰: 22:51:50\n",
      "Loss: 1.7493 | dice: 0.6598 | dice_neg: 0.8992 | dice_pos: 0.4205 | IoU: 0.3199\n",
      "\n",
      "Starting epoch: 38 | phase: train | ⏰: 22:53:00\n",
      "Loss: 0.6937 | dice: 0.7087 | dice_neg: 0.8707 | dice_pos: 0.5467 | IoU: 0.4277\n",
      "Starting epoch: 38 | phase: val | ⏰: 23:00:33\n",
      "Loss: 1.5555 | dice: 0.6594 | dice_neg: 0.9055 | dice_pos: 0.4134 | IoU: 0.3172\n",
      "Epoch    39: reducing learning rate of group 0 to 5.0000e-09.\n",
      "\n",
      "Starting epoch: 39 | phase: train | ⏰: 23:01:36\n",
      "Loss: 0.6582 | dice: 0.7217 | dice_neg: 0.8923 | dice_pos: 0.5511 | IoU: 0.4370\n",
      "Starting epoch: 39 | phase: val | ⏰: 23:09:06\n",
      "Loss: 1.6048 | dice: 0.6598 | dice_neg: 0.8824 | dice_pos: 0.4372 | IoU: 0.3378\n",
      "\n",
      "Starting epoch: 40 | phase: train | ⏰: 23:10:11\n",
      "Loss: 0.7294 | dice: 0.7149 | dice_neg: 0.8823 | dice_pos: 0.5474 | IoU: 0.4317\n",
      "Starting epoch: 40 | phase: val | ⏰: 23:17:43\n",
      "Loss: 1.7145 | dice: 0.6581 | dice_neg: 0.8824 | dice_pos: 0.4338 | IoU: 0.3343\n",
      "\n",
      "Starting epoch: 41 | phase: train | ⏰: 23:18:50\n",
      "Loss: 0.6786 | dice: 0.7204 | dice_neg: 0.8913 | dice_pos: 0.5493 | IoU: 0.4313\n",
      "Starting epoch: 41 | phase: val | ⏰: 23:26:21\n",
      "Loss: 1.7761 | dice: 0.6585 | dice_neg: 0.9055 | dice_pos: 0.4115 | IoU: 0.3094\n",
      "\n",
      "Starting epoch: 42 | phase: train | ⏰: 23:27:24\n",
      "Loss: 0.6639 | dice: 0.7152 | dice_neg: 0.8844 | dice_pos: 0.5461 | IoU: 0.4296\n",
      "Starting epoch: 42 | phase: val | ⏰: 23:34:54\n",
      "Loss: 1.6008 | dice: 0.6583 | dice_neg: 0.9013 | dice_pos: 0.4153 | IoU: 0.3192\n",
      "\n",
      "Starting epoch: 43 | phase: train | ⏰: 23:36:00\n",
      "Loss: 0.6631 | dice: 0.7194 | dice_neg: 0.8891 | dice_pos: 0.5497 | IoU: 0.4311\n",
      "Starting epoch: 43 | phase: val | ⏰: 23:43:32\n",
      "Loss: 1.6034 | dice: 0.6590 | dice_neg: 0.9034 | dice_pos: 0.4146 | IoU: 0.3222\n",
      "\n",
      "Starting epoch: 44 | phase: train | ⏰: 23:44:38\n",
      "Loss: 0.7167 | dice: 0.7138 | dice_neg: 0.8807 | dice_pos: 0.5469 | IoU: 0.4317\n",
      "Starting epoch: 44 | phase: val | ⏰: 23:51:55\n",
      "Loss: 1.8362 | dice: 0.6573 | dice_neg: 0.9013 | dice_pos: 0.4132 | IoU: 0.3119\n",
      "\n",
      "Starting epoch: 45 | phase: train | ⏰: 23:52:59\n",
      "Loss: 0.6766 | dice: 0.7235 | dice_neg: 0.8960 | dice_pos: 0.5510 | IoU: 0.4322\n",
      "Starting epoch: 45 | phase: val | ⏰: 00:00:32\n",
      "Loss: 1.7121 | dice: 0.6582 | dice_neg: 0.8824 | dice_pos: 0.4341 | IoU: 0.3295\n",
      "\n",
      "Starting epoch: 46 | phase: train | ⏰: 00:01:35\n",
      "Loss: 0.7375 | dice: 0.7178 | dice_neg: 0.8881 | dice_pos: 0.5474 | IoU: 0.4273\n",
      "Starting epoch: 46 | phase: val | ⏰: 00:09:07\n",
      "Loss: 1.9040 | dice: 0.6601 | dice_neg: 0.8887 | dice_pos: 0.4316 | IoU: 0.3296\n",
      "\n",
      "Starting epoch: 47 | phase: train | ⏰: 00:10:14\n",
      "Loss: 0.6485 | dice: 0.7216 | dice_neg: 0.8928 | dice_pos: 0.5504 | IoU: 0.4369\n",
      "Starting epoch: 47 | phase: val | ⏰: 00:17:44\n",
      "Loss: 1.7685 | dice: 0.6598 | dice_neg: 0.8887 | dice_pos: 0.4310 | IoU: 0.3149\n",
      "\n",
      "Starting epoch: 48 | phase: train | ⏰: 00:18:51\n",
      "Loss: 0.7253 | dice: 0.7206 | dice_neg: 0.8912 | dice_pos: 0.5501 | IoU: 0.4334\n",
      "Starting epoch: 48 | phase: val | ⏰: 00:26:07\n",
      "Loss: 1.7226 | dice: 0.6560 | dice_neg: 0.9034 | dice_pos: 0.4087 | IoU: 0.3133\n",
      "\n",
      "Starting epoch: 49 | phase: train | ⏰: 00:27:09\n",
      "Loss: 0.7108 | dice: 0.7139 | dice_neg: 0.8812 | dice_pos: 0.5465 | IoU: 0.4281\n",
      "Starting epoch: 49 | phase: val | ⏰: 00:34:41\n",
      "Loss: 1.8723 | dice: 0.6586 | dice_neg: 0.9034 | dice_pos: 0.4139 | IoU: 0.3127\n",
      "\n",
      "Starting epoch: 50 | phase: train | ⏰: 00:35:47\n",
      "Loss: 0.6858 | dice: 0.7098 | dice_neg: 0.8744 | dice_pos: 0.5452 | IoU: 0.4296\n",
      "Starting epoch: 50 | phase: val | ⏰: 00:43:00\n",
      "Loss: 1.5943 | dice: 0.6600 | dice_neg: 0.8971 | dice_pos: 0.4229 | IoU: 0.3183\n",
      "\n",
      "Starting epoch: 51 | phase: train | ⏰: 00:44:06\n",
      "Loss: 0.6714 | dice: 0.7161 | dice_neg: 0.8839 | dice_pos: 0.5484 | IoU: 0.4305\n",
      "Starting epoch: 51 | phase: val | ⏰: 00:51:30\n",
      "Loss: 1.7880 | dice: 0.6589 | dice_neg: 0.8824 | dice_pos: 0.4354 | IoU: 0.3354\n",
      "\n",
      "Starting epoch: 52 | phase: train | ⏰: 00:52:32\n",
      "Loss: 0.7083 | dice: 0.7155 | dice_neg: 0.8854 | dice_pos: 0.5455 | IoU: 0.4282\n",
      "Starting epoch: 52 | phase: val | ⏰: 01:00:00\n",
      "Loss: 1.7879 | dice: 0.6571 | dice_neg: 0.8992 | dice_pos: 0.4151 | IoU: 0.3241\n",
      "\n",
      "Starting epoch: 53 | phase: train | ⏰: 01:01:05\n",
      "Loss: 0.6657 | dice: 0.7124 | dice_neg: 0.8765 | dice_pos: 0.5484 | IoU: 0.4331\n",
      "Starting epoch: 53 | phase: val | ⏰: 01:08:38\n",
      "Loss: 1.7272 | dice: 0.6590 | dice_neg: 0.8971 | dice_pos: 0.4210 | IoU: 0.3194\n",
      "\n",
      "Starting epoch: 54 | phase: train | ⏰: 01:09:44\n",
      "Loss: 0.7199 | dice: 0.7121 | dice_neg: 0.8760 | dice_pos: 0.5483 | IoU: 0.4312\n",
      "Starting epoch: 54 | phase: val | ⏰: 01:17:15\n",
      "Loss: 1.6460 | dice: 0.6609 | dice_neg: 0.8929 | dice_pos: 0.4289 | IoU: 0.3258\n",
      "\n",
      "Starting epoch: 55 | phase: train | ⏰: 01:18:22\n",
      "Loss: 0.7171 | dice: 0.7124 | dice_neg: 0.8770 | dice_pos: 0.5477 | IoU: 0.4305\n",
      "Starting epoch: 55 | phase: val | ⏰: 01:25:54\n",
      "Loss: 1.6426 | dice: 0.6580 | dice_neg: 0.8845 | dice_pos: 0.4315 | IoU: 0.3238\n",
      "\n",
      "Starting epoch: 56 | phase: train | ⏰: 01:27:02\n",
      "Loss: 0.6958 | dice: 0.7177 | dice_neg: 0.8870 | dice_pos: 0.5484 | IoU: 0.4319\n",
      "Starting epoch: 56 | phase: val | ⏰: 01:34:36\n",
      "Loss: 1.7083 | dice: 0.6605 | dice_neg: 0.8950 | dice_pos: 0.4260 | IoU: 0.3285\n",
      "\n",
      "Starting epoch: 57 | phase: train | ⏰: 01:35:41\n",
      "Loss: 0.6660 | dice: 0.7206 | dice_neg: 0.8928 | dice_pos: 0.5484 | IoU: 0.4329\n",
      "Starting epoch: 57 | phase: val | ⏰: 01:43:15\n",
      "Loss: 1.7521 | dice: 0.6606 | dice_neg: 0.8824 | dice_pos: 0.4388 | IoU: 0.3292\n",
      "\n",
      "Starting epoch: 58 | phase: train | ⏰: 01:44:21\n",
      "Loss: 0.6815 | dice: 0.7208 | dice_neg: 0.8965 | dice_pos: 0.5451 | IoU: 0.4329\n",
      "Starting epoch: 58 | phase: val | ⏰: 01:51:48\n",
      "Loss: 1.6162 | dice: 0.6585 | dice_neg: 0.8887 | dice_pos: 0.4283 | IoU: 0.3279\n",
      "\n",
      "Starting epoch: 59 | phase: train | ⏰: 01:52:52\n",
      "Loss: 0.6891 | dice: 0.7169 | dice_neg: 0.8865 | dice_pos: 0.5473 | IoU: 0.4336\n",
      "Starting epoch: 59 | phase: val | ⏰: 02:00:19\n",
      "Loss: 1.6938 | dice: 0.6568 | dice_neg: 0.9034 | dice_pos: 0.4101 | IoU: 0.3160\n",
      "\n",
      "Starting epoch: 60 | phase: train | ⏰: 02:01:25\n",
      "Loss: 0.6855 | dice: 0.7204 | dice_neg: 0.8933 | dice_pos: 0.5476 | IoU: 0.4312\n",
      "Starting epoch: 60 | phase: val | ⏰: 02:08:55\n",
      "Loss: 1.7613 | dice: 0.6564 | dice_neg: 0.8971 | dice_pos: 0.4158 | IoU: 0.3163\n",
      "\n",
      "Starting epoch: 61 | phase: train | ⏰: 02:10:00\n",
      "Loss: 0.6906 | dice: 0.7096 | dice_neg: 0.8797 | dice_pos: 0.5395 | IoU: 0.4298\n",
      "Starting epoch: 61 | phase: val | ⏰: 02:17:28\n",
      "Loss: 1.7499 | dice: 0.6579 | dice_neg: 0.8971 | dice_pos: 0.4187 | IoU: 0.3109\n",
      "\n",
      "Starting epoch: 62 | phase: train | ⏰: 02:18:34\n",
      "Loss: 0.6961 | dice: 0.7103 | dice_neg: 0.8749 | dice_pos: 0.5458 | IoU: 0.4237\n",
      "Starting epoch: 62 | phase: val | ⏰: 02:26:10\n",
      "Loss: 1.5863 | dice: 0.6598 | dice_neg: 0.8950 | dice_pos: 0.4247 | IoU: 0.3319\n",
      "\n",
      "Starting epoch: 63 | phase: train | ⏰: 02:27:17\n",
      "Loss: 0.6967 | dice: 0.7070 | dice_neg: 0.8723 | dice_pos: 0.5417 | IoU: 0.4272\n",
      "Starting epoch: 63 | phase: val | ⏰: 02:34:54\n",
      "Loss: 1.5992 | dice: 0.6583 | dice_neg: 0.8845 | dice_pos: 0.4321 | IoU: 0.3253\n",
      "\n",
      "Starting epoch: 64 | phase: train | ⏰: 02:35:56\n",
      "Loss: 0.6658 | dice: 0.7182 | dice_neg: 0.8939 | dice_pos: 0.5426 | IoU: 0.4292\n",
      "Starting epoch: 64 | phase: val | ⏰: 02:43:27\n",
      "Loss: 1.6910 | dice: 0.6589 | dice_neg: 0.8929 | dice_pos: 0.4249 | IoU: 0.3200\n",
      "\n",
      "Starting epoch: 65 | phase: train | ⏰: 02:44:33\n",
      "Loss: 0.6768 | dice: 0.7143 | dice_neg: 0.8829 | dice_pos: 0.5456 | IoU: 0.4362\n",
      "Starting epoch: 65 | phase: val | ⏰: 02:52:09\n",
      "Loss: 1.6092 | dice: 0.6586 | dice_neg: 0.8908 | dice_pos: 0.4264 | IoU: 0.3291\n",
      "\n",
      "Starting epoch: 66 | phase: train | ⏰: 02:53:12\n",
      "Loss: 0.7234 | dice: 0.7188 | dice_neg: 0.8939 | dice_pos: 0.5437 | IoU: 0.4316\n",
      "Starting epoch: 66 | phase: val | ⏰: 03:00:50\n",
      "Loss: 1.7509 | dice: 0.6581 | dice_neg: 0.9055 | dice_pos: 0.4107 | IoU: 0.3162\n",
      "\n",
      "Starting epoch: 67 | phase: train | ⏰: 03:01:52\n",
      "Loss: 0.6906 | dice: 0.7111 | dice_neg: 0.8770 | dice_pos: 0.5451 | IoU: 0.4294\n",
      "Starting epoch: 67 | phase: val | ⏰: 03:09:27\n",
      "Loss: 1.7379 | dice: 0.6595 | dice_neg: 0.8824 | dice_pos: 0.4367 | IoU: 0.3357\n",
      "\n",
      "Starting epoch: 68 | phase: train | ⏰: 03:10:33\n",
      "Loss: 0.7071 | dice: 0.7144 | dice_neg: 0.8828 | dice_pos: 0.5460 | IoU: 0.4300\n",
      "Starting epoch: 68 | phase: val | ⏰: 03:18:09\n",
      "Loss: 1.5180 | dice: 0.6587 | dice_neg: 0.8971 | dice_pos: 0.4204 | IoU: 0.3266\n",
      "\n",
      "Starting epoch: 69 | phase: train | ⏰: 03:19:13\n",
      "Loss: 0.6764 | dice: 0.7224 | dice_neg: 0.8949 | dice_pos: 0.5498 | IoU: 0.4353\n",
      "Starting epoch: 69 | phase: val | ⏰: 03:26:49\n",
      "Loss: 1.8093 | dice: 0.6574 | dice_neg: 0.8824 | dice_pos: 0.4324 | IoU: 0.3223\n",
      "\n",
      "Starting epoch: 70 | phase: train | ⏰: 03:27:53\n",
      "Loss: 0.6693 | dice: 0.7144 | dice_neg: 0.8823 | dice_pos: 0.5464 | IoU: 0.4357\n",
      "Starting epoch: 70 | phase: val | ⏰: 03:35:31\n",
      "Loss: 1.5836 | dice: 0.6568 | dice_neg: 0.8929 | dice_pos: 0.4208 | IoU: 0.3175\n",
      "\n",
      "Starting epoch: 71 | phase: train | ⏰: 03:36:35\n",
      "Loss: 0.7092 | dice: 0.7157 | dice_neg: 0.8844 | dice_pos: 0.5470 | IoU: 0.4311\n",
      "Starting epoch: 71 | phase: val | ⏰: 03:44:09\n",
      "Loss: 1.7618 | dice: 0.6608 | dice_neg: 0.8845 | dice_pos: 0.4371 | IoU: 0.3340\n",
      "\n",
      "Starting epoch: 72 | phase: train | ⏰: 03:45:13\n",
      "Loss: 0.6464 | dice: 0.7193 | dice_neg: 0.8912 | dice_pos: 0.5474 | IoU: 0.4357\n",
      "Starting epoch: 72 | phase: val | ⏰: 03:52:45\n",
      "Loss: 1.9067 | dice: 0.6580 | dice_neg: 0.8950 | dice_pos: 0.4211 | IoU: 0.3186\n",
      "\n",
      "Starting epoch: 73 | phase: train | ⏰: 03:53:51\n",
      "Loss: 0.7081 | dice: 0.7156 | dice_neg: 0.8881 | dice_pos: 0.5431 | IoU: 0.4280\n",
      "Starting epoch: 73 | phase: val | ⏰: 04:01:26\n",
      "Loss: 1.7684 | dice: 0.6593 | dice_neg: 0.8971 | dice_pos: 0.4215 | IoU: 0.3159\n",
      "\n",
      "Starting epoch: 74 | phase: train | ⏰: 04:02:31\n",
      "Loss: 0.6632 | dice: 0.7186 | dice_neg: 0.8917 | dice_pos: 0.5455 | IoU: 0.4324\n",
      "Starting epoch: 74 | phase: val | ⏰: 04:10:06\n",
      "Loss: 1.7543 | dice: 0.6593 | dice_neg: 0.8929 | dice_pos: 0.4256 | IoU: 0.3097\n",
      "\n",
      "Starting epoch: 75 | phase: train | ⏰: 04:11:10\n",
      "Loss: 0.6729 | dice: 0.7172 | dice_neg: 0.8902 | dice_pos: 0.5442 | IoU: 0.4356\n",
      "Starting epoch: 75 | phase: val | ⏰: 04:18:46\n",
      "Loss: 1.7398 | dice: 0.6605 | dice_neg: 0.8929 | dice_pos: 0.4281 | IoU: 0.3286\n",
      "\n",
      "Starting epoch: 76 | phase: train | ⏰: 04:19:51\n",
      "Loss: 0.6725 | dice: 0.7152 | dice_neg: 0.8844 | dice_pos: 0.5459 | IoU: 0.4330\n",
      "Starting epoch: 76 | phase: val | ⏰: 04:27:27\n",
      "Loss: 1.6460 | dice: 0.6557 | dice_neg: 0.8761 | dice_pos: 0.4354 | IoU: 0.3342\n",
      "\n",
      "Starting epoch: 77 | phase: train | ⏰: 04:28:31\n",
      "Loss: 0.6753 | dice: 0.7202 | dice_neg: 0.8917 | dice_pos: 0.5486 | IoU: 0.4332\n",
      "Starting epoch: 77 | phase: val | ⏰: 04:36:11\n",
      "Loss: 1.8772 | dice: 0.6573 | dice_neg: 0.9139 | dice_pos: 0.4007 | IoU: 0.3040\n",
      "\n",
      "Starting epoch: 78 | phase: train | ⏰: 04:37:15\n",
      "Loss: 0.6550 | dice: 0.7195 | dice_neg: 0.8902 | dice_pos: 0.5488 | IoU: 0.4375\n",
      "Starting epoch: 78 | phase: val | ⏰: 04:44:48\n",
      "Loss: 1.7177 | dice: 0.6580 | dice_neg: 0.8803 | dice_pos: 0.4357 | IoU: 0.3314\n",
      "\n",
      "Starting epoch: 79 | phase: train | ⏰: 04:45:57\n",
      "Loss: 0.6884 | dice: 0.7130 | dice_neg: 0.8797 | dice_pos: 0.5463 | IoU: 0.4358\n",
      "Starting epoch: 79 | phase: val | ⏰: 04:53:31\n",
      "Loss: 1.6195 | dice: 0.6590 | dice_neg: 0.8908 | dice_pos: 0.4273 | IoU: 0.3205\n",
      "\n",
      "Starting epoch: 80 | phase: train | ⏰: 04:54:38\n",
      "Loss: 0.6643 | dice: 0.7139 | dice_neg: 0.8849 | dice_pos: 0.5429 | IoU: 0.4289\n",
      "Starting epoch: 80 | phase: val | ⏰: 05:02:06\n",
      "Loss: 1.6794 | dice: 0.6569 | dice_neg: 0.8761 | dice_pos: 0.4378 | IoU: 0.3348\n",
      "\n",
      "Starting epoch: 81 | phase: train | ⏰: 05:03:12\n",
      "Loss: 0.6227 | dice: 0.7179 | dice_neg: 0.8912 | dice_pos: 0.5446 | IoU: 0.4319\n",
      "Starting epoch: 81 | phase: val | ⏰: 05:10:44\n",
      "Loss: 1.6534 | dice: 0.6598 | dice_neg: 0.8971 | dice_pos: 0.4225 | IoU: 0.3262\n",
      "\n",
      "Starting epoch: 82 | phase: train | ⏰: 05:11:51\n",
      "Loss: 0.6857 | dice: 0.7148 | dice_neg: 0.8812 | dice_pos: 0.5483 | IoU: 0.4321\n",
      "Starting epoch: 82 | phase: val | ⏰: 05:19:22\n",
      "Loss: 1.6011 | dice: 0.6602 | dice_neg: 0.8971 | dice_pos: 0.4232 | IoU: 0.3358\n",
      "\n",
      "Starting epoch: 83 | phase: train | ⏰: 05:20:27\n",
      "Loss: 0.6660 | dice: 0.7160 | dice_neg: 0.8828 | dice_pos: 0.5493 | IoU: 0.4315\n",
      "Starting epoch: 83 | phase: val | ⏰: 05:28:00\n",
      "Loss: 1.7614 | dice: 0.6596 | dice_neg: 0.9034 | dice_pos: 0.4158 | IoU: 0.3159\n",
      "\n",
      "Starting epoch: 84 | phase: train | ⏰: 05:29:03\n",
      "Loss: 0.6404 | dice: 0.7227 | dice_neg: 0.8960 | dice_pos: 0.5495 | IoU: 0.4354\n",
      "Starting epoch: 84 | phase: val | ⏰: 05:36:38\n",
      "Loss: 1.6687 | dice: 0.6602 | dice_neg: 0.8950 | dice_pos: 0.4254 | IoU: 0.3271\n",
      "\n",
      "Starting epoch: 85 | phase: train | ⏰: 05:37:42\n",
      "Loss: 0.6502 | dice: 0.7223 | dice_neg: 0.8991 | dice_pos: 0.5456 | IoU: 0.4349\n",
      "Starting epoch: 85 | phase: val | ⏰: 05:45:22\n",
      "Loss: 1.7863 | dice: 0.6601 | dice_neg: 0.8950 | dice_pos: 0.4252 | IoU: 0.3232\n",
      "\n",
      "Starting epoch: 86 | phase: train | ⏰: 05:46:27\n",
      "Loss: 0.7013 | dice: 0.7121 | dice_neg: 0.8807 | dice_pos: 0.5435 | IoU: 0.4303\n",
      "Starting epoch: 86 | phase: val | ⏰: 05:54:04\n",
      "Loss: 1.6655 | dice: 0.6594 | dice_neg: 0.8992 | dice_pos: 0.4196 | IoU: 0.3222\n",
      "\n",
      "Starting epoch: 87 | phase: train | ⏰: 05:55:11\n",
      "Loss: 0.6615 | dice: 0.7188 | dice_neg: 0.8870 | dice_pos: 0.5505 | IoU: 0.4361\n",
      "Starting epoch: 87 | phase: val | ⏰: 06:02:47\n",
      "Loss: 1.7456 | dice: 0.6595 | dice_neg: 0.8971 | dice_pos: 0.4220 | IoU: 0.3273\n",
      "\n",
      "Starting epoch: 88 | phase: train | ⏰: 06:03:52\n",
      "Loss: 0.6740 | dice: 0.7166 | dice_neg: 0.8870 | dice_pos: 0.5462 | IoU: 0.4314\n",
      "Starting epoch: 88 | phase: val | ⏰: 06:11:28\n",
      "Loss: 1.5783 | dice: 0.6597 | dice_neg: 0.8824 | dice_pos: 0.4370 | IoU: 0.3365\n",
      "\n",
      "Starting epoch: 89 | phase: train | ⏰: 06:12:32\n",
      "Loss: 0.6759 | dice: 0.7120 | dice_neg: 0.8739 | dice_pos: 0.5502 | IoU: 0.4362\n",
      "Starting epoch: 89 | phase: val | ⏰: 06:20:02\n",
      "Loss: 1.7976 | dice: 0.6569 | dice_neg: 0.8782 | dice_pos: 0.4356 | IoU: 0.3329\n",
      "\n",
      "Starting epoch: 90 | phase: train | ⏰: 06:21:06\n",
      "Loss: 0.6883 | dice: 0.7121 | dice_neg: 0.8812 | dice_pos: 0.5430 | IoU: 0.4270\n",
      "Starting epoch: 90 | phase: val | ⏰: 06:28:41\n",
      "Loss: 1.6195 | dice: 0.6574 | dice_neg: 0.8971 | dice_pos: 0.4178 | IoU: 0.3174\n",
      "\n",
      "Starting epoch: 91 | phase: train | ⏰: 06:29:44\n",
      "Loss: 0.6544 | dice: 0.7201 | dice_neg: 0.8965 | dice_pos: 0.5437 | IoU: 0.4289\n",
      "Starting epoch: 91 | phase: val | ⏰: 06:37:18\n",
      "Loss: 1.7104 | dice: 0.6583 | dice_neg: 0.8929 | dice_pos: 0.4238 | IoU: 0.3186\n",
      "\n",
      "Starting epoch: 92 | phase: train | ⏰: 06:38:24\n",
      "Loss: 0.6915 | dice: 0.7132 | dice_neg: 0.8839 | dice_pos: 0.5424 | IoU: 0.4273\n",
      "Starting epoch: 92 | phase: val | ⏰: 06:45:52\n",
      "Loss: 1.7553 | dice: 0.6605 | dice_neg: 0.8992 | dice_pos: 0.4218 | IoU: 0.3234\n",
      "\n",
      "Starting epoch: 93 | phase: train | ⏰: 06:46:59\n",
      "Loss: 0.7061 | dice: 0.7217 | dice_neg: 0.8954 | dice_pos: 0.5481 | IoU: 0.4299\n",
      "Starting epoch: 93 | phase: val | ⏰: 06:54:28\n",
      "Loss: 1.5765 | dice: 0.6558 | dice_neg: 0.8866 | dice_pos: 0.4251 | IoU: 0.3311\n",
      "\n",
      "Starting epoch: 94 | phase: train | ⏰: 06:55:36\n",
      "Loss: 0.6424 | dice: 0.7215 | dice_neg: 0.8944 | dice_pos: 0.5485 | IoU: 0.4337\n",
      "Starting epoch: 94 | phase: val | ⏰: 07:03:05\n",
      "Loss: 1.7603 | dice: 0.6598 | dice_neg: 0.8887 | dice_pos: 0.4310 | IoU: 0.3230\n",
      "\n",
      "Starting epoch: 95 | phase: train | ⏰: 07:04:10\n",
      "Loss: 0.6684 | dice: 0.7161 | dice_neg: 0.8881 | dice_pos: 0.5442 | IoU: 0.4296\n",
      "Starting epoch: 95 | phase: val | ⏰: 07:11:38\n",
      "Loss: 1.6813 | dice: 0.6607 | dice_neg: 0.8887 | dice_pos: 0.4328 | IoU: 0.3106\n",
      "\n",
      "Starting epoch: 96 | phase: train | ⏰: 07:12:45\n",
      "Loss: 0.7008 | dice: 0.7203 | dice_neg: 0.8917 | dice_pos: 0.5488 | IoU: 0.4282\n",
      "Starting epoch: 96 | phase: val | ⏰: 07:20:16\n",
      "Loss: 1.7775 | dice: 0.6601 | dice_neg: 0.8908 | dice_pos: 0.4295 | IoU: 0.3293\n",
      "\n",
      "Starting epoch: 97 | phase: train | ⏰: 07:21:23\n",
      "Loss: 0.6378 | dice: 0.7155 | dice_neg: 0.8886 | dice_pos: 0.5424 | IoU: 0.4282\n",
      "Starting epoch: 97 | phase: val | ⏰: 07:28:53\n",
      "Loss: 1.6072 | dice: 0.6608 | dice_neg: 0.8908 | dice_pos: 0.4309 | IoU: 0.3287\n",
      "\n",
      "Starting epoch: 98 | phase: train | ⏰: 07:30:01\n",
      "Loss: 0.6656 | dice: 0.7160 | dice_neg: 0.8870 | dice_pos: 0.5449 | IoU: 0.4300\n",
      "Starting epoch: 98 | phase: val | ⏰: 07:37:34\n",
      "Loss: 1.9283 | dice: 0.6562 | dice_neg: 0.8739 | dice_pos: 0.4385 | IoU: 0.3233\n",
      "\n",
      "Starting epoch: 99 | phase: train | ⏰: 07:38:37\n",
      "Loss: 0.6488 | dice: 0.7248 | dice_neg: 0.8991 | dice_pos: 0.5505 | IoU: 0.4345\n",
      "Starting epoch: 99 | phase: val | ⏰: 07:46:11\n",
      "Loss: 1.7410 | dice: 0.6589 | dice_neg: 0.8950 | dice_pos: 0.4229 | IoU: 0.3200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_trainer = Trainer(model)\n",
    "model_trainer.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PLOT TRAINING\n",
    "losses = model_trainer.losses\n",
    "dice_scores = model_trainer.dice_scores # overall dice\n",
    "iou_scores = model_trainer.iou_scores\n",
    "\n",
    "def plot(scores, name):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(range(len(scores[\"train\"])), scores[\"train\"], label=f'train {name}')\n",
    "    plt.plot(range(len(scores[\"train\"])), scores[\"val\"], label=f'val {name}')\n",
    "    plt.title(f'{name} plot'); plt.xlabel('Epoch'); plt.ylabel(f'{name}');\n",
    "    plt.legend(); \n",
    "    plt.show()\n",
    "\n",
    "plot(losses, \"BCE loss\")\n",
    "plot(dice_scores, \"Dice score\")\n",
    "plot(iou_scores, \"IoU score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, root, df, size, mean, std, tta=4):\n",
    "        self.root = root\n",
    "        self.size = size\n",
    "        self.fnames = list(df[\"ImageId\"])\n",
    "        self.num_samples = len(self.fnames)\n",
    "        self.transform = Compose(\n",
    "            [\n",
    "                Normalize(mean=mean, std=std, p=1),\n",
    "                Resize(size, size),\n",
    "                ToTensorV2(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.fnames[idx]\n",
    "        path = os.path.join(self.root, fname + \".png\")\n",
    "        image = cv2.imread(path)\n",
    "        images = self.transform(image=image)[\"image\"]\n",
    "        return images\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "def post_process(probability, threshold, min_size):\n",
    "    mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]\n",
    "    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n",
    "    predictions = np.zeros((1024, 1024), np.float32)\n",
    "    num = 0\n",
    "    for c in range(1, num_component):\n",
    "        p = (component == c)\n",
    "        if p.sum() > min_size:\n",
    "            predictions[p] = 1\n",
    "            num += 1\n",
    "    return predictions, num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_submission_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-3c9681cfedf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmin_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_submission_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m testset = DataLoader(\n\u001b[1;32m     11\u001b[0m     \u001b[0mTestDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_submission_path' is not defined"
     ]
    }
   ],
   "source": [
    "size = 512\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "num_workers = 8\n",
    "batch_size = 16\n",
    "best_threshold = 0.5\n",
    "min_size = 3500\n",
    "device = torch.device(\"cuda:0\")\n",
    "df = pd.read_csv(sample_submission_path)\n",
    "testset = DataLoader(\n",
    "    TestDataset(test_data_folder, df, size, mean, std),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "model = model_trainer.net # get the model from model_trainer object\n",
    "model.eval()\n",
    "state = torch.load('./model.pth', map_location=lambda storage, loc: storage)\n",
    "model.load_state_dict(state[\"state_dict\"])\n",
    "encoded_pixels = []\n",
    "for i, batch in enumerate(tqdm(testset)):\n",
    "    preds = torch.sigmoid(model(batch.to(device)))\n",
    "    preds = preds.detach().cpu().numpy()[:, 0, :, :] # (batch_size, 1, size, size) -> (batch_size, size, size)\n",
    "    for probability in preds:\n",
    "        if probability.shape != (1024, 1024):\n",
    "            probability = cv2.resize(probability, dsize=(1024, 1024), interpolation=cv2.INTER_LINEAR)\n",
    "        predict, num_predict = post_process(probability, best_threshold, min_size)\n",
    "        if num_predict == 0:\n",
    "            encoded_pixels.append('-1')\n",
    "        else:\n",
    "            r = run_length_encode(predict)\n",
    "            encoded_pixels.append(r)\n",
    "df['EncodedPixels'] = encoded_pixels\n",
    "df.to_csv('submission.csv', columns=['ImageId', 'EncodedPixels'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/facebookresearch/moco/blob/colab-notebook/colab/moco_cifar10_demo.ipynb",
     "timestamp": 1677256490794
    }
   ]
  },
  "environment": {
   "name": "pytorch-gpu.1-4.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06768cfdf690415e9affdf2a74e59a30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "17032952ca804b558931b93c0fde1540": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cc091016bb8d423f80dddca00096ba0b",
       "IPY_MODEL_6bbbd0d7f0f749939610ccc1aa47bfec",
       "IPY_MODEL_eb9b54187a9b4404b93ce96cb5297a1e"
      ],
      "layout": "IPY_MODEL_48854cfeccd84183a1819d703353a4fa"
     }
    },
    "1d94bafa0c204861869e3c8656f88338": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20faf3fda02c4257b5ac68099de45581": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "23596ead25e842da821eee9281ddd44b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce5a59adc10e4ad3bbd5ee949704a493",
      "placeholder": "​",
      "style": "IPY_MODEL_2e92aabde375495f880ae9752b55e057",
      "value": " 4.64k/4.64k [00:00&lt;00:00, 115kB/s]"
     }
    },
    "2e92aabde375495f880ae9752b55e057": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "31ea43544fcd44a19f86c920afaa12ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3deab35d86db420c963d21fde4f4f770": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1d94bafa0c204861869e3c8656f88338",
      "placeholder": "​",
      "style": "IPY_MODEL_06768cfdf690415e9affdf2a74e59a30",
      "value": "Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"
     }
    },
    "3e187d79da6f477180cac4b95d949388": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "48854cfeccd84183a1819d703353a4fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5528a5ca02bc4cde9fa1bad142519e35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_79cc1507b29d4125ae5e3c704b2b8b75",
      "placeholder": "​",
      "style": "IPY_MODEL_31ea43544fcd44a19f86c920afaa12ab",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "55606bf393b940278f76baf65170eeab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5bffaeb6f389499c958f4a12dca192e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5d806cd2fe7e4424b7fb90371815f4d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3deab35d86db420c963d21fde4f4f770",
       "IPY_MODEL_e3214b7f823d4a12bed90f36b6cd3bbe",
       "IPY_MODEL_6bec28962d4740aabe8e9896e00134b7"
      ],
      "layout": "IPY_MODEL_5da754b21ef34dc9a7eff14bdc3a34bc"
     }
    },
    "5da754b21ef34dc9a7eff14bdc3a34bc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6bbbd0d7f0f749939610ccc1aa47bfec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4d8fcd96c2b44c1a21e6a0cf46d736d",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5bffaeb6f389499c958f4a12dca192e4",
      "value": 170498071
     }
    },
    "6bec28962d4740aabe8e9896e00134b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d083420df95d42ecb16cedd5037dc2a3",
      "placeholder": "​",
      "style": "IPY_MODEL_55606bf393b940278f76baf65170eeab",
      "value": " 223M/223M [00:02&lt;00:00, 86.7MB/s]"
     }
    },
    "73487b45ea444f8e81410a9627c85b40": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79cc1507b29d4125ae5e3c704b2b8b75": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7cdb20bd656249fe85674962d2a4ba7e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9cf2f5d1a7894a73861ce8bd69675ed1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a4d8fcd96c2b44c1a21e6a0cf46d736d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aeabbf27ff5e4ef5a0c3de652c86d632": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5528a5ca02bc4cde9fa1bad142519e35",
       "IPY_MODEL_c3b291f042fe422aa3a2b22f1208d0ca",
       "IPY_MODEL_23596ead25e842da821eee9281ddd44b"
      ],
      "layout": "IPY_MODEL_d74ac37d81974a4780581b554f3e7d23"
     }
    },
    "c3b291f042fe422aa3a2b22f1208d0ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73487b45ea444f8e81410a9627c85b40",
      "max": 4642,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3e187d79da6f477180cac4b95d949388",
      "value": 4642
     }
    },
    "cc091016bb8d423f80dddca00096ba0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7cdb20bd656249fe85674962d2a4ba7e",
      "placeholder": "​",
      "style": "IPY_MODEL_20faf3fda02c4257b5ac68099de45581",
      "value": "100%"
     }
    },
    "ce5a59adc10e4ad3bbd5ee949704a493": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf90228e48af46809d35402d94eb568e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d083420df95d42ecb16cedd5037dc2a3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d74ac37d81974a4780581b554f3e7d23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9070ee51e9c4ebe868777345a25a62d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3214b7f823d4a12bed90f36b6cd3bbe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f48ad91e88b340258d9fbe66bc58696b",
      "max": 223137427,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9cf2f5d1a7894a73861ce8bd69675ed1",
      "value": 223137427
     }
    },
    "eb9b54187a9b4404b93ce96cb5297a1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d9070ee51e9c4ebe868777345a25a62d",
      "placeholder": "​",
      "style": "IPY_MODEL_cf90228e48af46809d35402d94eb568e",
      "value": " 170498071/170498071 [00:14&lt;00:00, 14802420.28it/s]"
     }
    },
    "f48ad91e88b340258d9fbe66bc58696b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
