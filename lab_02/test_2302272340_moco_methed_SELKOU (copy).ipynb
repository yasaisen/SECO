{"cells":[{"cell_type":"markdown","metadata":{"id":"ai8NrWqIPi-g"},"source":["## MoCo Demo: CIFAR-10\n","\n","This is a simple demo for training MoCo on CIFAR-10. It can be run directly in a Colab notebook using a publicly available GPU.\n","\n","#### Results\n","\n","These are the ResNet-18 classification accuracy of a **kNN monitor** on the unsupervised pre-training features. \n","\n","| config | 200ep | 400ep | 800ep |\n","| --- | --- | --- | --- |\n","| Asymmetric | 82.6 | 86.3 | 88.7 |\n","| Symmetric | 85.3 | 88.5 | 89.7 |\n","\n","#### Notes\n","\n","* **Symmetric loss**: the original MoCo paper uses an *asymmetric* loss -- one crop is the query and the other crop is the key, and it backpropagates to one crop (query). Following SimCLR/BYOL, here we provide an option of a *symmetric* loss -- it swaps the two crops and computes an extra loss. The symmetric loss behaves like 2x epochs of the asymmetric counterpart: this may dominate the comparison results when the models are trained with a fixed epoch number.\n","\n","* **SplitBatchNorm**: the original MoCo was trained in 8 GPUs. To simulate the multi-GPU behavior of BatchNorm in this 1-GPU demo, we provide a SplitBatchNorm layer. We set `--bn-splits 8` by default to simulate 8 GPUs. `--bn-splits 1` is analogous to SyncBatchNorm in the multi-GPU case.\n","\n","* **kNN monitor**: this demo provides a kNN monitor on the test set. Training a linear classifier on frozen features often achieves higher accuracy. To train a linear classifier (not provided in this demo), we recommend using lr=30, wd=0, epochs=100 with a stepwise or cosine schedule. The ResNet-18 model (kNN 89.7) has 90.7 linear classification accuracy.\n","\n","#### Disclaimer\n","\n","This demo aims to provide an interface with a free GPU (thanks to Colab) for understanding how the code runs. We suggest users be careful to draw conclusions from CIFAR, which may not generalize beyond this dataset. We have verified that it is beneficial to have the momentum encoder (disabling it by `--moco-m 0.0` should fail), queue size (saturated at `--moco-k 4096`) and ShuffleBN (without which loses 4% at 800 epochs) on CIFAR, similar to the observations on ImageNet. But new observations made only on CIFAR should be judged with caution.\n","\n","#### References\n","This demo is adapted from:\n","* http://github.com/zhirongw/lemniscate.pytorch\n","* https://github.com/leftthomas/SimCLR\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9MXcbCW8f7Eh"},"source":["### Prepare\n","\n","Check GPU settings. A free GPU in Colab is <= Tesla P100. The log of the demo is based on a Tesla V100 from Google Cloud Platform.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5122,"status":"ok","timestamp":1677512171549,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"CvAFrPlS4bLU","outputId":"2900b98f-10a0-48ae-ae6a-0aff7f787c20"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/yasaisen/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["# gpu_info = !nvidia-smi -i 0\n","# gpu_info = '\\n'.join(gpu_info)\n","# print(gpu_info)\n","\n","from datetime import datetime\n","from functools import partial\n","from PIL import Image\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from torchvision.datasets import CIFAR10\n","from torchvision.models import resnet\n","from tqdm import tqdm\n","import argparse\n","import json\n","import math\n","import os\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"markdown","metadata":{"id":"S_rtoBGhgSae"},"source":["### Set arguments"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30443,"status":"ok","timestamp":1677512201987,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"UKCi74HiH1A9","outputId":"779d592f-9214-477e-c922-233d7845cb7c"},"outputs":[],"source":["# import os\n","# from google.colab import drive\n","\n","# drive.mount('/content/drive')\n","ROOT_PATH = '/home/yasaisen/Desktop/11_research/11_research_main/lab_02'"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1677512201988,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"Uh-pffjlIDDw"},"outputs":[],"source":["def checkpath(path):\n","    if not os.path.exists(path):\n","        os.makedirs(path)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":2263,"status":"ok","timestamp":1677512204231,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"SRDsG25wIKf-"},"outputs":[],"source":["# model\n","\n","Version = '230228_v0.0.2'\n","\n","root_folder = os.path.abspath(os.path.join(ROOT_PATH, Version))\n","\n","# model_DIR = os.path.abspath(os.path.join(root_folder, 'model'))\n","checkpath(root_folder)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10881,"status":"ok","timestamp":1677512215099,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"RfrSWrkM7uUK","outputId":"607a80e7-f848-46c4-cb2c-e0ce32de6b0f"},"outputs":[],"source":["# !pip install transformers"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":2463,"status":"ok","timestamp":1677512217559,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"1tdXYOBI7W7Q"},"outputs":[],"source":["import collections.abc\n","import math\n","from dataclasses import dataclass\n","from typing import Any, Optional, Tuple, Union\n","\n","import numpy as np\n","import torch\n","import torch.utils.checkpoint\n","from torch import nn\n","\n","# from ...activations import ACT2FN\n","from transformers.activations import ACT2FN\n","\n","# from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n","from transformers.modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n","# from ...modeling_utils import PreTrainedModel\n","from transformers.modeling_utils import PreTrainedModel\n","# from ...utils import (\n","#     ModelOutput,\n","#     add_start_docstrings,\n","#     add_start_docstrings_to_model_forward,\n","#     logging,\n","#     replace_return_docstrings,\n","# )\n","from transformers.utils import (\n","    ModelOutput,\n","    add_start_docstrings,\n","    add_start_docstrings_to_model_forward,\n","    logging,\n","    replace_return_docstrings,\n",")\n","# from .configuration_groupvit import GroupViTConfig, GroupViTTextConfig, GroupViTVisionConfig\n","from transformers.models.groupvit.configuration_groupvit import GroupViTConfig, GroupViTTextConfig, GroupViTVisionConfig\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1677512217559,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"M97ik37v9rbD"},"outputs":[],"source":["logger = logging.get_logger(__name__)\n","\n","_CHECKPOINT_FOR_DOC = \"nvidia/groupvit-gcc-yfcc\"\n","\n","GROUPVIT_PRETRAINED_MODEL_ARCHIVE_LIST = [\n","    \"nvidia/groupvit-gcc-yfcc\",\n","    # See all GroupViT models at https://huggingface.co/models?filter=groupvit\n","]"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1677512217560,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"TfjSCoDn9utr"},"outputs":[],"source":["# Copied from transformers.models.bart.modeling_bart._expand_mask\n","def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n","    \"\"\"\n","    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n","    \"\"\"\n","    bsz, src_len = mask.size()\n","    tgt_len = tgt_len if tgt_len is not None else src_len\n","\n","    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n","\n","    inverted_mask = 1.0 - expanded_mask\n","\n","    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1677512217560,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"6PRDe_fu9xoT"},"outputs":[],"source":["# contrastive loss function, adapted from\n","# https://sachinruk.github.io/blog/pytorch/pytorch%20lightning/loss%20function/gpu/2021/03/07/CLIP.html\n","def contrastive_loss(logits: torch.Tensor) -> torch.Tensor:\n","    return nn.functional.cross_entropy(logits, torch.arange(len(logits), device=logits.device))"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1677512217560,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"NTvFTrtG9z1z"},"outputs":[],"source":["# Copied from transformers.models.clip.modeling_clip.clip_loss with clip->groupvit\n","def groupvit_loss(similarity: torch.Tensor) -> torch.Tensor:\n","    caption_loss = contrastive_loss(similarity)\n","    image_loss = contrastive_loss(similarity.t())\n","    return (caption_loss + image_loss) / 2.0"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1677512217560,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"iOvAJP0L91ns"},"outputs":[],"source":["def hard_softmax(logits: torch.Tensor, dim: int):\n","    y_soft = logits.softmax(dim)\n","    # Straight through.\n","    index = y_soft.max(dim, keepdim=True)[1]\n","    y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(dim, index, 1.0)\n","    ret = y_hard - y_soft.detach() + y_soft\n","\n","    return ret"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1677512217560,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"6-b2H_fE93b1"},"outputs":[],"source":["def gumbel_softmax(logits: torch.Tensor, tau: float = 1, hard: bool = False, dim: int = -1) -> torch.Tensor:\n","    # more stable https://github.com/pytorch/pytorch/issues/41663\n","    gumbel_dist = torch.distributions.gumbel.Gumbel(\n","        torch.tensor(0.0, device=logits.device, dtype=logits.dtype),\n","        torch.tensor(1.0, device=logits.device, dtype=logits.dtype),\n","    )\n","    gumbels = gumbel_dist.sample(logits.shape)\n","\n","    gumbels = (logits + gumbels) / tau  # ~Gumbel(logits,tau)\n","    y_soft = gumbels.softmax(dim)\n","\n","    if hard:\n","        # Straight through.\n","        index = y_soft.max(dim, keepdim=True)[1]\n","        y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(dim, index, 1.0)\n","        ret = y_hard - y_soft.detach() + y_soft\n","    else:\n","        # Reparametrization trick.\n","        ret = y_soft\n","    return ret"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1677512217561,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"sh78Xa0q9564"},"outputs":[],"source":["def resize_attention_map(attentions, height, width, align_corners=False):\n","    \"\"\"\n","    Args:\n","        attentions (`torch.Tensor`): attention map of shape [batch_size, groups, feat_height*feat_width]\n","        height (`int`): height of the output attention map\n","        width (`int`): width of the output attention map\n","        align_corners (`bool`, *optional*): the `align_corner` argument for `nn.functional.interpolate`.\n","\n","    Returns:\n","        `torch.Tensor`: resized attention map of shape [batch_size, groups, height, width]\n","    \"\"\"\n","\n","    scale = (height * width // attentions.shape[2]) ** 0.5\n","    if height > width:\n","        feat_width = int(np.round(width / scale))\n","        feat_height = attentions.shape[2] // feat_width\n","    else:\n","        feat_height = int(np.round(height / scale))\n","        feat_width = attentions.shape[2] // feat_height\n","\n","    batch_size = attentions.shape[0]\n","    groups = attentions.shape[1]  # number of group token\n","    # [batch_size, groups, height*width, groups] -> [batch_size, groups, height, width]\n","    attentions = attentions.reshape(batch_size, groups, feat_height, feat_width)\n","    attentions = nn.functional.interpolate(\n","        attentions, size=(height, width), mode=\"bilinear\", align_corners=align_corners\n","    )\n","    return attentions"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1677512217561,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"KyZtU92l98ix"},"outputs":[],"source":["def get_grouping_from_attentions(attentions, hw_shape):\n","    \"\"\"\n","    Args:\n","        attentions (`tuple(torch.FloatTensor)`: tuple of attention maps returned by `GroupViTVisionTransformer`\n","        hw_shape (`tuple(int)`): height and width of the output attention map\n","    Returns:\n","        `torch.Tensor`: the attention map of shape [batch_size, groups, height, width]\n","    \"\"\"\n","\n","    attn_maps = []\n","    with torch.no_grad():\n","        prev_attn_masks = None\n","        for attn_masks in attentions:\n","            # [batch_size, num_groups, height x width] -> [batch_size, height x width, num_groups]\n","            attn_masks = attn_masks.permute(0, 2, 1).contiguous()\n","            if prev_attn_masks is None:\n","                prev_attn_masks = attn_masks\n","            else:\n","                prev_attn_masks = prev_attn_masks @ attn_masks\n","            # [batch_size, heightxwidth, num_groups] -> [batch_size, num_groups, heightxwidth] -> [batch_size, num_groups, height, width]\n","            cur_attn_map = resize_attention_map(prev_attn_masks.permute(0, 2, 1).contiguous(), *hw_shape)\n","            attn_maps.append(cur_attn_map)\n","\n","    # [batch_size, num_groups, height, width]\n","    final_grouping = attn_maps[-1]\n","\n","    return final_grouping"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1677512217561,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"sX37CrAn9_Mw"},"outputs":[],"source":["class GroupViTCrossAttentionLayer(nn.Module):\n","    def __init__(self, config: GroupViTVisionConfig):\n","        super().__init__()\n","        self.attn = GroupViTAttention(config)\n","        self.norm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","        self.mlp = GroupViTMLP(config)\n","        self.norm_post = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","\n","    def forward(self, query, key):\n","        x = query\n","        x = x + self.attn(query, encoder_hidden_states=key)[0]\n","        x = x + self.mlp(self.norm2(x))\n","        x = self.norm_post(x)\n","        return x"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1677512217561,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"LfMDcjbu-BFh"},"outputs":[],"source":["class GroupViTAssignAttention(nn.Module):\n","    def __init__(self, config: GroupViTVisionConfig):\n","        super().__init__()\n","        self.scale = config.hidden_size**-0.5\n","\n","        self.q_proj = nn.Linear(config.hidden_size, config.hidden_size)\n","        self.k_proj = nn.Linear(config.hidden_size, config.hidden_size)\n","        self.v_proj = nn.Linear(config.hidden_size, config.hidden_size)\n","        self.proj = nn.Linear(config.hidden_size, config.hidden_size)\n","        self.assign_eps = config.assign_eps\n","\n","    def get_attn(self, attn, gumbel=True, hard=True):\n","        if gumbel and self.training:\n","            attn = gumbel_softmax(attn, dim=-2, hard=hard)\n","        else:\n","            if hard:\n","                attn = hard_softmax(attn, dim=-2)\n","            else:\n","                attn = nn.functional.softmax(attn, dim=-2)\n","\n","        return attn\n","\n","    def forward(self, query, key):\n","        value = key\n","        # [batch_size, query_length, channels]\n","        query = self.q_proj(query)\n","\n","        # [batch_size, key_length, channels]\n","        key = self.k_proj(key)\n","\n","        # [batch_size, key_length, channels]\n","        value = self.v_proj(value)\n","\n","        # [batch_size, query_length, key_length]\n","        raw_attn = (query @ key.transpose(-2, -1)) * self.scale\n","\n","        attn = self.get_attn(raw_attn)\n","        soft_attn = self.get_attn(raw_attn, gumbel=False, hard=False)\n","\n","        attn = attn / (attn.sum(dim=-1, keepdim=True) + self.assign_eps)\n","\n","        out = attn @ value\n","\n","        out = self.proj(out)\n","\n","        return out, soft_attn"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1677512217562,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"yOJBrOGt-GKD"},"outputs":[],"source":["class GroupViTTokenAssign(nn.Module):\n","    def __init__(self, config: GroupViTVisionConfig, num_group_token, num_output_group):\n","        super().__init__()\n","        self.num_output_group = num_output_group\n","        # norm on group_tokens\n","        self.norm_tokens = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","        assign_mlp_ratio = (\n","            config.assign_mlp_ratio\n","            if isinstance(config.assign_mlp_ratio, collections.abc.Iterable)\n","            else (config.assign_mlp_ratio, config.assign_mlp_ratio)\n","        )\n","        tokens_dim, channels_dim = [int(x * config.hidden_size) for x in assign_mlp_ratio]\n","        self.mlp_inter = GroupViTMixerMLP(config, num_group_token, tokens_dim, num_output_group)\n","        self.norm_post_tokens = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","        # norm on x\n","        self.norm_x = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","        self.pre_assign_attn = GroupViTCrossAttentionLayer(config)\n","\n","        self.assign = GroupViTAssignAttention(config)\n","        self.norm_new_x = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","        self.mlp_channels = GroupViTMLP(config, config.hidden_size, channels_dim, config.hidden_size)\n","\n","    def project_group_token(self, group_tokens):\n","        \"\"\"\n","        Args:\n","            group_tokens (torch.Tensor): group tokens, [batch_size, num_group_tokens, channels]\n","\n","        Returns:\n","            projected_group_tokens (torch.Tensor): [batch_size, num_output_groups, channels]\n","        \"\"\"\n","        # [B, num_output_groups, C] <- [B, num_group_tokens, C]\n","        projected_group_tokens = self.mlp_inter(group_tokens)\n","        projected_group_tokens = self.norm_post_tokens(projected_group_tokens)\n","        return projected_group_tokens\n","\n","    def forward(self, image_tokens, group_tokens):\n","        \"\"\"\n","        Args:\n","            image_tokens (`torch.Tensor`): image tokens, of shape [batch_size, input_length, channels]\n","            group_tokens (`torch.Tensor`): group tokens, [batch_size, num_group_tokens, channels]\n","        \"\"\"\n","\n","        group_tokens = self.norm_tokens(group_tokens)\n","        image_tokens = self.norm_x(image_tokens)\n","        # [batch_size, num_output_groups, channels]\n","        projected_group_tokens = self.project_group_token(group_tokens)\n","        projected_group_tokens = self.pre_assign_attn(projected_group_tokens, image_tokens)\n","        new_image_tokens, attention = self.assign(projected_group_tokens, image_tokens)\n","        new_image_tokens += projected_group_tokens\n","\n","        new_image_tokens = new_image_tokens + self.mlp_channels(self.norm_new_x(new_image_tokens))\n","\n","        return new_image_tokens, attention\n"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1677512217562,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"1eBLeUct-JCq"},"outputs":[],"source":["@dataclass\n","class GroupViTModelOutput(ModelOutput):\n","    \"\"\"\n","    Args:\n","        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n","            Contrastive loss for image-text similarity.\n","        logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n","            The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n","            similarity scores.\n","        logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n","            The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n","            similarity scores.\n","        segmentation_logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels, logits_height, logits_width)`):\n","            Classification scores for each pixel.\n","\n","            <Tip warning={true}>\n","\n","            The logits returned do not necessarily have the same size as the `pixel_values` passed as inputs. This is\n","            to avoid doing two interpolations and lose some quality when a user needs to resize the logits to the\n","            original image size as post-processing. You should always check your logits shape and resize as needed.\n","\n","            </Tip>\n","\n","        text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n","            The text embeddings obtained by applying the projection layer to the pooled output of\n","            [`GroupViTTextModel`].\n","        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n","            The image embeddings obtained by applying the projection layer to the pooled output of\n","            [`GroupViTVisionModel`].\n","        text_model_output (`BaseModelOutputWithPooling`):\n","            The output of the [`GroupViTTextModel`].\n","        vision_model_output (`BaseModelOutputWithPooling`):\n","            The output of the [`GroupViTVisionModel`].\n","    \"\"\"\n","\n","    loss: Optional[torch.FloatTensor] = None\n","    logits_per_image: torch.FloatTensor = None\n","    logits_per_text: torch.FloatTensor = None\n","    segmentation_logits: torch.FloatTensor = None\n","    text_embeds: torch.FloatTensor = None\n","    image_embeds: torch.FloatTensor = None\n","    text_model_output: BaseModelOutputWithPooling = None\n","    vision_model_output: BaseModelOutputWithPooling = None\n","\n","    def to_tuple(self) -> Tuple[Any]:\n","        return tuple(\n","            self[k] if k not in [\"text_model_output\", \"vision_model_output\"] else getattr(self, k).to_tuple()\n","            for k in self.keys()\n","        )"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1677512217562,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"8atGssmE-OgA"},"outputs":[],"source":["class GroupViTPatchEmbeddings(nn.Module):\n","    \"\"\"\n","    Image to Patch Embedding.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        image_size: int = 224,\n","        patch_size: Union[int, Tuple[int, int]] = 16,\n","        num_channels: int = 3,\n","        embed_dim: int = 768,\n","    ):\n","        super().__init__()\n","        image_size = image_size if isinstance(image_size, collections.abc.Iterable) else (image_size, image_size)\n","        patch_size = patch_size if isinstance(patch_size, collections.abc.Iterable) else (patch_size, patch_size)\n","        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n","        self.image_size = image_size\n","        self.patch_size = patch_size\n","        self.num_patches = num_patches\n","\n","        self.projection = nn.Conv2d(num_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n","\n","    def forward(self, pixel_values: torch.Tensor, interpolate_pos_encoding: bool = False) -> torch.Tensor:\n","        batch_size, num_channels, height, width = pixel_values.shape\n","        if not interpolate_pos_encoding:\n","            if height != self.image_size[0] or width != self.image_size[1]:\n","                raise ValueError(\n","                    f\"Input image size ({height}*{width}) doesn't match model\"\n","                    f\" ({self.image_size[0]}*{self.image_size[1]}).\"\n","                )\n","        x = self.projection(pixel_values).flatten(2).transpose(1, 2)\n","        return x"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1677512217562,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"pvmCYxCX-Q5x"},"outputs":[],"source":["class GroupViTVisionEmbeddings(nn.Module):\n","    def __init__(self, config: GroupViTVisionConfig):\n","        super().__init__()\n","\n","        self.patch_embeddings = GroupViTPatchEmbeddings(\n","            image_size=config.image_size,\n","            patch_size=config.patch_size,\n","            num_channels=config.num_channels,\n","            embed_dim=config.hidden_size,\n","        )\n","        num_patches = self.patch_embeddings.num_patches\n","        self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches, config.hidden_size))\n","        self.dropout = nn.Dropout(config.dropout)\n","        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","        self.config = config\n","\n","    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n","        \"\"\"\n","        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher\n","        resolution images.\n","\n","        Source:\n","        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n","        \"\"\"\n","\n","        npatch = embeddings.shape[1]\n","        if npatch == self.position_embeddings.shape[1] and height == width:\n","            return self.position_embeddings\n","        patch_pos_embed = self.position_embeddings\n","        num_original_pos_embed = patch_pos_embed.shape[1]\n","        dim = embeddings.shape[-1]\n","        feat_height = height // self.config.patch_size\n","        feat_width = width // self.config.patch_size\n","        # we add a small number to avoid floating point error in the interpolation\n","        # see discussion at https://github.com/facebookresearch/dino/issues/8\n","        feat_height, feat_width = feat_height + 0.1, feat_width + 0.1\n","        original_height = original_width = math.sqrt(num_original_pos_embed)\n","        reshaped_patch_pos_embed = patch_pos_embed.reshape(1, int(original_height), int(original_width), dim).permute(\n","            0, 3, 1, 2\n","        )\n","        scale_factor = (feat_height / original_height, feat_width / original_width)\n","        patch_pos_embed = nn.functional.interpolate(\n","            reshaped_patch_pos_embed,\n","            scale_factor=scale_factor,\n","            mode=\"bicubic\",\n","            align_corners=False,\n","        )\n","        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n","        return patch_pos_embed\n","\n","    def forward(self, pixel_values: torch.Tensor, interpolate_pos_encoding: bool = False) -> torch.Tensor:\n","        batch_size, num_channels, height, width = pixel_values.shape\n","        embeddings = self.patch_embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n","\n","        embeddings = self.layernorm(embeddings)\n","\n","        batch_size, seq_len, _ = embeddings.size()\n","\n","        # add positional encoding to each token\n","        if interpolate_pos_encoding:\n","            embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)\n","        else:\n","            embeddings = embeddings + self.position_embeddings\n","\n","        embeddings = self.dropout(embeddings)\n","\n","        return embeddings"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1677512217563,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"jZ_xSDus-ULA"},"outputs":[],"source":["# Copied from transformers.models.clip.modeling_clip.CLIPTextEmbeddings with CLIP->GroupViT\n","class GroupViTTextEmbeddings(nn.Module):\n","    def __init__(self, config: GroupViTTextConfig):\n","        super().__init__()\n","        embed_dim = config.hidden_size\n","\n","        self.token_embedding = nn.Embedding(config.vocab_size, embed_dim)\n","        self.position_embedding = nn.Embedding(config.max_position_embeddings, embed_dim)\n","\n","        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n","        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n","\n","    def forward(\n","        self,\n","        input_ids: Optional[torch.LongTensor] = None,\n","        position_ids: Optional[torch.LongTensor] = None,\n","        inputs_embeds: Optional[torch.FloatTensor] = None,\n","    ) -> torch.Tensor:\n","        seq_length = input_ids.shape[-1] if input_ids is not None else inputs_embeds.shape[-2]\n","\n","        if position_ids is None:\n","            position_ids = self.position_ids[:, :seq_length]\n","\n","        if inputs_embeds is None:\n","            inputs_embeds = self.token_embedding(input_ids)\n","\n","        position_embeddings = self.position_embedding(position_ids)\n","        embeddings = inputs_embeds + position_embeddings\n","\n","        return embeddings"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1677512217563,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"WI0LJqF--anN"},"outputs":[],"source":["class GroupViTStage(nn.Module):\n","    \"\"\"This corresponds to the `GroupingLayer` class in the GroupViT implementation.\"\"\"\n","\n","    def __init__(\n","        self,\n","        config: GroupViTVisionConfig,\n","        depth: int,\n","        num_prev_group_token: int,\n","        num_group_token: int,\n","        num_output_group: int,\n","    ):\n","        super().__init__()\n","        self.depth = depth\n","        self.num_group_token = num_group_token\n","        if num_group_token > 0:\n","            self.group_token = nn.Parameter(torch.zeros(1, num_group_token, config.hidden_size))\n","        else:\n","            self.group_token = None\n","        self.gradient_checkpointing = False\n","        self.layers = nn.ModuleList([GroupViTEncoderLayer(config) for _ in range(depth)])\n","\n","        if num_group_token > 0:\n","            self.downsample = GroupViTTokenAssign(\n","                config=config,\n","                num_group_token=num_group_token,\n","                num_output_group=num_output_group,\n","            )\n","        else:\n","            self.downsample = None\n","\n","        if num_prev_group_token > 0 and num_group_token > 0:\n","            self.group_projector = nn.Sequential(\n","                nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps),\n","                GroupViTMixerMLP(config, num_prev_group_token, config.hidden_size // 2, num_group_token),\n","            )\n","        else:\n","            self.group_projector = None\n","\n","    @property\n","    def with_group_token(self):\n","        return self.group_token is not None\n","\n","    def split_x(self, x):\n","        if self.with_group_token:\n","            return x[:, : -self.num_group_token], x[:, -self.num_group_token :]\n","        else:\n","            return x, None\n","\n","    def concat_x(self, x: torch.Tensor, group_token: Optional[torch.Tensor] = None) -> torch.Tensor:\n","        if group_token is None:\n","            return x\n","        return torch.cat([x, group_token], dim=1)\n","\n","    def forward(\n","        self,\n","        hidden_states: torch.Tensor,\n","        prev_group_token: Optional[torch.Tensor] = None,\n","        output_attentions: Optional[bool] = False,\n","    ) -> Tuple[torch.FloatTensor]:\n","        \"\"\"\n","        Args:\n","            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n","            attention_mask (`torch.FloatTensor`): attention mask of size\n","                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n","                `(config.encoder_attention_heads,)`.\n","            output_attentions (`bool`, *optional*):\n","                Whether or not to return the grouping tensors of Grouping block.\n","        \"\"\"\n","        if self.with_group_token:\n","            group_token = self.group_token.expand(hidden_states.size(0), -1, -1)\n","            if self.group_projector is not None:\n","                group_token = group_token + self.group_projector(prev_group_token)\n","        else:\n","            group_token = None\n","\n","        x = hidden_states\n","\n","        cat_x = self.concat_x(x, group_token)\n","        for layer in self.layers:\n","            layer_out = layer(cat_x, attention_mask=None, causal_attention_mask=None)\n","            cat_x = layer_out[0]\n","\n","        x, group_token = self.split_x(cat_x)\n","\n","        attention = None\n","        if self.downsample is not None:\n","            x, attention = self.downsample(x, group_token)\n","\n","        outputs = (x, group_token)\n","        if output_attentions:\n","            outputs = outputs + (attention,)\n","\n","        return outputs"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1677512217563,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"lZNeThbG-bdS"},"outputs":[],"source":["class GroupViTMLP(nn.Module):\n","    def __init__(\n","        self,\n","        config: GroupViTVisionConfig,\n","        hidden_size: Optional[int] = None,\n","        intermediate_size: Optional[int] = None,\n","        output_size: Optional[int] = None,\n","    ):\n","        super().__init__()\n","        self.config = config\n","        self.activation_fn = ACT2FN[config.hidden_act]\n","        hidden_size = hidden_size if hidden_size is not None else config.hidden_size\n","        intermediate_size = intermediate_size if intermediate_size is not None else config.intermediate_size\n","        output_size = output_size if output_size is not None else hidden_size\n","        self.fc1 = nn.Linear(hidden_size, intermediate_size)\n","        self.fc2 = nn.Linear(intermediate_size, output_size)\n","\n","    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n","        hidden_states = self.fc1(hidden_states)\n","        hidden_states = self.activation_fn(hidden_states)\n","        hidden_states = self.fc2(hidden_states)\n","        return hidden_states"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1677512217563,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"F5PfLQe2-dnX"},"outputs":[],"source":["class GroupViTMixerMLP(GroupViTMLP):\n","    def forward(self, x):\n","        x = super().forward(x.transpose(1, 2))\n","        return x.transpose(1, 2)"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1677512217564,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"zIDEv8MG-fPE"},"outputs":[],"source":["class GroupViTAttention(nn.Module):\n","    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","        self.embed_dim = config.hidden_size\n","        self.num_heads = config.num_attention_heads\n","        self.head_dim = self.embed_dim // self.num_heads\n","        if self.head_dim * self.num_heads != self.embed_dim:\n","            raise ValueError(\n","                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n","                f\" {self.num_heads}).\"\n","            )\n","        self.scale = self.head_dim**-0.5\n","        self.dropout = config.attention_dropout\n","\n","        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n","        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n","        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n","        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n","\n","    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n","        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n","\n","    def forward(\n","        self,\n","        hidden_states: torch.Tensor,\n","        attention_mask: Optional[torch.Tensor] = None,\n","        causal_attention_mask: Optional[torch.Tensor] = None,\n","        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n","        output_attentions: Optional[bool] = False,\n","    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n","        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n","\n","        bsz, tgt_len, embed_dim = hidden_states.size()\n","        is_cross_attention = encoder_hidden_states is not None\n","\n","        # get query proj\n","        query_states = self.q_proj(hidden_states) * self.scale\n","        if is_cross_attention:\n","            key_states = self._shape(self.k_proj(encoder_hidden_states), -1, bsz)\n","            value_states = self._shape(self.v_proj(encoder_hidden_states), -1, bsz)\n","        else:\n","            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n","            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n","\n","        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n","        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n","        key_states = key_states.view(*proj_shape)\n","        value_states = value_states.view(*proj_shape)\n","\n","        src_len = key_states.size(1)\n","        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n","\n","        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n","            raise ValueError(\n","                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n","                f\" {attn_weights.size()}\"\n","            )\n","\n","        # apply the causal_attention_mask first\n","        if causal_attention_mask is not None:\n","            if causal_attention_mask.size() != (bsz, 1, tgt_len, src_len):\n","                raise ValueError(\n","                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is\"\n","                    f\" {causal_attention_mask.size()}\"\n","                )\n","            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n","            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n","\n","        if attention_mask is not None:\n","            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n","                raise ValueError(\n","                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n","                )\n","            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n","            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n","\n","        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n","\n","        if output_attentions:\n","            # this operation is a bit akward, but it's required to\n","            # make sure that attn_weights keeps its gradient.\n","            # In order to do so, attn_weights have to reshaped\n","            # twice and have to be reused in the following\n","            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n","            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n","        else:\n","            attn_weights_reshaped = None\n","\n","        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n","\n","        attn_output = torch.bmm(attn_probs, value_states)\n","\n","        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n","            raise ValueError(\n","                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n","                f\" {attn_output.size()}\"\n","            )\n","\n","        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n","        attn_output = attn_output.transpose(1, 2)\n","        attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n","\n","        attn_output = self.out_proj(attn_output)\n","\n","        return attn_output, attn_weights_reshaped\n"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1677512217564,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"MxxVauJA-ioM"},"outputs":[],"source":["# Copied from transformers.models.clip.modeling_clip.CLIPEncoderLayer with CLIP->GroupViT\n","class GroupViTEncoderLayer(nn.Module):\n","    def __init__(self, config: GroupViTConfig):\n","        super().__init__()\n","        self.embed_dim = config.hidden_size\n","        self.self_attn = GroupViTAttention(config)\n","        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n","        self.mlp = GroupViTMLP(config)\n","        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n","\n","    def forward(\n","        self,\n","        hidden_states: torch.Tensor,\n","        attention_mask: torch.Tensor,\n","        causal_attention_mask: torch.Tensor,\n","        output_attentions: Optional[bool] = False,\n","    ) -> Tuple[torch.FloatTensor]:\n","        \"\"\"\n","        Args:\n","            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n","            attention_mask (`torch.FloatTensor`): attention mask of size\n","                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n","                `(config.encoder_attention_heads,)`.\n","            output_attentions (`bool`, *optional*):\n","                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n","                returned tensors for more detail.\n","        \"\"\"\n","        residual = hidden_states\n","\n","        hidden_states = self.layer_norm1(hidden_states)\n","        hidden_states, attn_weights = self.self_attn(\n","            hidden_states=hidden_states,\n","            attention_mask=attention_mask,\n","            causal_attention_mask=causal_attention_mask,\n","            output_attentions=output_attentions,\n","        )\n","        hidden_states = residual + hidden_states\n","\n","        residual = hidden_states\n","        hidden_states = self.layer_norm2(hidden_states)\n","        hidden_states = self.mlp(hidden_states)\n","        hidden_states = residual + hidden_states\n","\n","        outputs = (hidden_states,)\n","\n","        if output_attentions:\n","            outputs += (attn_weights,)\n","\n","        return outputs"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1677512217564,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"iFAIWiAZ-k9h"},"outputs":[],"source":["class GroupViTPreTrainedModel(PreTrainedModel):\n","    \"\"\"\n","    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n","    models.\n","    \"\"\"\n","\n","    config_class = GroupViTConfig\n","    base_model_prefix = \"groupvit\"\n","    supports_gradient_checkpointing = True\n","    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n","\n","    def _init_weights(self, module):\n","        \"\"\"Initialize the weights\"\"\"\n","\n","        init_range = self.config.initializer_range\n","        if isinstance(module, (nn.Linear, nn.Conv2d)):\n","            # Slightly different from the TF version which uses truncated_normal for initialization\n","            # cf https://github.com/pytorch/pytorch/pull/5617\n","            module.weight.data.normal_(mean=0.0, std=init_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","\n","        factor = self.config.initializer_factor\n","        if isinstance(module, GroupViTTextEmbeddings):\n","            module.token_embedding.weight.data.normal_(mean=0.0, std=factor * 0.02)\n","            module.position_embedding.weight.data.normal_(mean=0.0, std=factor * 0.02)\n","        elif isinstance(module, GroupViTAttention):\n","            factor = self.config.initializer_factor\n","            in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n","            out_proj_std = (module.embed_dim**-0.5) * factor\n","            nn.init.normal_(module.q_proj.weight, std=in_proj_std)\n","            nn.init.normal_(module.k_proj.weight, std=in_proj_std)\n","            nn.init.normal_(module.v_proj.weight, std=in_proj_std)\n","            nn.init.normal_(module.out_proj.weight, std=out_proj_std)\n","        elif isinstance(module, GroupViTMLP):\n","            factor = self.config.initializer_factor\n","            in_proj_std = (\n","                (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n","            )\n","            fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n","            nn.init.normal_(module.fc1.weight, std=fc_std)\n","            nn.init.normal_(module.fc2.weight, std=in_proj_std)\n","\n","    def _set_gradient_checkpointing(self, module, value=False):\n","        if isinstance(module, (GroupViTTextEncoder, GroupViTVisionEncoder)):\n","            module.gradient_checkpointing = value"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1677512217564,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"jzru1jUe-wAJ"},"outputs":[],"source":["GROUPVIT_START_DOCSTRING = r\"\"\"\n","    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n","    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n","    behavior.\n","\n","    Parameters:\n","        config ([`GroupViTConfig`]): Model configuration class with all the parameters of the model.\n","            Initializing with a config file does not load the weights associated with the model, only the\n","            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n","\"\"\"\n","\n","GROUPVIT_TEXT_INPUTS_DOCSTRING = r\"\"\"\n","    Args:\n","        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n","            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n","            it.\n","\n","            Indices can be obtained using [`CLIPTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n","            [`PreTrainedTokenizer.__call__`] for details.\n","\n","            [What are input IDs?](../glossary#input-ids)\n","        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n","            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n","\n","            - 1 for tokens that are **not masked**,\n","            - 0 for tokens that are **masked**.\n","\n","            [What are attention masks?](../glossary#attention-mask)\n","        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n","            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n","            config.max_position_embeddings - 1]`.\n","\n","            [What are position IDs?](../glossary#position-ids)\n","        output_attentions (`bool`, *optional*):\n","            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n","            tensors for more detail.\n","        output_hidden_states (`bool`, *optional*):\n","            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n","            more detail.\n","        return_dict (`bool`, *optional*):\n","            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n","\"\"\"\n","\n","GROUPVIT_VISION_INPUTS_DOCSTRING = r\"\"\"\n","    Args:\n","        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n","            Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using\n","            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details.\n","        output_attentions (`bool`, *optional*):\n","            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n","            tensors for more detail.\n","        output_hidden_states (`bool`, *optional*):\n","            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n","            more detail.\n","        return_dict (`bool`, *optional*):\n","            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n","\"\"\"\n","\n","GROUPVIT_INPUTS_DOCSTRING = r\"\"\"\n","    Args:\n","        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n","            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n","            it.\n","\n","            Indices can be obtained using [`CLIPTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n","            [`PreTrainedTokenizer.__call__`] for details.\n","\n","            [What are input IDs?](../glossary#input-ids)\n","        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n","            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n","\n","            - 1 for tokens that are **not masked**,\n","            - 0 for tokens that are **masked**.\n","\n","            [What are attention masks?](../glossary#attention-mask)\n","        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n","            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n","            config.max_position_embeddings - 1]`.\n","\n","            [What are position IDs?](../glossary#position-ids)\n","        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n","            Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See\n","            [`CLIPImageProcessor.__call__`] for details.\n","        return_loss (`bool`, *optional*):\n","            Whether or not to return the contrastive loss.\n","        output_attentions (`bool`, *optional*):\n","            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n","            tensors for more detail.\n","        output_hidden_states (`bool`, *optional*):\n","            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n","            more detail.\n","        return_dict (`bool`, *optional*):\n","            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n","\"\"\""]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1677512217564,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"diwTOUkc-yuM"},"outputs":[],"source":["class GroupViTVisionEncoder(nn.Module):\n","    def __init__(self, config: GroupViTVisionConfig) -> None:\n","        super().__init__()\n","        self.config = config\n","        self.stages = nn.ModuleList(\n","            [\n","                GroupViTStage(\n","                    config=config,\n","                    depth=config.depths[i],\n","                    num_group_token=config.num_group_tokens[i],\n","                    num_output_group=config.num_output_groups[i],\n","                    num_prev_group_token=config.num_output_groups[i - 1] if i > 0 else 0,\n","                )\n","                for i in range(len(config.depths))\n","            ]\n","        )\n","        self.gradient_checkpointing = False\n","\n","    def forward(\n","        self,\n","        hidden_states: torch.Tensor,\n","        output_hidden_states: Optional[bool] = None,\n","        output_attentions: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","    ) -> Union[tuple, BaseModelOutput]:\n","        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n","        output_hidden_states = (\n","            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n","        )\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        all_hidden_states = () if output_hidden_states else None\n","        all_groupings = () if output_attentions else None\n","\n","        group_tokens = None\n","\n","        for i, stage in enumerate(self.stages):\n","            if output_hidden_states:\n","                all_hidden_states = all_hidden_states + (hidden_states,)\n","\n","            layer_outputs = stage(hidden_states, group_tokens, output_attentions)\n","\n","            hidden_states = layer_outputs[0]\n","            group_tokens = layer_outputs[1]\n","\n","            if output_attentions and layer_outputs[2] is not None:\n","                all_groupings = all_groupings + (layer_outputs[2],)\n","\n","        if output_hidden_states:\n","            all_hidden_states = all_hidden_states + (hidden_states,)\n","\n","        if not return_dict:\n","            return tuple(v for v in [hidden_states, all_hidden_states, all_groupings] if v is not None)\n","        return BaseModelOutput(\n","            last_hidden_state=hidden_states, hidden_states=all_hidden_states, attentions=all_groupings\n","        )"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1677512217565,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"ErgnsQgL-zvh"},"outputs":[],"source":["class GroupViTTextEncoder(nn.Module):\n","    \"\"\"\n","    Transformer encoder consisting of `config.num_hidden_layers` self-attention layers. Each layer is a\n","    [`GroupViTEncoderLayer`].\n","\n","    Args:\n","        config: GroupViTTextConfig\n","    \"\"\"\n","\n","    def __init__(self, config: GroupViTTextConfig):\n","        super().__init__()\n","        self.config = config\n","        self.layers = nn.ModuleList([GroupViTEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n","        self.gradient_checkpointing = False\n","\n","    def forward(\n","        self,\n","        inputs_embeds,\n","        attention_mask: Optional[torch.Tensor] = None,\n","        causal_attention_mask: Optional[torch.Tensor] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","    ) -> Union[Tuple, BaseModelOutput]:\n","        r\"\"\"\n","        Args:\n","            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n","                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n","                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n","                than the model's internal embedding lookup matrix.\n","            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n","                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n","\n","                - 1 for tokens that are **not masked**,\n","                - 0 for tokens that are **masked**.\n","\n","                [What are attention masks?](../glossary#attention-mask)\n","            causal_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n","                Causal mask for the text model. Mask values selected in `[0, 1]`:\n","\n","                - 1 for tokens that are **not masked**,\n","                - 0 for tokens that are **masked**.\n","\n","                [What are attention masks?](../glossary#attention-mask)\n","            output_attentions (`bool`, *optional*):\n","                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n","                returned tensors for more detail.\n","            output_hidden_states (`bool`, *optional*):\n","                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n","                for more detail.\n","            return_dict (`bool`, *optional*):\n","                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n","        \"\"\"\n","        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n","        output_hidden_states = (\n","            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n","        )\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        encoder_states = () if output_hidden_states else None\n","        all_attentions = () if output_attentions else None\n","\n","        hidden_states = inputs_embeds\n","        for idx, encoder_layer in enumerate(self.layers):\n","            if output_hidden_states:\n","                encoder_states = encoder_states + (hidden_states,)\n","            if self.gradient_checkpointing and self.training:\n","\n","                def create_custom_forward(module):\n","                    def custom_forward(*inputs):\n","                        return module(*inputs, output_attentions)\n","\n","                    return custom_forward\n","\n","                layer_outputs = torch.utils.checkpoint.checkpoint(\n","                    create_custom_forward(encoder_layer),\n","                    hidden_states,\n","                    attention_mask,\n","                    causal_attention_mask,\n","                )\n","            else:\n","                layer_outputs = encoder_layer(\n","                    hidden_states,\n","                    attention_mask,\n","                    causal_attention_mask,\n","                    output_attentions=output_attentions,\n","                )\n","\n","            hidden_states = layer_outputs[0]\n","\n","            if output_attentions:\n","                all_attentions = all_attentions + (layer_outputs[1],)\n","\n","        if output_hidden_states:\n","            encoder_states = encoder_states + (hidden_states,)\n","\n","        if not return_dict:\n","            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n","        return BaseModelOutput(\n","            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n","        )"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1677512217566,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"fsgeLvGW_gB0"},"outputs":[],"source":["# Copied from transformers.models.clip.modeling_clip.CLIPTextTransformer with CLIPText->GroupViTText, CLIPEncoder->GroupViTTextEncoder, CLIP_TEXT->GROUPVIT_TEXT\n","class GroupViTTextTransformer(nn.Module):\n","    def __init__(self, config: GroupViTTextConfig):\n","        super().__init__()\n","        self.config = config\n","        embed_dim = config.hidden_size\n","        self.embeddings = GroupViTTextEmbeddings(config)\n","        self.encoder = GroupViTTextEncoder(config)\n","        self.final_layer_norm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n","\n","    @add_start_docstrings_to_model_forward(GROUPVIT_TEXT_INPUTS_DOCSTRING)\n","    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=GroupViTTextConfig)\n","    def forward(\n","        self,\n","        input_ids: Optional[torch.Tensor] = None,\n","        attention_mask: Optional[torch.Tensor] = None,\n","        position_ids: Optional[torch.Tensor] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n","        r\"\"\"\n","        Returns:\n","\n","        \"\"\"\n","        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n","        output_hidden_states = (\n","            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n","        )\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        if input_ids is None:\n","            raise ValueError(\"You have to specify input_ids\")\n","\n","        input_shape = input_ids.size()\n","        input_ids = input_ids.view(-1, input_shape[-1])\n","\n","        hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n","\n","        bsz, seq_len = input_shape\n","        # CLIP's text model uses causal mask, prepare it here.\n","        # https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/clip/model.py#L324\n","        causal_attention_mask = self._build_causal_attention_mask(bsz, seq_len, hidden_states.dtype).to(\n","            hidden_states.device\n","        )\n","        # expand attention_mask\n","        if attention_mask is not None:\n","            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n","            attention_mask = _expand_mask(attention_mask, hidden_states.dtype)\n","\n","        encoder_outputs = self.encoder(\n","            inputs_embeds=hidden_states,\n","            attention_mask=attention_mask,\n","            causal_attention_mask=causal_attention_mask,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        last_hidden_state = encoder_outputs[0]\n","        last_hidden_state = self.final_layer_norm(last_hidden_state)\n","\n","        # text_embeds.shape = [batch_size, sequence_length, transformer.width]\n","        # take features from the eot embedding (eot_token is the highest number in each sequence)\n","        # casting to torch.int for onnx compatibility: argmax doesn't support int64 inputs with opset 14\n","        pooled_output = last_hidden_state[\n","            torch.arange(last_hidden_state.shape[0], device=last_hidden_state.device),\n","            input_ids.to(dtype=torch.int, device=last_hidden_state.device).argmax(dim=-1),\n","        ]\n","\n","        if not return_dict:\n","            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n","\n","        return BaseModelOutputWithPooling(\n","            last_hidden_state=last_hidden_state,\n","            pooler_output=pooled_output,\n","            hidden_states=encoder_outputs.hidden_states,\n","            attentions=encoder_outputs.attentions,\n","        )\n","\n","    def _build_causal_attention_mask(self, bsz, seq_len, dtype):\n","        # lazily create causal attention mask, with full attention between the vision tokens\n","        # pytorch uses additive attention mask; fill with -inf\n","        mask = torch.empty(bsz, seq_len, seq_len, dtype=dtype)\n","        mask.fill_(torch.tensor(torch.finfo(dtype).min))\n","        mask.triu_(1)  # zero out the lower diagonal\n","        mask = mask.unsqueeze(1)  # expand mask\n","        return mask"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":1409,"status":"ok","timestamp":1677512218961,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"QAAhPken_ibc"},"outputs":[],"source":["class GroupViTTextModel(GroupViTPreTrainedModel):\n","    config_class = GroupViTTextConfig\n","\n","    def __init__(self, config: GroupViTTextConfig):\n","        super().__init__(config)\n","        self.text_model = GroupViTTextTransformer(config)\n","        # Initialize weights and apply final processing\n","        self.post_init()\n","\n","    def get_input_embeddings(self) -> nn.Module:\n","        return self.text_model.embeddings.token_embedding\n","\n","    def set_input_embeddings(self, value):\n","        self.text_model.embeddings.token_embedding = value\n","\n","    @add_start_docstrings_to_model_forward(GROUPVIT_TEXT_INPUTS_DOCSTRING)\n","    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=GroupViTTextConfig)\n","    def forward(\n","        self,\n","        input_ids: Optional[torch.Tensor] = None,\n","        attention_mask: Optional[torch.Tensor] = None,\n","        position_ids: Optional[torch.Tensor] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n","        r\"\"\"\n","        Returns:\n","\n","        Examples:\n","\n","        ```python\n","        >>> from transformers import CLIPTokenizer, GroupViTTextModel\n","\n","        >>> tokenizer = CLIPTokenizer.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n","        >>> model = GroupViTTextModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n","\n","        >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n","\n","        >>> outputs = model(**inputs)\n","        >>> last_hidden_state = outputs.last_hidden_state\n","        >>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n","        ```\"\"\"\n","        return self.text_model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            position_ids=position_ids,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1677512218961,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"pNJ4HozD_ls2"},"outputs":[],"source":["class GroupViTVisionTransformer(nn.Module):########################\n","    def __init__(self, config: GroupViTVisionConfig):\n","        super().__init__()\n","        self.config = config\n","        embed_dim = config.hidden_size\n","\n","        self.embeddings = GroupViTVisionEmbeddings(config)\n","        self.encoder = GroupViTVisionEncoder(config)\n","        self.layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n","\n","    @add_start_docstrings_to_model_forward(GROUPVIT_VISION_INPUTS_DOCSTRING)\n","    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=GroupViTVisionConfig)\n","    def forward(\n","        self,\n","        pixel_values: Optional[torch.FloatTensor] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        output_attentions: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n","        r\"\"\"\n","        Returns:\n","\n","        \"\"\"\n","        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n","        output_hidden_states = (\n","            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n","        )\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        if pixel_values is None:\n","            raise ValueError(\"You have to specify pixel_values\")\n","\n","        hidden_states = self.embeddings(pixel_values)\n","\n","        encoder_outputs = self.encoder(\n","            hidden_states=hidden_states,\n","            output_hidden_states=output_hidden_states,\n","            output_attentions=output_attentions,\n","            return_dict=return_dict,\n","        )\n","\n","        last_hidden_state = encoder_outputs[0]\n","\n","        # normalize the last hidden state\n","        last_hidden_state = self.layernorm(last_hidden_state)\n","        pooled_output = last_hidden_state.mean(dim=1)\n","\n","        if not return_dict:\n","            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n","\n","        return BaseModelOutputWithPooling(\n","            last_hidden_state=last_hidden_state,\n","            pooler_output=pooled_output,\n","            hidden_states=encoder_outputs.hidden_states,\n","            attentions=encoder_outputs.attentions,\n","        )"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1677512218961,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"Tf5zCFfp_pS0"},"outputs":[],"source":["class GroupViTVisionModel(GroupViTPreTrainedModel):\n","    config_class = GroupViTVisionConfig\n","    main_input_name = \"pixel_values\"\n","\n","    def __init__(self, config: GroupViTVisionConfig):\n","        super().__init__(config)\n","        self.vision_model = GroupViTVisionTransformer(config)\n","\n","        self.projection_dim = 128\n","        self.projection_intermediate_dim = 4096\n","        self.vision_embed_dim = config.hidden_size\n","\n","        self.visual_projection = nn.Sequential(\n","            nn.Linear(self.vision_embed_dim, self.projection_intermediate_dim, bias=True),\n","            nn.BatchNorm1d(self.projection_intermediate_dim),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(self.projection_intermediate_dim, self.projection_dim, bias=True),\n","        )\n","\n","        # Initialize weights and apply final processing\n","        self.post_init()\n","\n","    def get_input_embeddings(self) -> GroupViTPatchEmbeddings:\n","        return self.vision_model.embeddings.patch_embeddings\n","\n","    @add_start_docstrings_to_model_forward(GROUPVIT_VISION_INPUTS_DOCSTRING)\n","    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=GroupViTVisionConfig)\n","    def forward(\n","        self,\n","        pixel_values: Optional[torch.FloatTensor] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n","        r\"\"\"\n","        Returns:\n","\n","        Examples:\n","\n","        ```python\n","        >>> from PIL import Image\n","        >>> import requests\n","        >>> from transformers import AutoProcessor, GroupViTVisionModel\n","\n","        >>> processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n","        >>> model = GroupViTVisionModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n","\n","        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n","        >>> image = Image.open(requests.get(url, stream=True).raw)\n","\n","        >>> inputs = processor(images=image, return_tensors=\"pt\")\n","\n","        >>> outputs = model(**inputs)\n","        >>> last_hidden_state = outputs.last_hidden_state\n","        >>> pooled_output = outputs.pooler_output  # pooled CLS states\n","        ```\"\"\"\n","\n","        # print(type(pixel_values), type(output_attentions), type(output_hidden_states), type(return_dict))\n","        # print(pixel_values.shape, output_attentions, output_hidden_states, return_dict)\n","        # print('pixel_values=', pixel_values.shape)\n","        output_attentions = True\n","        output_hidden_states = False\n","        return_dict = True\n","        # print(pixel_values.shape, output_attentions, output_hidden_states, return_dict)\n","        vision_outputs = self.vision_model(\n","            pixel_values=pixel_values,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","        # print(vision_outputs)\n","\n","        attentions = vision_outputs[2]\n","            \n","        # [batch_size_image, num_group, height, width]\n","        grouping = get_grouping_from_attentions(attentions, pixel_values.shape[2:])\n","        seg_logits = grouping\n","\n","        pooled_output = vision_outputs[1]  # pooled_output\n","        image_features = self.visual_projection(pooled_output)\n","\n","        # print(image_features.shape)\n","        return vision_outputs, seg_logits, image_features"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1677512218962,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"Q3lIeXoH_slB"},"outputs":[],"source":["@add_start_docstrings(GROUPVIT_START_DOCSTRING)\n","class GroupViTModel(GroupViTPreTrainedModel):\n","    config_class = GroupViTConfig\n","\n","    def __init__(self, config: GroupViTConfig):\n","        super().__init__(config)\n","\n","        # if not isinstance(config.text_config, GroupViTTextConfig):\n","        #     raise ValueError(\n","        #         \"config.text_config is expected to be of type GroupViTTextConfig but is of type\"\n","        #         f\" {type(config.text_config)}.\"\n","        #     )\n","\n","        if not isinstance(config.vision_config, GroupViTVisionConfig):\n","            raise ValueError(\n","                \"config.vision_config is expected to be of type GroupViTVisionConfig but is of type\"\n","                f\" {type(config.vision_config)}.\"\n","            )\n","\n","        # text_config = config.text_config\n","        vision_config = config.vision_config\n","\n","        self.projection_dim = config.projection_dim\n","        self.projection_intermediate_dim = config.projection_intermediate_dim\n","        # self.text_embed_dim = text_config.hidden_size\n","        self.vision_embed_dim = vision_config.hidden_size\n","        print('hidden_size', vision_config.hidden_size)\n","\n","        # self.text_model = GroupViTTextTransformer(text_config)\n","        self.vision_model = GroupViTVisionTransformer(vision_config)\n","\n","        self.visual_projection = nn.Sequential(\n","            nn.Linear(self.vision_embed_dim, self.projection_intermediate_dim, bias=True),\n","            nn.BatchNorm1d(self.projection_intermediate_dim),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(self.projection_intermediate_dim, self.projection_dim, bias=True),\n","        )\n","        # self.text_projection = nn.Sequential(\n","        #     nn.Linear(self.text_embed_dim, self.projection_intermediate_dim, bias=True),\n","        #     nn.BatchNorm1d(self.projection_intermediate_dim),\n","        #     nn.ReLU(inplace=True),\n","        #     nn.Linear(self.projection_intermediate_dim, self.projection_dim, bias=True),\n","        # )\n","        self.logit_scale = nn.Parameter(torch.ones([]) * self.config.logit_scale_init_value)\n","\n","        # Initialize weights and apply final processing\n","        self.post_init()\n","\n","    # @add_start_docstrings_to_model_forward(GROUPVIT_TEXT_INPUTS_DOCSTRING)\n","    # def get_text_features(\n","    #     self,\n","    #     input_ids: Optional[torch.Tensor] = None,\n","    #     attention_mask: Optional[torch.Tensor] = None,\n","    #     position_ids: Optional[torch.Tensor] = None,\n","    #     output_attentions: Optional[bool] = None,\n","    #     output_hidden_states: Optional[bool] = None,\n","    #     return_dict: Optional[bool] = None,\n","    # ) -> torch.FloatTensor:\n","    #     r\"\"\"\n","    #     Returns:\n","    #         text_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The text embeddings obtained by\n","    #         applying the projection layer to the pooled output of [`GroupViTTextModel`].\n","\n","    #     Examples:\n","\n","    #     ```python\n","    #     >>> from transformers import CLIPTokenizer, GroupViTModel\n","\n","    #     >>> model = GroupViTModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n","    #     >>> tokenizer = CLIPTokenizer.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n","\n","    #     >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n","    #     >>> text_features = model.get_text_features(**inputs)\n","    #     ```\"\"\"\n","    #     # Use GROUPVIT model's config for some fields (if specified) instead of those of vision & text components.\n","    #     output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n","    #     output_hidden_states = (\n","    #         output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n","    #     )\n","    #     return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","    #     text_outputs = self.text_model(\n","    #         input_ids=input_ids,\n","    #         attention_mask=attention_mask,\n","    #         position_ids=position_ids,\n","    #         output_attentions=output_attentions,\n","    #         output_hidden_states=output_hidden_states,\n","    #         return_dict=return_dict,\n","    #     )\n","\n","    #     pooled_output = text_outputs[1]\n","    #     text_features = self.text_projection(pooled_output)\n","\n","    #     return text_features\n","\n","    @add_start_docstrings_to_model_forward(GROUPVIT_VISION_INPUTS_DOCSTRING)\n","    def get_image_features(\n","        self,\n","        pixel_values: Optional[torch.FloatTensor] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","    ) -> torch.FloatTensor:\n","        r\"\"\"\n","        Returns:\n","            image_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The image embeddings obtained by\n","            applying the projection layer to the pooled output of [`GroupViTVisionModel`].\n","\n","        Examples:\n","\n","        ```python\n","        >>> from PIL import Image\n","        >>> import requests\n","        >>> from transformers import AutoProcessor, GroupViTModel\n","\n","        >>> model = GroupViTModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n","        >>> processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n","\n","        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n","        >>> image = Image.open(requests.get(url, stream=True).raw)\n","\n","        >>> inputs = processor(images=image, return_tensors=\"pt\")\n","\n","        >>> image_features = model.get_image_features(**inputs)\n","        ```\"\"\"\n","        # Use GROUPVIT model's config for some fields (if specified) instead of those of vision & text components.\n","        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n","        output_hidden_states = (\n","            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n","        )\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        print(pixel_values.shape, output_attentions, output_hidden_states, return_dict)\n","\n","        vision_outputs = self.vision_model(\n","            pixel_values=pixel_values,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        pooled_output = vision_outputs[1]  # pooled_output\n","        print('01 ', pooled_output.shape)\n","\n","        image_features = self.visual_projection(pooled_output)\n","        print('02 ', image_features.shape)\n","\n","        return image_features\n","\n","    @add_start_docstrings_to_model_forward(GROUPVIT_INPUTS_DOCSTRING)\n","    @replace_return_docstrings(output_type=GroupViTModelOutput, config_class=GroupViTConfig)\n","    def forward(\n","        self,\n","        input_ids: Optional[torch.LongTensor] = None,\n","        pixel_values: Optional[torch.FloatTensor] = None,\n","        attention_mask: Optional[torch.Tensor] = None,\n","        position_ids: Optional[torch.LongTensor] = None,\n","        return_loss: Optional[bool] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        output_segmentation: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","    ) -> Union[Tuple, GroupViTModelOutput]:\n","        r\"\"\"\n","        Returns:\n","\n","        Examples:\n","\n","        ```python\n","        >>> from PIL import Image\n","        >>> import requests\n","        >>> from transformers import AutoProcessor, GroupViTModel\n","\n","        >>> model = GroupViTModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n","        >>> processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n","\n","        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n","        >>> image = Image.open(requests.get(url, stream=True).raw)\n","\n","        >>> inputs = processor(\n","        ...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True\n","        ... )\n","\n","        >>> outputs = model(**inputs)\n","        >>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n","        >>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n","        ```\"\"\"\n","        # Use GROUPVIT model's config for some fields (if specified) instead of those of vision & text components.\n","        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n","        output_segmentation = (\n","            output_segmentation if output_segmentation is not None else self.config.output_segmentation\n","        )\n","        if output_segmentation:\n","            output_attentions = True\n","        output_hidden_states = (\n","            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n","        )\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        # print(type(pixel_values), type(output_attentions), type(output_hidden_states), type(return_dict))\n","        print(pixel_values.shape, output_attentions, output_hidden_states, return_dict)\n","\n","        vision_outputs = self.vision_model(\n","            pixel_values=pixel_values,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","        # print(vision_outputs)\n","\n","        # text_outputs = self.text_model(\n","        #     input_ids=input_ids,\n","        #     attention_mask=attention_mask,\n","        #     position_ids=position_ids,\n","        #     output_attentions=output_attentions,\n","        #     output_hidden_states=output_hidden_states,\n","        #     return_dict=return_dict,\n","        # )\n","\n","        image_embeds = vision_outputs[1]\n","        image_embeds = self.visual_projection(image_embeds)\n","\n","        # text_embeds = text_outputs[1]\n","        # text_embeds = self.text_projection(text_embeds)\n","\n","        # normalized features\n","        image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n","        # text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n","\n","        # cosine similarity as logits\n","        # logit_scale = self.logit_scale.exp()\n","        # logits_per_text = torch.matmul(text_embeds, image_embeds.t()) * logit_scale\n","        # logits_per_image = logits_per_text.t()\n","\n","        seg_logits = None\n","        if output_segmentation:\n","            # grouped features\n","            # [batch_size_image, num_group, hidden_size]\n","            image_group_embeds = vision_outputs[0]\n","            print('image_group_embeds_01', image_group_embeds.shape, type(image_group_embeds)) # torch.Size([1, 8, 384]) <class 'torch.Tensor'>\n","\n","            # [batch_size_image*num_group, hidden_size]\n","            image_group_embeds = self.visual_projection(image_group_embeds.reshape(-1, image_group_embeds.shape[-1]))\n","            print('image_group_embeds_02', image_group_embeds.shape, type(image_group_embeds)) # torch.Size([8, 256]) <class 'torch.Tensor'>\n","\n","            if output_hidden_states:\n","                attentions = vision_outputs[3]\n","                print('attentions_01', attentions.shape, type(attentions)) # *\n","\n","            else:\n","                attentions = vision_outputs[2]\n","                print('attentions_02', attentions[0].shape, type(attentions[0]), attentions[1].shape, type(attentions[1])) # torch.Size([1, 64, 196]) torch.Size([1, 8, 64]) <class 'torch.Tensor'>\n","                \n","            # [batch_size_image, num_group, height, width]\n","            grouping = get_grouping_from_attentions(attentions, pixel_values.shape[2:])\n","            print(pixel_values.shape)\n","            print(pixel_values.shape[2:])\n","            print('grouping_01', grouping.shape, type(grouping)) # torch.Size([1, 8, 224, 224]) <class 'torch.Tensor'>\n","            seg_logits = grouping\n","\n","            # # normalized features\n","            # image_group_embeds = image_group_embeds / image_group_embeds.norm(dim=-1, keepdim=True)\n","            # print('image_group_embeds_03', image_group_embeds.shape, type(image_group_embeds)) # torch.Size([8, 256]) <class 'torch.Tensor'>\n","\n","            # # [batch_size_image x num_group, batch_size_text]\n","            # logits_per_image_group = torch.matmul(image_group_embeds, text_embeds.t()) * logit_scale\n","            # print('logits_per_image_group_01', logits_per_image_group.shape, type(logits_per_image_group)) # torch.Size([8, 3]) <class 'torch.Tensor'>\n","\n","            # # [batch_size_image, batch_size_text, num_group]\n","            # logits_per_image_group = logits_per_image_group.reshape(\n","            #     image_embeds.shape[0], -1, text_embeds.shape[0]\n","            # ).permute(0, 2, 1)\n","            # print('logits_per_image_group_02', logits_per_image_group.shape, type(logits_per_image_group)) # torch.Size([1, 3, 8]) <class 'torch.Tensor'>\n","\n","\n","            # # [batch_size_image, batch_size_text, height x width]\n","            # flatten_grouping = grouping.reshape(grouping.shape[0], grouping.shape[1], -1)\n","            # print('flatten_grouping_01', flatten_grouping.shape, type(flatten_grouping)) # torch.Size([1, 8, 50176]) <class 'torch.Tensor'>\n","\n","\n","            # # [batch_size_image, batch_size_text, height, width]\n","            # seg_logits = torch.matmul(logits_per_image_group, flatten_grouping) * logit_scale\n","            # print('seg_logits_01', seg_logits.shape, type(seg_logits)) # torch.Size([1, 3, 50176]) <class 'torch.Tensor'>\n","\n","            # seg_logits = seg_logits.reshape(\n","            #     seg_logits.shape[0], seg_logits.shape[1], grouping.shape[2], grouping.shape[3]\n","            # )\n","            # print('seg_logits_02', seg_logits.shape, type(seg_logits)) # torch.Size([1, 3, 224, 224]) <class 'torch.Tensor'>\n","\n","        loss = None\n","        if return_loss:\n","            loss = groupvit_loss(logits_per_text)\n","\n","        if not return_dict:\n","            if seg_logits is not None:\n","                output = (\n","                    logits_per_image,\n","                    logits_per_text,\n","                    seg_logits,\n","                    text_embeds,\n","                    image_embeds,\n","                    text_outputs,\n","                    vision_outputs,\n","                )\n","            else:\n","                output = (logits_per_image, logits_per_text, text_embeds, image_embeds, text_outputs, vision_outputs)\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return GroupViTModelOutput(\n","            loss=loss,\n","            # logits_per_image=logits_per_image,\n","            # logits_per_text=logits_per_text,\n","            segmentation_logits=seg_logits,\n","            # text_embeds=text_embeds,\n","            image_embeds=image_embeds,\n","            # text_model_output=text_outputs,\n","            vision_model_output=vision_outputs,\n","        )"]},{"cell_type":"markdown","metadata":{"id":"uuwerq7N9s5S"},"source":["model import end"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1677512218962,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"dvuxcmejkKt8","outputId":"4e6311d9-6fd0-43ed-c40a-e5541e35f105"},"outputs":[{"name":"stdout","output_type":"stream","text":["Namespace(arch='resnet18', batch_size=8, bn_splits=8, cos=True, epochs=200, knn_k=200, knn_t=0.1, lr=0.06, moco_dim=128, moco_k=4096, moco_m=0.99, moco_t=0.1, results_dir='/home/yasaisen/Desktop/11_research/11_research_main/lab_02/230228_v0.0.22023-03-01-15-05-49-moco', resume='', schedule=[], symmetric=False, wd=0.0005)\n"]}],"source":["parser = argparse.ArgumentParser(description='Train MoCo on CIFAR-10')\n","\n","parser.add_argument('-a', '--arch', default='resnet18')\n","\n","# lr: 0.06 for batch 512 (or 0.03 for batch 256)\n","parser.add_argument('--lr', '--learning-rate', default=0.06, type=float, metavar='LR', help='initial learning rate', dest='lr')\n","parser.add_argument('--epochs', default=200, type=int, metavar='N', help='number of total epochs to run')\n","parser.add_argument('--schedule', default=[120, 160], nargs='*', type=int, help='learning rate schedule (when to drop lr by 10x); does not take effect if --cos is on')\n","parser.add_argument('--cos', action='store_true', help='use cosine lr schedule')\n","\n","parser.add_argument('--batch-size', default=8, type=int, metavar='N', help='mini-batch size')\n","parser.add_argument('--wd', default=5e-4, type=float, metavar='W', help='weight decay')\n","\n","# moco specific configs:\n","parser.add_argument('--moco-dim', default=128, type=int, help='feature dimension')\n","parser.add_argument('--moco-k', default=4096, type=int, help='queue size; number of negative keys')\n","parser.add_argument('--moco-m', default=0.99, type=float, help='moco momentum of updating key encoder')\n","parser.add_argument('--moco-t', default=0.1, type=float, help='softmax temperature')\n","\n","parser.add_argument('--bn-splits', default=8, type=int, help='simulate multi-gpu behavior of BatchNorm in one gpu; 1 is SyncBatchNorm in multi-gpu')\n","\n","parser.add_argument('--symmetric', action='store_true', help='use a symmetric loss function that backprops to both crops')\n","\n","# knn monitor\n","parser.add_argument('--knn-k', default=200, type=int, help='k in kNN monitor')\n","parser.add_argument('--knn-t', default=0.1, type=float, help='softmax temperature in kNN monitor; could be different with moco-t')\n","\n","# utils\n","parser.add_argument('--resume', default='', type=str, metavar='PATH', help='path to latest checkpoint (default: none)')\n","parser.add_argument('--results-dir', default='', type=str, metavar='PATH', help='path to cache (default: none)')\n","\n","'''\n","args = parser.parse_args()  # running in command line\n","'''\n","args = parser.parse_args('')  # running in ipynb\n","\n","# set command line arguments here when running in ipynb\n","args.epochs = 200\n","args.cos = True\n","args.schedule = []  # cos in use\n","args.symmetric = False\n","if args.results_dir == '':\n","    args.results_dir = root_folder + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S-moco\")\n","\n","print(args)"]},{"cell_type":"markdown","metadata":{"id":"ygQeHtsngrC8"},"source":["### Define data loaders"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1677512218962,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"PvfVDZ8cdNqR"},"outputs":[],"source":["import torchvision.transforms as T\n","from transformers import AutoProcessor"]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1677512218963,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"7GIMNYoPcbDD"},"outputs":[],"source":["def processor(tensor):\n","    processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n","    transform = T.ToPILImage()\n","\n","    img = transform(tensor)\n","    inputs = processor(images=img, return_tensors=\"pt\")\n","    pixel_values = np.array(inputs[\"pixel_values\"])\n","    pixel_values = pixel_values.squeeze()\n","    pixel_values = torch.tensor(pixel_values)\n","\n","    return pixel_values"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":179,"referenced_widgets":["17032952ca804b558931b93c0fde1540","cc091016bb8d423f80dddca00096ba0b","6bbbd0d7f0f749939610ccc1aa47bfec","eb9b54187a9b4404b93ce96cb5297a1e","48854cfeccd84183a1819d703353a4fa","7cdb20bd656249fe85674962d2a4ba7e","20faf3fda02c4257b5ac68099de45581","a4d8fcd96c2b44c1a21e6a0cf46d736d","5bffaeb6f389499c958f4a12dca192e4","d9070ee51e9c4ebe868777345a25a62d","cf90228e48af46809d35402d94eb568e"]},"executionInfo":{"elapsed":23888,"status":"ok","timestamp":1677512242845,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"AoliFX5AnBJ0","outputId":"978c2de8-f216-4ff2-b94f-ae3d2c15be6a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n"]}],"source":["class CIFAR10Pair(CIFAR10):\n","    \"\"\"CIFAR10 Dataset.\n","    \"\"\"\n","    def __getitem__(self, index):\n","        img = self.data[index]\n","        img = Image.fromarray(img)\n","\n","\n","        if self.transform is not None:\n","            im_1 = self.transform(img)\n","            im_2 = self.transform(img)\n","\n","        # return processor(im_1), processor(im_2)\n","        return im_1, im_2\n","\n","train_transform = transforms.Compose([\n","    transforms.RandomResizedCrop(224),\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n","    transforms.RandomGrayscale(p=0.2),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n","\n","test_transform = transforms.Compose([\n","    transforms.RandomResizedCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n","\n","# data prepare\n","train_data = CIFAR10Pair(root='data', train=True, transform=train_transform, download=True)\n","train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True, num_workers=8, pin_memory=True, drop_last=True)\n","\n","memory_data = CIFAR10(root='data', train=True, transform=test_transform, download=True)\n","memory_loader = DataLoader(memory_data, batch_size=args.batch_size, shuffle=False, num_workers=8, pin_memory=True)\n","\n","test_data = CIFAR10(root='data', train=False, transform=test_transform, download=True)\n","test_loader = DataLoader(test_data, batch_size=args.batch_size, shuffle=False, num_workers=8, pin_memory=True)\n"]},{"cell_type":"markdown","metadata":{"id":"KsAVAtRoiBbG"},"source":["### Define base encoder"]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1677512242845,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"bNd_Q_Osi0SO"},"outputs":[],"source":["# SplitBatchNorm: simulate multi-gpu behavior of BatchNorm in one gpu by splitting alone the batch dimension\n","# implementation adapted from https://github.com/davidcpage/cifar10-fast/blob/master/torch_backend.py\n","class SplitBatchNorm(nn.BatchNorm2d):\n","    def __init__(self, num_features, num_splits, **kw):\n","        super().__init__(num_features, **kw)\n","        self.num_splits = num_splits\n","        \n","    def forward(self, input):\n","        N, C, H, W = input.shape\n","        if self.training or not self.track_running_stats:\n","            running_mean_split = self.running_mean.repeat(self.num_splits)\n","            running_var_split = self.running_var.repeat(self.num_splits)\n","            outcome = nn.functional.batch_norm(\n","                input.view(-1, C * self.num_splits, H, W), running_mean_split, running_var_split, \n","                self.weight.repeat(self.num_splits), self.bias.repeat(self.num_splits),\n","                True, self.momentum, self.eps).view(N, C, H, W)\n","            self.running_mean.data.copy_(running_mean_split.view(self.num_splits, C).mean(dim=0))\n","            self.running_var.data.copy_(running_var_split.view(self.num_splits, C).mean(dim=0))\n","            return outcome\n","        else:\n","            return nn.functional.batch_norm(\n","                input, self.running_mean, self.running_var, \n","                self.weight, self.bias, False, self.momentum, self.eps)\n","\n","# class ModelBase(nn.Module):\n","#     \"\"\"\n","#     Common CIFAR ResNet recipe.\n","#     Comparing with ImageNet ResNet recipe, it:\n","#     (i) replaces conv1 with kernel=3, str=1\n","#     (ii) removes pool1\n","#     \"\"\"\n","#     def __init__(self, feature_dim=128, arch=None, bn_splits=16):\n","#         super(ModelBase, self).__init__()\n","\n","#         # use split batchnorm\n","#         norm_layer = partial(SplitBatchNorm, num_splits=bn_splits) if bn_splits > 1 else nn.BatchNorm2d\n","#         resnet_arch = getattr(resnet, arch)\n","#         net = resnet_arch(num_classes=feature_dim, norm_layer=norm_layer)\n","\n","#         self.net = []\n","#         for name, module in net.named_children():\n","#             if name == 'conv1':\n","#                 module = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","#             if isinstance(module, nn.MaxPool2d):\n","#                 continue\n","#             if isinstance(module, nn.Linear):\n","#                 self.net.append(nn.Flatten(1))\n","#             self.net.append(module)\n","\n","#         self.net = nn.Sequential(*self.net)\n","\n","#     def forward(self, x):\n","#         x = self.net(x)\n","#         # note: not normalized here\n","#         return x"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1677512242846,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"ZpKV6k_koZw8"},"outputs":[],"source":["# # generate embedding for images\n","# def embed_images(images, processor, model):\n","#     inputs = processor(images=images)\n","#     pixel_values = torch.tensor(np.array(inputs[\"pixel_values\"]))\n","\n","#     with torch.no_grad():\n","#         embeddings = model.get_image_features(pixel_values=pixel_values)\n","#     return embeddings"]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":24,"status":"ok","timestamp":1677512242846,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"xnA_hHbFXlgp"},"outputs":[],"source":["class ModelBase(nn.Module):\n","    def __init__(self, feature_dim=128, arch=None, bn_splits=16):\n","        super(ModelBase, self).__init__()\n","\n","        # self.processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n","        self.model = GroupViTVisionModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\", ignore_mismatched_sizes=True)\n","\n","    def forward(self, x):# x = self.net(x)\n","\n","        vision_outputs, logits, image_features= self.model(x)\n","\n","        return image_features"]},{"cell_type":"markdown","metadata":{"id":"jJFOHlOBpLay"},"source":["### Define MoCo wrapper"]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":24,"status":"ok","timestamp":1677512242846,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"TiHBQ_4WMr3f"},"outputs":[],"source":["# model = ModelBase(feature_dim=128, arch='resnet18', bn_splits=8)"]},{"cell_type":"code","execution_count":44,"metadata":{"executionInfo":{"elapsed":24,"status":"ok","timestamp":1677512242847,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"XgC0NQBjNAcF"},"outputs":[],"source":["# t = torch.randn((1,3,244,244))\n","# t.shape"]},{"cell_type":"code","execution_count":45,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1677512242847,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"S66RB6EvNHvt"},"outputs":[],"source":["# model(t).shape"]},{"cell_type":"code","execution_count":46,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1677512242847,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"Azh9Lkd5a5pb"},"outputs":[],"source":["# t = t.squeeze()\n","# t.shape"]},{"cell_type":"code","execution_count":47,"metadata":{"executionInfo":{"elapsed":934,"status":"ok","timestamp":1677512243758,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"uOM1OxxUa5CB"},"outputs":[],"source":["# print('10_', type(pixel_values), pixel_values.shape)\n","#             pixel_values = pixel_values.unsqueeze(0)\n","#             print('10_', type(pixel_values), pixel_values.shape)"]},{"cell_type":"code","execution_count":48,"metadata":{"cellView":"both","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["aeabbf27ff5e4ef5a0c3de652c86d632","5528a5ca02bc4cde9fa1bad142519e35","c3b291f042fe422aa3a2b22f1208d0ca","23596ead25e842da821eee9281ddd44b","d74ac37d81974a4780581b554f3e7d23","79cc1507b29d4125ae5e3c704b2b8b75","31ea43544fcd44a19f86c920afaa12ab","73487b45ea444f8e81410a9627c85b40","3e187d79da6f477180cac4b95d949388","ce5a59adc10e4ad3bbd5ee949704a493","2e92aabde375495f880ae9752b55e057","5d806cd2fe7e4424b7fb90371815f4d6","3deab35d86db420c963d21fde4f4f770","e3214b7f823d4a12bed90f36b6cd3bbe","6bec28962d4740aabe8e9896e00134b7","5da754b21ef34dc9a7eff14bdc3a34bc","1d94bafa0c204861869e3c8656f88338","06768cfdf690415e9affdf2a74e59a30","f48ad91e88b340258d9fbe66bc58696b","9cf2f5d1a7894a73861ce8bd69675ed1","d083420df95d42ecb16cedd5037dc2a3","55606bf393b940278f76baf65170eeab"]},"executionInfo":{"elapsed":16199,"status":"ok","timestamp":1677512259937,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"lzFyFnhbk8hj","outputId":"9f79d4b9-3c50-4991-e28b-d5632b74bbc2"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at nvidia/groupvit-gcc-yfcc were not used when initializing GroupViTVisionModel: ['text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_projection.1.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_projection.1.weight', 'text_projection.1.running_var', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_projection.3.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.final_layer_norm.weight', 'text_projection.1.num_batches_tracked', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_projection.0.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'logit_scale', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_projection.1.running_mean', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_projection.3.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_projection.0.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias']\n","- This IS expected if you are initializing GroupViTVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing GroupViTVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of GroupViTVisionModel were not initialized from the model checkpoint at nvidia/groupvit-gcc-yfcc and are newly initialized because the shapes did not match:\n","- visual_projection.3.weight: found shape torch.Size([256, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n","- visual_projection.3.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at nvidia/groupvit-gcc-yfcc were not used when initializing GroupViTVisionModel: ['text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_projection.1.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_projection.1.weight', 'text_projection.1.running_var', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_projection.3.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.final_layer_norm.weight', 'text_projection.1.num_batches_tracked', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_projection.0.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'logit_scale', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_projection.1.running_mean', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_projection.3.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_projection.0.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias']\n","- This IS expected if you are initializing GroupViTVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing GroupViTVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of GroupViTVisionModel were not initialized from the model checkpoint at nvidia/groupvit-gcc-yfcc and are newly initialized because the shapes did not match:\n","- visual_projection.3.weight: found shape torch.Size([256, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n","- visual_projection.3.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["ModelBase(\n","  (model): GroupViTVisionModel(\n","    (vision_model): GroupViTVisionTransformer(\n","      (embeddings): GroupViTVisionEmbeddings(\n","        (patch_embeddings): GroupViTPatchEmbeddings(\n","          (projection): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","        (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (encoder): GroupViTVisionEncoder(\n","        (stages): ModuleList(\n","          (0): GroupViTStage(\n","            (layers): ModuleList(\n","              (0): GroupViTEncoderLayer(\n","                (self_attn): GroupViTAttention(\n","                  (k_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (v_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (q_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (out_proj): Linear(in_features=384, out_features=384, bias=True)\n","                )\n","                (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","                (mlp): GroupViTMLP(\n","                  (activation_fn): GELUActivation()\n","                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","                )\n","                (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","              )\n","              (1): GroupViTEncoderLayer(\n","                (self_attn): GroupViTAttention(\n","                  (k_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (v_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (q_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (out_proj): Linear(in_features=384, out_features=384, bias=True)\n","                )\n","                (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","                (mlp): GroupViTMLP(\n","                  (activation_fn): GELUActivation()\n","                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","                )\n","                (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","              )\n","              (2): GroupViTEncoderLayer(\n","                (self_attn): GroupViTAttention(\n","                  (k_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (v_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (q_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (out_proj): Linear(in_features=384, out_features=384, bias=True)\n","                )\n","                (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","                (mlp): GroupViTMLP(\n","                  (activation_fn): GELUActivation()\n","                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","                )\n","                (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","              )\n","              (3): GroupViTEncoderLayer(\n","                (self_attn): GroupViTAttention(\n","                  (k_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (v_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (q_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (out_proj): Linear(in_features=384, out_features=384, bias=True)\n","                )\n","                (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","                (mlp): GroupViTMLP(\n","                  (activation_fn): GELUActivation()\n","                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","                )\n","                (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","              )\n","              (4): GroupViTEncoderLayer(\n","                (self_attn): GroupViTAttention(\n","                  (k_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (v_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (q_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (out_proj): Linear(in_features=384, out_features=384, bias=True)\n","                )\n","                (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","                (mlp): GroupViTMLP(\n","                  (activation_fn): GELUActivation()\n","                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","                )\n","                (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","              )\n","              (5): GroupViTEncoderLayer(\n","                (self_attn): GroupViTAttention(\n","                  (k_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (v_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (q_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (out_proj): Linear(in_features=384, out_features=384, bias=True)\n","                )\n","                (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","                (mlp): GroupViTMLP(\n","                  (activation_fn): GELUActivation()\n","                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","                )\n","                (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","              )\n","            )\n","            (downsample): GroupViTTokenAssign(\n","              (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","              (mlp_inter): GroupViTMixerMLP(\n","                (activation_fn): GELUActivation()\n","                (fc1): Linear(in_features=64, out_features=192, bias=True)\n","                (fc2): Linear(in_features=192, out_features=64, bias=True)\n","              )\n","              (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","              (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","              (pre_assign_attn): GroupViTCrossAttentionLayer(\n","                (attn): GroupViTAttention(\n","                  (k_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (v_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (q_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (out_proj): Linear(in_features=384, out_features=384, bias=True)\n","                )\n","                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","                (mlp): GroupViTMLP(\n","                  (activation_fn): GELUActivation()\n","                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","                )\n","                (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","              )\n","              (assign): GroupViTAssignAttention(\n","                (q_proj): Linear(in_features=384, out_features=384, bias=True)\n","                (k_proj): Linear(in_features=384, out_features=384, bias=True)\n","                (v_proj): Linear(in_features=384, out_features=384, bias=True)\n","                (proj): Linear(in_features=384, out_features=384, bias=True)\n","              )\n","              (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","              (mlp_channels): GroupViTMLP(\n","                (activation_fn): GELUActivation()\n","                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","              )\n","            )\n","          )\n","          (1): GroupViTStage(\n","            (layers): ModuleList(\n","              (0): GroupViTEncoderLayer(\n","                (self_attn): GroupViTAttention(\n","                  (k_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (v_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (q_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (out_proj): Linear(in_features=384, out_features=384, bias=True)\n","                )\n","                (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","                (mlp): GroupViTMLP(\n","                  (activation_fn): GELUActivation()\n","                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","                )\n","                (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","              )\n","              (1): GroupViTEncoderLayer(\n","                (self_attn): GroupViTAttention(\n","                  (k_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (v_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (q_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (out_proj): Linear(in_features=384, out_features=384, bias=True)\n","                )\n","                (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","                (mlp): GroupViTMLP(\n","                  (activation_fn): GELUActivation()\n","                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","                )\n","                (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","              )\n","              (2): GroupViTEncoderLayer(\n","                (self_attn): GroupViTAttention(\n","                  (k_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (v_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (q_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (out_proj): Linear(in_features=384, out_features=384, bias=True)\n","                )\n","                (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","                (mlp): GroupViTMLP(\n","                  (activation_fn): GELUActivation()\n","                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","                )\n","                (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","              )\n","            )\n","            (downsample): GroupViTTokenAssign(\n","              (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","              (mlp_inter): GroupViTMixerMLP(\n","                (activation_fn): GELUActivation()\n","                (fc1): Linear(in_features=8, out_features=192, bias=True)\n","                (fc2): Linear(in_features=192, out_features=8, bias=True)\n","              )\n","              (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","              (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","              (pre_assign_attn): GroupViTCrossAttentionLayer(\n","                (attn): GroupViTAttention(\n","                  (k_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (v_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (q_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (out_proj): Linear(in_features=384, out_features=384, bias=True)\n","                )\n","                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","                (mlp): GroupViTMLP(\n","                  (activation_fn): GELUActivation()\n","                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","                )\n","                (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","              )\n","              (assign): GroupViTAssignAttention(\n","                (q_proj): Linear(in_features=384, out_features=384, bias=True)\n","                (k_proj): Linear(in_features=384, out_features=384, bias=True)\n","                (v_proj): Linear(in_features=384, out_features=384, bias=True)\n","                (proj): Linear(in_features=384, out_features=384, bias=True)\n","              )\n","              (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","              (mlp_channels): GroupViTMLP(\n","                (activation_fn): GELUActivation()\n","                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","              )\n","            )\n","            (group_projector): Sequential(\n","              (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","              (1): GroupViTMixerMLP(\n","                (activation_fn): GELUActivation()\n","                (fc1): Linear(in_features=64, out_features=192, bias=True)\n","                (fc2): Linear(in_features=192, out_features=8, bias=True)\n","              )\n","            )\n","          )\n","          (2): GroupViTStage(\n","            (layers): ModuleList(\n","              (0): GroupViTEncoderLayer(\n","                (self_attn): GroupViTAttention(\n","                  (k_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (v_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (q_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (out_proj): Linear(in_features=384, out_features=384, bias=True)\n","                )\n","                (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","                (mlp): GroupViTMLP(\n","                  (activation_fn): GELUActivation()\n","                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","                )\n","                (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","              )\n","              (1): GroupViTEncoderLayer(\n","                (self_attn): GroupViTAttention(\n","                  (k_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (v_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (q_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (out_proj): Linear(in_features=384, out_features=384, bias=True)\n","                )\n","                (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","                (mlp): GroupViTMLP(\n","                  (activation_fn): GELUActivation()\n","                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","                )\n","                (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","              )\n","              (2): GroupViTEncoderLayer(\n","                (self_attn): GroupViTAttention(\n","                  (k_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (v_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (q_proj): Linear(in_features=384, out_features=384, bias=True)\n","                  (out_proj): Linear(in_features=384, out_features=384, bias=True)\n","                )\n","                (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","                (mlp): GroupViTMLP(\n","                  (activation_fn): GELUActivation()\n","                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","                )\n","                (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","              )\n","            )\n","          )\n","        )\n","      )\n","      (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (visual_projection): Sequential(\n","      (0): Linear(in_features=384, out_features=4096, bias=True)\n","      (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","      (3): Linear(in_features=4096, out_features=128, bias=True)\n","    )\n","  )\n",")\n"]}],"source":["class ModelMoCo(nn.Module):\n","    def __init__(self, dim=128, K=4096, m=0.99, T=0.1, arch='resnet18', bn_splits=8, symmetric=True):\n","        super(ModelMoCo, self).__init__()\n","\n","        self.K = K\n","        self.m = m\n","        self.T = T\n","        self.symmetric = symmetric\n","\n","        # create the encoders\n","        self.encoder_q = ModelBase(feature_dim=dim, arch=arch, bn_splits=bn_splits)\n","        self.encoder_k = ModelBase(feature_dim=dim, arch=arch, bn_splits=bn_splits)\n","\n","        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n","            param_k.data.copy_(param_q.data)  # initialize\n","            param_k.requires_grad = False  # not update by gradient\n","\n","        # create the queue\n","        self.register_buffer(\"queue\", torch.randn(dim, K))\n","        self.queue = nn.functional.normalize(self.queue, dim=0)\n","\n","        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n","\n","    @torch.no_grad()\n","    def _momentum_update_key_encoder(self):\n","        \"\"\"\n","        Momentum update of the key encoder\n","        \"\"\"\n","        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n","            param_k.data = param_k.data * self.m + param_q.data * (1. - self.m)\n","\n","    @torch.no_grad()\n","    def _dequeue_and_enqueue(self, keys):\n","        batch_size = keys.shape[0]\n","\n","        ptr = int(self.queue_ptr)\n","        assert self.K % batch_size == 0  # for simplicity\n","\n","        # replace the keys at ptr (dequeue and enqueue)\n","        self.queue[:, ptr:ptr + batch_size] = keys.t()  # transpose\n","        ptr = (ptr + batch_size) % self.K  # move pointer\n","\n","        self.queue_ptr[0] = ptr\n","\n","    @torch.no_grad()\n","    def _batch_shuffle_single_gpu(self, x):\n","        \"\"\"\n","        Batch shuffle, for making use of BatchNorm.\n","        \"\"\"\n","        # random shuffle index\n","        idx_shuffle = torch.randperm(x.shape[0]).cuda()\n","\n","        # index for restoring\n","        idx_unshuffle = torch.argsort(idx_shuffle)\n","\n","        return x[idx_shuffle], idx_unshuffle\n","\n","    @torch.no_grad()\n","    def _batch_unshuffle_single_gpu(self, x, idx_unshuffle):\n","        \"\"\"\n","        Undo batch shuffle.\n","        \"\"\"\n","        return x[idx_unshuffle]\n","\n","    def contrastive_loss(self, im_q, im_k):\n","        # compute query features\n","        q = self.encoder_q(im_q)  # queries: NxC\n","        q = nn.functional.normalize(q, dim=1)  # already normalized\n","\n","        # compute key features\n","        with torch.no_grad():  # no gradient to keys\n","            # shuffle for making use of BN\n","            im_k_, idx_unshuffle = self._batch_shuffle_single_gpu(im_k)\n","\n","            k = self.encoder_k(im_k_)  # keys: NxC\n","            k = nn.functional.normalize(k, dim=1)  # already normalized\n","\n","            # undo shuffle\n","            k = self._batch_unshuffle_single_gpu(k, idx_unshuffle)\n","\n","        # compute logits\n","        # Einstein sum is more intuitive\n","        # positive logits: Nx1\n","        l_pos = torch.einsum('nc,nc->n', [q, k]).unsqueeze(-1)\n","        # negative logits: NxK\n","        l_neg = torch.einsum('nc,ck->nk', [q, self.queue.clone().detach()])\n","\n","        # logits: Nx(1+K)\n","        logits = torch.cat([l_pos, l_neg], dim=1)\n","\n","        # apply temperature\n","        logits /= self.T\n","\n","        # labels: positive key indicators\n","        labels = torch.zeros(logits.shape[0], dtype=torch.long).cuda()\n","        \n","        loss = nn.CrossEntropyLoss().cuda()(logits, labels)\n","\n","        return loss, q, k\n","\n","    def forward(self, im1, im2):\n","        \"\"\"\n","        Input:\n","            im_q: a batch of query images\n","            im_k: a batch of key images\n","        Output:\n","            loss\n","        \"\"\"\n","\n","        # update the key encoder\n","        with torch.no_grad():  # no gradient to keys\n","            self._momentum_update_key_encoder()\n","\n","        # compute loss\n","        if self.symmetric:  # asymmetric loss\n","            loss_12, q1, k2 = self.contrastive_loss(im1, im2)\n","            loss_21, q2, k1 = self.contrastive_loss(im2, im1)\n","            loss = loss_12 + loss_21\n","            k = torch.cat([k1, k2], dim=0)\n","        else:  # asymmetric loss\n","            loss, q, k = self.contrastive_loss(im1, im2)\n","\n","        self._dequeue_and_enqueue(k)\n","\n","        return loss\n","\n","# create model\n","model = ModelMoCo(\n","        dim=args.moco_dim,\n","        K=args.moco_k,\n","        m=args.moco_m,\n","        T=args.moco_t,\n","        arch=args.arch,\n","        bn_splits=args.bn_splits,\n","        symmetric=args.symmetric,\n","    ).cuda()\n","print(model.encoder_q)"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["# load_entire_model\n","\n","# checkpoint = torch.load(os.path.abspath(os.path.join(ROOT_PATH, '230226_v0.0.12023-02-27-16-28-16-moco/model_last.pth')))\n","# model.load_state_dict(checkpoint['state_dict'])"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["# load_encoder_q_dage\n","\n","# checkpoint = torch.load(os.path.abspath(os.path.join(ROOT_PATH, '230226_v0.0.12023-02-27-16-28-16-moco/model_last.pth')))\n","# model.load_state_dict(checkpoint['encoder_q_state_dict'])"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["# from PIL import Image\n","# import requests\n","# from transformers import AutoProcessor\n","\n","# processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n","\n","# url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n","# image = Image.open(requests.get(url, stream=True).raw)\n","# inputs = processor(images=image, return_tensors=\"pt\")\n","\n","# logits = model.encoder_q(inputs[\"pixel_values\"].cuda(), True)"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["# def ade_palette():\n","#     return [[0, 255, 120], [180, 120, 120], [6, 230, 230], [80, 50, 50],\n","#             [4, 200, 3], [120, 120, 80], [255, 0, 140], [204, 5, 255]]"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["# from torch import nn\n","# import numpy as np\n","# import matplotlib.pyplot as plt\n","\n","# print(logits.shape, type(logits))\n","\n","# # First, rescale logits to original image size\n","# logits = nn.functional.interpolate(logits.detach().cpu(),\n","#                 size=image.size[::-1], # (height, width)\n","#                 mode='bilinear',\n","#                 align_corners=False)\n","\n","# print(logits.shape, type(logits))\n","\n","# # Second, apply argmax on the class dimension\n","# seg = logits.argmax(dim=1)[0]\n","# color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8) # height, width, 3\n","# palette = np.array(ade_palette())\n","# for label, color in enumerate(palette):\n","#     color_seg[seg == label, :] = color\n","# # Convert to BGR\n","# color_seg = color_seg[..., ::-1]\n","\n","# # Show image + mask\n","# img = np.array(image) * 0.5 + color_seg * 0.5\n","# img = img.astype(np.uint8)\n","\n","# plt.figure(figsize=(15, 10))\n","# plt.imshow(img)\n","# plt.show()"]},{"cell_type":"markdown","metadata":{"id":"9YXcpXBwi8KV"},"source":["### Define train/test\n","\n"]},{"cell_type":"code","execution_count":54,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1677512259938,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"DBKFqboqnqty"},"outputs":[],"source":["# train for one epoch\n","def train(net, data_loader, train_optimizer, epoch, args):\n","    net.train()\n","    adjust_learning_rate(optimizer, epoch, args)\n","\n","    total_loss, total_num, train_bar = 0.0, 0, tqdm(data_loader)\n","    for im_1, im_2 in train_bar:\n","        im_1, im_2 = im_1.cuda(non_blocking=True), im_2.cuda(non_blocking=True)\n","\n","        loss = net(im_1, im_2) # torch.Size([512, 3, 32, 32])*2\n","        \n","        train_optimizer.zero_grad()\n","        loss.backward()\n","        train_optimizer.step()\n","\n","        total_num += data_loader.batch_size\n","        total_loss += loss.item() * data_loader.batch_size\n","        train_bar.set_description('Train Epoch: [{}/{}], lr: {:.6f}, Loss: {:.4f}'.format(epoch, args.epochs, optimizer.param_groups[0]['lr'], total_loss / total_num))\n","\n","    return total_loss / total_num\n","\n","# lr scheduler for training\n","def adjust_learning_rate(optimizer, epoch, args):\n","    \"\"\"Decay the learning rate based on schedule\"\"\"\n","    lr = args.lr\n","    if args.cos:  # cosine lr schedule\n","        lr *= 0.5 * (1. + math.cos(math.pi * epoch / args.epochs))\n","    else:  # stepwise lr schedule\n","        for milestone in args.schedule:\n","            lr *= 0.1 if epoch >= milestone else 1.\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr"]},{"cell_type":"code","execution_count":55,"metadata":{"cellView":"both","executionInfo":{"elapsed":22,"status":"ok","timestamp":1677512259938,"user":{"displayName":"野菜浅","userId":"15074908195438095604"},"user_tz":-480},"id":"RI1Y8bSImD7N"},"outputs":[],"source":["# test using a knn monitor\n","def test(net, memory_data_loader, test_data_loader, epoch, args):\n","    net.eval()\n","    classes = len(memory_data_loader.dataset.classes)\n","    total_top1, total_top5, total_num, feature_bank = 0.0, 0.0, 0, []\n","    with torch.no_grad():\n","        # generate feature bank\n","        for data, target in tqdm(memory_data_loader, desc='Feature extracting'):\n","            feature = net(data.cuda(non_blocking=True))\n","            feature = F.normalize(feature, dim=1)\n","            feature_bank.append(feature)\n","        # [D, N]\n","        feature_bank = torch.cat(feature_bank, dim=0).t().contiguous()\n","        # [N]\n","        feature_labels = torch.tensor(memory_data_loader.dataset.targets, device=feature_bank.device)\n","        # loop test data to predict the label by weighted knn search\n","        test_bar = tqdm(test_data_loader)\n","        for data, target in test_bar:\n","            data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n","            feature = net(data)\n","            feature = F.normalize(feature, dim=1)\n","            \n","            pred_labels = knn_predict(feature, feature_bank, feature_labels, classes, args.knn_k, args.knn_t)\n","\n","            total_num += data.size(0)\n","            total_top1 += (pred_labels[:, 0] == target).float().sum().item()\n","            test_bar.set_description('Test Epoch: [{}/{}] Acc@1:{:.2f}%'.format(epoch, args.epochs, total_top1 / total_num * 100))\n","\n","    return total_top1 / total_num * 100\n","\n","# knn monitor as in InstDisc https://arxiv.org/abs/1805.01978\n","# implementation follows http://github.com/zhirongw/lemniscate.pytorch and https://github.com/leftthomas/SimCLR\n","def knn_predict(feature, feature_bank, feature_labels, classes, knn_k, knn_t):\n","    # compute cos similarity between each feature vector and feature bank ---> [B, N]\n","    sim_matrix = torch.mm(feature, feature_bank)\n","    # [B, K]\n","    sim_weight, sim_indices = sim_matrix.topk(k=knn_k, dim=-1)\n","    # [B, K]\n","    sim_labels = torch.gather(feature_labels.expand(feature.size(0), -1), dim=-1, index=sim_indices)\n","    sim_weight = (sim_weight / knn_t).exp()\n","\n","    # counts for each class\n","    one_hot_label = torch.zeros(feature.size(0) * knn_k, classes, device=sim_labels.device)\n","    # [B*K, C]\n","    one_hot_label = one_hot_label.scatter(dim=-1, index=sim_labels.view(-1, 1), value=1.0)\n","    # weighted score ---> [B, C]\n","    pred_scores = torch.sum(one_hot_label.view(feature.size(0), -1, classes) * sim_weight.unsqueeze(dim=-1), dim=1)\n","\n","    pred_labels = pred_scores.argsort(dim=-1, descending=True)\n","    return pred_labels"]},{"cell_type":"markdown","metadata":{"id":"86lHkiKox3KO"},"source":["### Start training"]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M3INyoTC3-bf","outputId":"8e5b5e60-a8e8-4360-eb17-3b3ef4e744aa"},"outputs":[{"name":"stderr","output_type":"stream","text":["<>:6: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n","<>:6: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n","/tmp/ipykernel_27673/3792128569.py:6: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n","  if args.resume is not '':\n","Train Epoch: [1/200], lr: 0.059996, Loss: nan: 100%|██████████| 6250/6250 [31:44<00:00,  3.28it/s]  \n","Feature extracting:   0%|          | 0/6250 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","Feature extracting: 100%|██████████| 6250/6250 [08:07<00:00, 13.54it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","Feature extracting: 100%|██████████| 6250/6250 [08:08<00:00, 12.81it/s]\n","  0%|          | 0/1250 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","Test Epoch: [1/200] Acc@1:10.00%: 100%|█████████▉| 1249/1250 [01:32<00:00, 13.69it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","Test Epoch: [1/200] Acc@1:10.00%: 100%|██████████| 1250/1250 [01:32<00:00, 13.48it/s]\n","  0%|          | 0/6250 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","Train Epoch: [2/200], lr: 0.059985, Loss: nan: 100%|██████████| 6250/6250 [29:15<00:00,  3.64it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","Train Epoch: [2/200], lr: 0.059985, Loss: nan: 100%|██████████| 6250/6250 [29:16<00:00,  3.56it/s]\n","Feature extracting:   0%|          | 0/6250 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","Feature extracting: 100%|██████████| 6250/6250 [07:19<00:00, 14.16it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","Feature extracting: 100%|██████████| 6250/6250 [07:19<00:00, 14.23it/s]\n","  0%|          | 0/1250 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","Test Epoch: [2/200] Acc@1:10.00%: 100%|█████████▉| 1249/1250 [01:33<00:00, 13.37it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","Test Epoch: [2/200] Acc@1:10.00%: 100%|██████████| 1250/1250 [01:33<00:00, 13.40it/s]\n","  0%|          | 0/6250 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","Train Epoch: [3/200], lr: 0.059967, Loss: nan: 100%|██████████| 6250/6250 [29:33<00:00,  3.64it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","Train Epoch: [3/200], lr: 0.059967, Loss: nan: 100%|██████████| 6250/6250 [29:33<00:00,  3.52it/s]\n","Feature extracting:   0%|          | 0/6250 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","Feature extracting: 100%|██████████| 6250/6250 [07:19<00:00, 14.24it/s]\n","  0%|          | 0/1250 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","Test Epoch: [3/200] Acc@1:10.00%: 100%|█████████▉| 1249/1250 [01:26<00:00, 11.86it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","Test Epoch: [3/200] Acc@1:10.00%: 100%|██████████| 1250/1250 [01:27<00:00, 14.34it/s]\n","  0%|          | 0/6250 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","Train Epoch: [4/200], lr: 0.059941, Loss: nan:  68%|██████▊   | 4273/6250 [20:52<09:33,  3.45it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","Train Epoch: [4/200], lr: 0.059941, Loss: nan:  68%|██████▊   | 4273/6250 [20:52<09:39,  3.41it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[56], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39m# training loop\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epoch_start, args\u001b[39m.\u001b[39mepochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m---> 23\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, train_loader, optimizer, epoch, args)\n\u001b[1;32m     24\u001b[0m     results[\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[1;32m     25\u001b[0m     test_acc_1 \u001b[39m=\u001b[39m test(model\u001b[39m.\u001b[39mencoder_q, memory_loader, test_loader, epoch, args)\n","Cell \u001b[0;32mIn[54], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, data_loader, train_optimizer, epoch, args)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m im_1, im_2 \u001b[39min\u001b[39;00m train_bar:\n\u001b[1;32m      8\u001b[0m     im_1, im_2 \u001b[39m=\u001b[39m im_1\u001b[39m.\u001b[39mcuda(non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), im_2\u001b[39m.\u001b[39mcuda(non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 10\u001b[0m     loss \u001b[39m=\u001b[39m net(im_1, im_2) \u001b[39m# torch.Size([512, 3, 32, 32])*2\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     train_optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     13\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[48], line 121\u001b[0m, in \u001b[0;36mModelMoCo.forward\u001b[0;34m(self, im1, im2)\u001b[0m\n\u001b[1;32m    119\u001b[0m     k \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([k1, k2], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    120\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# asymmetric loss\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m     loss, q, k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontrastive_loss(im1, im2)\n\u001b[1;32m    123\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dequeue_and_enqueue(k)\n\u001b[1;32m    125\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n","Cell \u001b[0;32mIn[48], line 75\u001b[0m, in \u001b[0;36mModelMoCo.contrastive_loss\u001b[0;34m(self, im_q, im_k)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():  \u001b[39m# no gradient to keys\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[39m# shuffle for making use of BN\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     im_k_, idx_unshuffle \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_shuffle_single_gpu(im_k)\n\u001b[0;32m---> 75\u001b[0m     k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder_k(im_k_)  \u001b[39m# keys: NxC\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     k \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mnormalize(k, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# already normalized\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     \u001b[39m# undo shuffle\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[42], line 10\u001b[0m, in \u001b[0;36mModelBase.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\u001b[39m# x = self.net(x)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     vision_outputs, logits, image_features\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x)\n\u001b[1;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m image_features\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[34], line 65\u001b[0m, in \u001b[0;36mGroupViTVisionModel.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m     63\u001b[0m return_dict \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39m# print(pixel_values.shape, output_attentions, output_hidden_states, return_dict)\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m vision_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvision_model(\n\u001b[1;32m     66\u001b[0m     pixel_values\u001b[39m=\u001b[39;49mpixel_values,\n\u001b[1;32m     67\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m     68\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m     69\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m     70\u001b[0m )\n\u001b[1;32m     71\u001b[0m \u001b[39m# print(vision_outputs)\u001b[39;00m\n\u001b[1;32m     73\u001b[0m attentions \u001b[39m=\u001b[39m vision_outputs[\u001b[39m2\u001b[39m]\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[33], line 35\u001b[0m, in \u001b[0;36mGroupViTVisionTransformer.forward\u001b[0;34m(self, pixel_values, output_hidden_states, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to specify pixel_values\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(pixel_values)\n\u001b[0;32m---> 35\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m     36\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m     37\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m     38\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m     39\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     42\u001b[0m last_hidden_state \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m     44\u001b[0m \u001b[39m# normalize the last hidden state\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[29], line 41\u001b[0m, in \u001b[0;36mGroupViTVisionEncoder.forward\u001b[0;34m(self, hidden_states, output_hidden_states, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m     39\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_states,)\n\u001b[0;32m---> 41\u001b[0m layer_outputs \u001b[39m=\u001b[39m stage(hidden_states, group_tokens, output_attentions)\n\u001b[1;32m     43\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m     44\u001b[0m group_tokens \u001b[39m=\u001b[39m layer_outputs[\u001b[39m1\u001b[39m]\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[22], line 87\u001b[0m, in \u001b[0;36mGroupViTStage.forward\u001b[0;34m(self, hidden_states, prev_group_token, output_attentions)\u001b[0m\n\u001b[1;32m     85\u001b[0m attention \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m     x, attention \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownsample(x, group_token)\n\u001b[1;32m     89\u001b[0m outputs \u001b[39m=\u001b[39m (x, group_token)\n\u001b[1;32m     90\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[17], line 48\u001b[0m, in \u001b[0;36mGroupViTTokenAssign.forward\u001b[0;34m(self, image_tokens, group_tokens)\u001b[0m\n\u001b[1;32m     46\u001b[0m projected_group_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproject_group_token(group_tokens)\n\u001b[1;32m     47\u001b[0m projected_group_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_assign_attn(projected_group_tokens, image_tokens)\n\u001b[0;32m---> 48\u001b[0m new_image_tokens, attention \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49massign(projected_group_tokens, image_tokens)\n\u001b[1;32m     49\u001b[0m new_image_tokens \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m projected_group_tokens\n\u001b[1;32m     51\u001b[0m new_image_tokens \u001b[39m=\u001b[39m new_image_tokens \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp_channels(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_new_x(new_image_tokens))\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[16], line 37\u001b[0m, in \u001b[0;36mGroupViTAssignAttention.forward\u001b[0;34m(self, query, key)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39m# [batch_size, query_length, key_length]\u001b[39;00m\n\u001b[1;32m     35\u001b[0m raw_attn \u001b[39m=\u001b[39m (query \u001b[39m@\u001b[39m key\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale\n\u001b[0;32m---> 37\u001b[0m attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_attn(raw_attn)\n\u001b[1;32m     38\u001b[0m soft_attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_attn(raw_attn, gumbel\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, hard\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     40\u001b[0m attn \u001b[39m=\u001b[39m attn \u001b[39m/\u001b[39m (attn\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39massign_eps)\n","Cell \u001b[0;32mIn[16], line 14\u001b[0m, in \u001b[0;36mGroupViTAssignAttention.get_attn\u001b[0;34m(self, attn, gumbel, hard)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_attn\u001b[39m(\u001b[39mself\u001b[39m, attn, gumbel\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, hard\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     13\u001b[0m     \u001b[39mif\u001b[39;00m gumbel \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m---> 14\u001b[0m         attn \u001b[39m=\u001b[39m gumbel_softmax(attn, dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m, hard\u001b[39m=\u001b[39;49mhard)\n\u001b[1;32m     15\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m         \u001b[39mif\u001b[39;00m hard:\n","Cell \u001b[0;32mIn[12], line 4\u001b[0m, in \u001b[0;36mgumbel_softmax\u001b[0;34m(logits, tau, hard, dim)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgumbel_softmax\u001b[39m(logits: torch\u001b[39m.\u001b[39mTensor, tau: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, hard: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, dim: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m      2\u001b[0m     \u001b[39m# more stable https://github.com/pytorch/pytorch/issues/41663\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     gumbel_dist \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdistributions\u001b[39m.\u001b[39mgumbel\u001b[39m.\u001b[39mGumbel(\n\u001b[0;32m----> 4\u001b[0m         torch\u001b[39m.\u001b[39;49mtensor(\u001b[39m0.0\u001b[39;49m, device\u001b[39m=\u001b[39;49mlogits\u001b[39m.\u001b[39;49mdevice, dtype\u001b[39m=\u001b[39;49mlogits\u001b[39m.\u001b[39;49mdtype),\n\u001b[1;32m      5\u001b[0m         torch\u001b[39m.\u001b[39mtensor(\u001b[39m1.0\u001b[39m, device\u001b[39m=\u001b[39mlogits\u001b[39m.\u001b[39mdevice, dtype\u001b[39m=\u001b[39mlogits\u001b[39m.\u001b[39mdtype),\n\u001b[1;32m      6\u001b[0m     )\n\u001b[1;32m      7\u001b[0m     gumbels \u001b[39m=\u001b[39m gumbel_dist\u001b[39m.\u001b[39msample(logits\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      9\u001b[0m     gumbels \u001b[39m=\u001b[39m (logits \u001b[39m+\u001b[39m gumbels) \u001b[39m/\u001b[39m tau  \u001b[39m# ~Gumbel(logits,tau)\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# define optimizer\n","optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.wd, momentum=0.9)\n","\n","# load model if resume\n","epoch_start = 1\n","if args.resume is not '':\n","    checkpoint = torch.load(args.resume)\n","    model.load_state_dict(checkpoint['state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","    epoch_start = checkpoint['epoch'] + 1\n","    print('Loaded from: {}'.format(args.resume))\n","\n","# logging\n","results = {'train_loss': [], 'test_acc@1': []}\n","if not os.path.exists(args.results_dir):\n","    os.mkdir(args.results_dir)\n","# dump args\n","with open(args.results_dir + '/args.json', 'w') as fid:\n","    json.dump(args.__dict__, fid, indent=2)\n","\n","# training loop\n","for epoch in range(epoch_start, args.epochs + 1):\n","    train_loss = train(model, train_loader, optimizer, epoch, args)\n","    results['train_loss'].append(train_loss)\n","    test_acc_1 = test(model.encoder_q, memory_loader, test_loader, epoch, args)\n","    results['test_acc@1'].append(test_acc_1)\n","    # save statistics\n","    data_frame = pd.DataFrame(data=results, index=range(epoch_start, epoch + 1))\n","    data_frame.to_csv(args.results_dir + '/log.csv', index_label='epoch')\n","    # save model\n","    torch.save({'epoch': epoch, 'state_dict': model.state_dict(), 'optimizer' : optimizer.state_dict(),}, args.results_dir + '/model_last.pth')\n","    torch.save({'epoch': epoch, 'encoder_q_state_dict': model.encoder_q.state_dict(), 'optimizer' : optimizer.state_dict(),}, args.results_dir + '/encoder_q_model_last.pth')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/facebookresearch/moco/blob/colab-notebook/colab/moco_cifar10_demo.ipynb","timestamp":1677256490794}]},"environment":{"name":"pytorch-gpu.1-4.m50","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"vscode":{"interpreter":{"hash":"df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"06768cfdf690415e9affdf2a74e59a30":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"17032952ca804b558931b93c0fde1540":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cc091016bb8d423f80dddca00096ba0b","IPY_MODEL_6bbbd0d7f0f749939610ccc1aa47bfec","IPY_MODEL_eb9b54187a9b4404b93ce96cb5297a1e"],"layout":"IPY_MODEL_48854cfeccd84183a1819d703353a4fa"}},"1d94bafa0c204861869e3c8656f88338":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20faf3fda02c4257b5ac68099de45581":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"23596ead25e842da821eee9281ddd44b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce5a59adc10e4ad3bbd5ee949704a493","placeholder":"​","style":"IPY_MODEL_2e92aabde375495f880ae9752b55e057","value":" 4.64k/4.64k [00:00&lt;00:00, 115kB/s]"}},"2e92aabde375495f880ae9752b55e057":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"31ea43544fcd44a19f86c920afaa12ab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3deab35d86db420c963d21fde4f4f770":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d94bafa0c204861869e3c8656f88338","placeholder":"​","style":"IPY_MODEL_06768cfdf690415e9affdf2a74e59a30","value":"Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"}},"3e187d79da6f477180cac4b95d949388":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"48854cfeccd84183a1819d703353a4fa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5528a5ca02bc4cde9fa1bad142519e35":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_79cc1507b29d4125ae5e3c704b2b8b75","placeholder":"​","style":"IPY_MODEL_31ea43544fcd44a19f86c920afaa12ab","value":"Downloading (…)lve/main/config.json: 100%"}},"55606bf393b940278f76baf65170eeab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5bffaeb6f389499c958f4a12dca192e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5d806cd2fe7e4424b7fb90371815f4d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3deab35d86db420c963d21fde4f4f770","IPY_MODEL_e3214b7f823d4a12bed90f36b6cd3bbe","IPY_MODEL_6bec28962d4740aabe8e9896e00134b7"],"layout":"IPY_MODEL_5da754b21ef34dc9a7eff14bdc3a34bc"}},"5da754b21ef34dc9a7eff14bdc3a34bc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bbbd0d7f0f749939610ccc1aa47bfec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a4d8fcd96c2b44c1a21e6a0cf46d736d","max":170498071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5bffaeb6f389499c958f4a12dca192e4","value":170498071}},"6bec28962d4740aabe8e9896e00134b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d083420df95d42ecb16cedd5037dc2a3","placeholder":"​","style":"IPY_MODEL_55606bf393b940278f76baf65170eeab","value":" 223M/223M [00:02&lt;00:00, 86.7MB/s]"}},"73487b45ea444f8e81410a9627c85b40":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79cc1507b29d4125ae5e3c704b2b8b75":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7cdb20bd656249fe85674962d2a4ba7e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9cf2f5d1a7894a73861ce8bd69675ed1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a4d8fcd96c2b44c1a21e6a0cf46d736d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aeabbf27ff5e4ef5a0c3de652c86d632":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5528a5ca02bc4cde9fa1bad142519e35","IPY_MODEL_c3b291f042fe422aa3a2b22f1208d0ca","IPY_MODEL_23596ead25e842da821eee9281ddd44b"],"layout":"IPY_MODEL_d74ac37d81974a4780581b554f3e7d23"}},"c3b291f042fe422aa3a2b22f1208d0ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_73487b45ea444f8e81410a9627c85b40","max":4642,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3e187d79da6f477180cac4b95d949388","value":4642}},"cc091016bb8d423f80dddca00096ba0b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7cdb20bd656249fe85674962d2a4ba7e","placeholder":"​","style":"IPY_MODEL_20faf3fda02c4257b5ac68099de45581","value":"100%"}},"ce5a59adc10e4ad3bbd5ee949704a493":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf90228e48af46809d35402d94eb568e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d083420df95d42ecb16cedd5037dc2a3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d74ac37d81974a4780581b554f3e7d23":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9070ee51e9c4ebe868777345a25a62d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3214b7f823d4a12bed90f36b6cd3bbe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f48ad91e88b340258d9fbe66bc58696b","max":223137427,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9cf2f5d1a7894a73861ce8bd69675ed1","value":223137427}},"eb9b54187a9b4404b93ce96cb5297a1e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d9070ee51e9c4ebe868777345a25a62d","placeholder":"​","style":"IPY_MODEL_cf90228e48af46809d35402d94eb568e","value":" 170498071/170498071 [00:14&lt;00:00, 14802420.28it/s]"}},"f48ad91e88b340258d9fbe66bc58696b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
