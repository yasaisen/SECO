{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MXcbCW8f7Eh"
   },
   "source": [
    "### Some Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.9.0+cu111 in /opt/conda/lib/python3.7/site-packages (1.9.0+cu111)\n",
      "Requirement already satisfied: torchvision==0.10.0+cu111 in /opt/conda/lib/python3.7/site-packages (0.10.0+cu111)\n",
      "Requirement already satisfied: torchaudio==0.9.0 in /opt/conda/lib/python3.7/site-packages (0.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.9.0+cu111) (4.4.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision==0.10.0+cu111) (1.18.1)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision==0.10.0+cu111) (9.5.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: segmentation_models_pytorch in /opt/conda/lib/python3.7/site-packages (0.3.2)\n",
      "Requirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from segmentation_models_pytorch) (0.10.0+cu111)\n",
      "Requirement already satisfied: pretrainedmodels==0.7.4 in /opt/conda/lib/python3.7/site-packages (from segmentation_models_pytorch) (0.7.4)\n",
      "Requirement already satisfied: efficientnet-pytorch==0.7.1 in /opt/conda/lib/python3.7/site-packages (from segmentation_models_pytorch) (0.7.1)\n",
      "Requirement already satisfied: timm==0.6.12 in /opt/conda/lib/python3.7/site-packages (from segmentation_models_pytorch) (0.6.12)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from segmentation_models_pytorch) (4.65.0)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from segmentation_models_pytorch) (9.5.0)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.9.0+cu111)\n",
      "Requirement already satisfied: munch in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (2.5.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from timm==0.6.12->segmentation_models_pytorch) (5.3.1)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.7/site-packages (from timm==0.6.12->segmentation_models_pytorch) (0.14.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.18.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (4.4.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm==0.6.12->segmentation_models_pytorch) (3.0.12)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm==0.6.12->segmentation_models_pytorch) (2023.1.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm==0.6.12->segmentation_models_pytorch) (2.22.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm==0.6.12->segmentation_models_pytorch) (20.9)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm==0.6.12->segmentation_models_pytorch) (4.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from munch->pretrainedmodels==0.7.4->segmentation_models_pytorch) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.9->huggingface-hub->timm==0.6.12->segmentation_models_pytorch) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub->timm==0.6.12->segmentation_models_pytorch) (3.4.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm==0.6.12->segmentation_models_pytorch) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm==0.6.12->segmentation_models_pytorch) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm==0.6.12->segmentation_models_pytorch) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm==0.6.12->segmentation_models_pytorch) (2020.4.5.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping opencv-python as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: opencv-contrib-python in /opt/conda/lib/python3.7/site-packages (4.7.0.72)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.7/site-packages (from opencv-contrib-python) (1.18.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.18.1)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (3.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: albumentations in /opt/conda/lib/python3.7/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from albumentations) (1.18.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from albumentations) (1.7.3)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in /opt/conda/lib/python3.7/site-packages (from albumentations) (0.19.3)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from albumentations) (5.3.1)\n",
      "Requirement already satisfied: qudida>=0.0.4 in /opt/conda/lib/python3.7/site-packages (from albumentations) (0.0.4)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from albumentations) (4.7.0.72)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from qudida>=0.0.4->albumentations) (1.0.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from qudida>=0.0.4->albumentations) (4.4.0)\n",
      "Requirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2.6.3)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (9.5.0)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2.28.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2021.11.2)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (1.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (20.9)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->scikit-image>=0.16.1->albumentations) (2.4.7)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.28.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.18.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.0.1)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.4.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2020.4.5.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "Hit:1 http://free.nchc.org.tw/ubuntu bionic InRelease\n",
      "Hit:2 http://free.nchc.org.tw/ubuntu bionic-updates InRelease                  \n",
      "Hit:3 http://free.nchc.org.tw/ubuntu bionic-backports InRelease                \n",
      "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1581 B]\n",
      "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
      "Hit:7 http://security.ubuntu.com/ubuntu bionic-security InRelease              \n",
      "Err:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC\n",
      "Ign:8 https://deb.obspy.org xenial InRelease                               \n",
      "Err:10 https://deb.obspy.org xenial Release    \n",
      "  Certificate verification failed: The certificate is NOT trusted. The certificate chain uses expired certificate.  Could not handshake: Error in the certificate verification. [IP: 95.217.194.95 443]\n",
      "Reading package lists... Done                  \n",
      "W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC\n",
      "E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease' is not signed.\n",
      "N: Updating from such a repository can't be done securely, and is therefore disabled by default.\n",
      "N: See apt-secure(8) manpage for repository creation and user configuration details.\n",
      "E: The repository 'http://deb.obspy.org xenial Release' does not have a Release file.\n",
      "N: Updating from such a repository can't be done securely, and is therefore disabled by default.\n",
      "N: See apt-secure(8) manpage for repository creation and user configuration details.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "ffmpeg is already the newest version (7:3.4.11-0ubuntu0.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 137 not upgraded.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "libsm6 is already the newest version (2:1.2.2-1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 137 not upgraded.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "libxext6 is already the newest version (2:1.3.3-1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 137 not upgraded.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "unzip is already the newest version (6.0-21ubuntu1.2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 137 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install segmentation_models_pytorch\n",
    "!pip uninstall opencv-python\n",
    "!pip install opencv-contrib-python\n",
    "!pip install scikit-learn\n",
    "!pip install albumentations\n",
    "!pip install transformers\n",
    "!apt-get update\n",
    "!apt-get install ffmpeg -y\n",
    "!apt-get install libsm6 -y\n",
    "!apt-get install libxext6 -y\n",
    "!apt-get install unzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 10 01:30:38 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.76       Driver Version: 515.76       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:20:00.0 Off |                  N/A |\n",
      "| 36%   56C    P8    30W / 200W |      0MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# ### for_multi_GPU\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5122,
     "status": "ok",
     "timestamp": 1677512171549,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "CvAFrPlS4bLU",
    "outputId": "2900b98f-10a0-48ae-ae6a-0aff7f787c20"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pdb\n",
    "import time\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader, Dataset, sampler\n",
    "from matplotlib import pyplot as plt\n",
    "from albumentations import (HorizontalFlip, ShiftScaleRotate, Normalize, Resize, Compose, GaussNoise)\n",
    "# from albumentations.torch import ToTensorV2\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_rtoBGhgSae"
   },
   "source": [
    "### Set Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30443,
     "status": "ok",
     "timestamp": 1677512201987,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "UKCi74HiH1A9",
    "outputId": "779d592f-9214-477e-c922-233d7845cb7c"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# ROOT_PATH = '/home/yasaisen/Desktop/11_research/11_research_main/lab_08'\n",
    "ROOT_PATH = '/home'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_submission_path = '../input/siim-acr-pneumothorax-segmentation/sample_submission.csv'\n",
    "# sample_submission_path = '/content/drive/My Drive/11_research_main/lab_01/CXR_Dataset/stage_2_sample_submission.csv'\n",
    "\n",
    "train_rle_path = os.path.join(ROOT_PATH, 'SIIM_ACR_dataset', 'train-rle.csv')\n",
    "data_folder = os.path.join(ROOT_PATH, 'SIIM_ACR_dataset', 'train_png')\n",
    "test_data_folder = os.path.join(ROOT_PATH, 'SIIM_ACR_dataset', 'test_png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1677512201988,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "Uh-pffjlIDDw"
   },
   "outputs": [],
   "source": [
    "def checkpath(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2263,
     "status": "ok",
     "timestamp": 1677512204231,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "SRDsG25wIKf-"
   },
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "Version = '230510_v0.0.1'\n",
    "\n",
    "root_folder = os.path.abspath(os.path.join(ROOT_PATH, Version))\n",
    "\n",
    "model_DIR = os.path.abspath(os.path.join(root_folder, 'model.pth'))\n",
    "checkpath(root_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupVit Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2463,
     "status": "ok",
     "timestamp": 1677512217559,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "1tdXYOBI7W7Q"
   },
   "outputs": [],
   "source": [
    "import collections.abc\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "\n",
    "# from ...activations import ACT2FN\n",
    "from transformers.activations import ACT2FN\n",
    "\n",
    "# from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n",
    "from transformers.modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n",
    "# from ...modeling_utils import PreTrainedModel\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "# from ...utils import (\n",
    "#     ModelOutput,\n",
    "#     add_start_docstrings,\n",
    "#     add_start_docstrings_to_model_forward,\n",
    "#     logging,\n",
    "#     replace_return_docstrings,\n",
    "# )\n",
    "from transformers.utils import (\n",
    "    ModelOutput,\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "# from .configuration_groupvit import GroupViTConfig, GroupViTTextConfig, GroupViTVisionConfig\n",
    "from transformers.models.groupvit.configuration_groupvit import GroupViTConfig, GroupViTTextConfig #, GroupViTVisionConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from typing import TYPE_CHECKING, Any, Mapping, Optional, Union\n",
    "\n",
    "# from ...configuration_utils import PretrainedConfig\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "# from ...onnx import OnnxConfig\n",
    "from transformers.onnx import OnnxConfig\n",
    "# from ...utils import logging\n",
    "from transformers.utils import logging\n",
    "\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    # from ...processing_utils import ProcessorMixin\n",
    "    from transformers.processing_utils import ProcessorMixin\n",
    "    # from ...utils import TensorType\n",
    "    from transformers.utils import TensorType\n",
    "\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "GROUPVIT_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n",
    "    \"nvidia/groupvit-gcc-yfcc\": \"https://huggingface.co/nvidia/groupvit-gcc-yfcc/resolve/main/config.json\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupViTVisionConfig(PretrainedConfig):\n",
    "    r\"\"\"\n",
    "    This is the configuration class to store the configuration of a [`GroupViTVisionModel`]. It is used to instantiate\n",
    "    an GroupViT model according to the specified arguments, defining the model architecture. Instantiating a\n",
    "    configuration with the defaults will yield a similar configuration to that of the GroupViT\n",
    "    [nvidia/groupvit-gcc-yfcc](https://huggingface.co/nvidia/groupvit-gcc-yfcc) architecture.\n",
    "\n",
    "    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n",
    "    documentation from [`PretrainedConfig`] for more information.\n",
    "\n",
    "    Args:\n",
    "        hidden_size (`int`, *optional*, defaults to 384):\n",
    "            Dimensionality of the encoder layers and the pooler layer.\n",
    "        intermediate_size (`int`, *optional*, defaults to 1536):\n",
    "            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n",
    "        depths (`List[int]`, *optional*, defaults to [6, 3, 3]):\n",
    "            The number of layers in each encoder block.\n",
    "        num_group_tokens (`List[int]`, *optional*, defaults to [64, 8, 0]):\n",
    "            The number of group tokens for each stage.\n",
    "        num_output_groups (`List[int]`, *optional*, defaults to [64, 8, 8]):\n",
    "            The number of output groups for each stage, 0 means no group.\n",
    "        num_attention_heads (`int`, *optional*, defaults to 6):\n",
    "            Number of attention heads for each attention layer in the Transformer encoder.\n",
    "        image_size (`int`, *optional*, defaults to 224):\n",
    "            The size (resolution) of each image.\n",
    "        patch_size (`int`, *optional*, defaults to 16):\n",
    "            The size (resolution) of each patch.\n",
    "        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n",
    "            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n",
    "            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` ``\"quick_gelu\"` are supported.\n",
    "        layer_norm_eps (`float`, *optional*, defaults to 1e-5):\n",
    "            The epsilon used by the layer normalization layers.\n",
    "        dropout (`float`, *optional*, defaults to 0.0):\n",
    "            The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.\n",
    "        attention_dropout (`float`, *optional*, defaults to 0.0):\n",
    "            The dropout ratio for the attention probabilities.\n",
    "        initializer_range (`float`, *optional*, defaults to 0.02):\n",
    "            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
    "        initializer_factor (`float`, *optional*, defaults to 1.0):\n",
    "            A factor for initializing all weight matrices (should be kept to 1, used internally for initialization\n",
    "            testing).\n",
    "\n",
    "    Example:\n",
    "\n",
    "    ```python\n",
    "    >>> from transformers import GroupViTVisionConfig, GroupViTVisionModel\n",
    "\n",
    "    >>> # Initializing a GroupViTVisionModel with nvidia/groupvit-gcc-yfcc style configuration\n",
    "    >>> configuration = GroupViTVisionConfig()\n",
    "\n",
    "    >>> model = GroupViTVisionModel(configuration)\n",
    "\n",
    "    >>> # Accessing the model configuration\n",
    "    >>> configuration = model.config\n",
    "    ```\"\"\"\n",
    "\n",
    "    model_type = \"groupvit_vision_model\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=384,\n",
    "        intermediate_size=1536,\n",
    "        depths=[6, 3, 3],\n",
    "        num_hidden_layers=12,\n",
    "        num_group_tokens=[64, 8, 0],\n",
    "        num_output_groups=[64, 8, 8],\n",
    "        num_attention_heads=6,\n",
    "        image_size=1024,\n",
    "        patch_size=16,\n",
    "        num_channels=3,\n",
    "        hidden_act=\"gelu\",\n",
    "        layer_norm_eps=1e-5,\n",
    "        dropout=0.0,\n",
    "        attention_dropout=0.0,\n",
    "        initializer_range=0.02,\n",
    "        initializer_factor=1.0,\n",
    "        assign_eps=1.0,\n",
    "        assign_mlp_ratio=[0.5, 4],\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.depths = depths\n",
    "        if num_hidden_layers != sum(depths):\n",
    "            logger.warning(\n",
    "                f\"Manually setting num_hidden_layers to {num_hidden_layers}, but we expect num_hidden_layers =\"\n",
    "                f\" sum(depth) = {sum(depths)}\"\n",
    "            )\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_group_tokens = num_group_tokens\n",
    "        self.num_output_groups = num_output_groups\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_channels = num_channels\n",
    "        self.hidden_act = hidden_act\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.dropout = dropout\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.initializer_range = initializer_range\n",
    "        self.initializer_factor = initializer_factor\n",
    "        self.assign_eps = assign_eps\n",
    "        self.assign_mlp_ratio = assign_mlp_ratio\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n",
    "        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
    "\n",
    "        # get the vision config dict if we are loading from GroupViTConfig\n",
    "        if config_dict.get(\"model_type\") == \"groupvit\":\n",
    "            config_dict = config_dict[\"vision_config\"]\n",
    "\n",
    "        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n",
    "            logger.warning(\n",
    "                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n",
    "                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n",
    "            )\n",
    "\n",
    "        return cls.from_dict(config_dict, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217559,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "M97ik37v9rbD"
   },
   "outputs": [],
   "source": [
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "_CHECKPOINT_FOR_DOC = \"nvidia/groupvit-gcc-yfcc\"\n",
    "\n",
    "GROUPVIT_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
    "    \"nvidia/groupvit-gcc-yfcc\",\n",
    "    # See all GroupViT models at https://huggingface.co/models?filter=groupvit\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217560,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "TfjSCoDn9utr"
   },
   "outputs": [],
   "source": [
    "# Copied from transformers.models.bart.modeling_bart._expand_mask\n",
    "def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n",
    "    \"\"\"\n",
    "    bsz, src_len = mask.size()\n",
    "    tgt_len = tgt_len if tgt_len is not None else src_len\n",
    "\n",
    "    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
    "\n",
    "    inverted_mask = 1.0 - expanded_mask\n",
    "\n",
    "    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217560,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "6PRDe_fu9xoT"
   },
   "outputs": [],
   "source": [
    "# contrastive loss function, adapted from\n",
    "# https://sachinruk.github.io/blog/pytorch/pytorch%20lightning/loss%20function/gpu/2021/03/07/CLIP.html\n",
    "def contrastive_loss(logits: torch.Tensor) -> torch.Tensor:\n",
    "    return nn.functional.cross_entropy(logits, torch.arange(len(logits), device=logits.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217560,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "NTvFTrtG9z1z"
   },
   "outputs": [],
   "source": [
    "# Copied from transformers.models.clip.modeling_clip.clip_loss with clip->groupvit\n",
    "def groupvit_loss(similarity: torch.Tensor) -> torch.Tensor:\n",
    "    caption_loss = contrastive_loss(similarity)\n",
    "    image_loss = contrastive_loss(similarity.t())\n",
    "    return (caption_loss + image_loss) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217560,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "iOvAJP0L91ns"
   },
   "outputs": [],
   "source": [
    "def hard_softmax(logits: torch.Tensor, dim: int):\n",
    "    y_soft = logits.softmax(dim)\n",
    "    # Straight through.\n",
    "    index = y_soft.max(dim, keepdim=True)[1]\n",
    "    y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(dim, index, 1.0)\n",
    "    ret = y_hard - y_soft.detach() + y_soft\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217560,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "6-b2H_fE93b1"
   },
   "outputs": [],
   "source": [
    "def gumbel_softmax(logits: torch.Tensor, tau: float = 1, hard: bool = False, dim: int = -1) -> torch.Tensor:\n",
    "    # more stable https://github.com/pytorch/pytorch/issues/41663\n",
    "    gumbel_dist = torch.distributions.gumbel.Gumbel(\n",
    "        torch.tensor(0.0, device=logits.device, dtype=logits.dtype),\n",
    "        torch.tensor(1.0, device=logits.device, dtype=logits.dtype),\n",
    "    )\n",
    "    gumbels = gumbel_dist.sample(logits.shape)\n",
    "\n",
    "    gumbels = (logits + gumbels) / tau  # ~Gumbel(logits,tau)\n",
    "    y_soft = gumbels.softmax(dim)\n",
    "\n",
    "    if hard:\n",
    "        # Straight through.\n",
    "        index = y_soft.max(dim, keepdim=True)[1]\n",
    "        y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(dim, index, 1.0)\n",
    "        ret = y_hard - y_soft.detach() + y_soft\n",
    "    else:\n",
    "        # Reparametrization trick.\n",
    "        ret = y_soft\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217561,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "sh78Xa0q9564"
   },
   "outputs": [],
   "source": [
    "def resize_attention_map(attentions, height, width, align_corners=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        attentions (`torch.Tensor`): attention map of shape [batch_size, groups, feat_height*feat_width]\n",
    "        height (`int`): height of the output attention map\n",
    "        width (`int`): width of the output attention map\n",
    "        align_corners (`bool`, *optional*): the `align_corner` argument for `nn.functional.interpolate`.\n",
    "\n",
    "    Returns:\n",
    "        `torch.Tensor`: resized attention map of shape [batch_size, groups, height, width]\n",
    "    \"\"\"\n",
    "\n",
    "    scale = (height * width // attentions.shape[2]) ** 0.5\n",
    "    if height > width:\n",
    "        feat_width = int(np.round(width / scale))\n",
    "        feat_height = attentions.shape[2] // feat_width\n",
    "    else:\n",
    "        feat_height = int(np.round(height / scale))\n",
    "        feat_width = attentions.shape[2] // feat_height\n",
    "\n",
    "    batch_size = attentions.shape[0]\n",
    "    groups = attentions.shape[1]  # number of group token\n",
    "    # [batch_size, groups, height*width, groups] -> [batch_size, groups, height, width]\n",
    "    attentions = attentions.reshape(batch_size, groups, feat_height, feat_width)\n",
    "    attentions = nn.functional.interpolate(\n",
    "        attentions, size=(height, width), mode=\"bilinear\", align_corners=align_corners\n",
    "    )\n",
    "    return attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217561,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "KyZtU92l98ix"
   },
   "outputs": [],
   "source": [
    "def get_grouping_from_attentions(attentions, hw_shape):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        attentions (`tuple(torch.FloatTensor)`: tuple of attention maps returned by `GroupViTVisionTransformer`\n",
    "        hw_shape (`tuple(int)`): height and width of the output attention map\n",
    "    Returns:\n",
    "        `torch.Tensor`: the attention map of shape [batch_size, groups, height, width]\n",
    "    \"\"\"\n",
    "\n",
    "    attn_maps = []\n",
    "    with torch.no_grad():\n",
    "        prev_attn_masks = None\n",
    "        for attn_masks in attentions:\n",
    "            # [batch_size, num_groups, height x width] -> [batch_size, height x width, num_groups]\n",
    "            attn_masks = attn_masks.permute(0, 2, 1).contiguous()\n",
    "            if prev_attn_masks is None:\n",
    "                prev_attn_masks = attn_masks\n",
    "            else:\n",
    "                prev_attn_masks = prev_attn_masks @ attn_masks\n",
    "            # [batch_size, heightxwidth, num_groups] -> [batch_size, num_groups, heightxwidth] -> [batch_size, num_groups, height, width]\n",
    "            cur_attn_map = resize_attention_map(prev_attn_masks.permute(0, 2, 1).contiguous(), *hw_shape)\n",
    "            attn_maps.append(cur_attn_map)\n",
    "\n",
    "    # [batch_size, num_groups, height, width]\n",
    "    final_grouping = attn_maps[-1]\n",
    "\n",
    "    return final_grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217561,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "sX37CrAn9_Mw"
   },
   "outputs": [],
   "source": [
    "class GroupViTCrossAttentionLayer(nn.Module):\n",
    "    def __init__(self, config: GroupViTVisionConfig):\n",
    "        super().__init__()\n",
    "        self.attn = GroupViTAttention(config)\n",
    "        self.norm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.mlp = GroupViTMLP(config)\n",
    "        self.norm_post = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, query, key):\n",
    "        x = query\n",
    "        x = x + self.attn(query, encoder_hidden_states=key)[0]\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        x = self.norm_post(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217561,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "LfMDcjbu-BFh"
   },
   "outputs": [],
   "source": [
    "class GroupViTAssignAttention(nn.Module):\n",
    "    def __init__(self, config: GroupViTVisionConfig):\n",
    "        super().__init__()\n",
    "        self.scale = config.hidden_size**-0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.proj = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.assign_eps = config.assign_eps\n",
    "\n",
    "    def get_attn(self, attn, gumbel=True, hard=True):\n",
    "        if gumbel and self.training:\n",
    "            attn = gumbel_softmax(attn, dim=-2, hard=hard)\n",
    "        else:\n",
    "            if hard:\n",
    "                attn = hard_softmax(attn, dim=-2)\n",
    "            else:\n",
    "                attn = nn.functional.softmax(attn, dim=-2)\n",
    "\n",
    "        return attn\n",
    "\n",
    "    def forward(self, query, key):\n",
    "        value = key\n",
    "        # [batch_size, query_length, channels]\n",
    "        query = self.q_proj(query)\n",
    "\n",
    "        # [batch_size, key_length, channels]\n",
    "        key = self.k_proj(key)\n",
    "\n",
    "        # [batch_size, key_length, channels]\n",
    "        value = self.v_proj(value)\n",
    "\n",
    "        # [batch_size, query_length, key_length]\n",
    "        raw_attn = (query @ key.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        attn = self.get_attn(raw_attn)\n",
    "        soft_attn = self.get_attn(raw_attn, gumbel=False, hard=False)\n",
    "\n",
    "        attn = attn / (attn.sum(dim=-1, keepdim=True) + self.assign_eps)\n",
    "\n",
    "        out = attn @ value\n",
    "\n",
    "        out = self.proj(out)\n",
    "\n",
    "        return out, soft_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217562,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "yOJBrOGt-GKD"
   },
   "outputs": [],
   "source": [
    "class GroupViTTokenAssign(nn.Module):\n",
    "    def __init__(self, config: GroupViTVisionConfig, num_group_token, num_output_group):\n",
    "        super().__init__()\n",
    "        self.num_output_group = num_output_group\n",
    "        # norm on group_tokens\n",
    "        self.norm_tokens = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        assign_mlp_ratio = (\n",
    "            config.assign_mlp_ratio\n",
    "            if isinstance(config.assign_mlp_ratio, collections.abc.Iterable)\n",
    "            else (config.assign_mlp_ratio, config.assign_mlp_ratio)\n",
    "        )\n",
    "        tokens_dim, channels_dim = [int(x * config.hidden_size) for x in assign_mlp_ratio]\n",
    "        self.mlp_inter = GroupViTMixerMLP(config, num_group_token, tokens_dim, num_output_group)\n",
    "        self.norm_post_tokens = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        # norm on x\n",
    "        self.norm_x = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.pre_assign_attn = GroupViTCrossAttentionLayer(config)\n",
    "\n",
    "        self.assign = GroupViTAssignAttention(config)\n",
    "        self.norm_new_x = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.mlp_channels = GroupViTMLP(config, config.hidden_size, channels_dim, config.hidden_size)\n",
    "\n",
    "    def project_group_token(self, group_tokens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            group_tokens (torch.Tensor): group tokens, [batch_size, num_group_tokens, channels]\n",
    "\n",
    "        Returns:\n",
    "            projected_group_tokens (torch.Tensor): [batch_size, num_output_groups, channels]\n",
    "        \"\"\"\n",
    "        # [B, num_output_groups, C] <- [B, num_group_tokens, C]\n",
    "        projected_group_tokens = self.mlp_inter(group_tokens)\n",
    "        projected_group_tokens = self.norm_post_tokens(projected_group_tokens)\n",
    "        return projected_group_tokens\n",
    "\n",
    "    def forward(self, image_tokens, group_tokens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_tokens (`torch.Tensor`): image tokens, of shape [batch_size, input_length, channels]\n",
    "            group_tokens (`torch.Tensor`): group tokens, [batch_size, num_group_tokens, channels]\n",
    "        \"\"\"\n",
    "\n",
    "        group_tokens = self.norm_tokens(group_tokens)\n",
    "        image_tokens = self.norm_x(image_tokens)\n",
    "        # [batch_size, num_output_groups, channels]\n",
    "        projected_group_tokens = self.project_group_token(group_tokens)\n",
    "        projected_group_tokens = self.pre_assign_attn(projected_group_tokens, image_tokens)\n",
    "        new_image_tokens, attention = self.assign(projected_group_tokens, image_tokens)\n",
    "        new_image_tokens += projected_group_tokens\n",
    "\n",
    "        new_image_tokens = new_image_tokens + self.mlp_channels(self.norm_new_x(new_image_tokens))\n",
    "\n",
    "        return new_image_tokens, attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217562,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "1eBLeUct-JCq"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GroupViTModelOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n",
    "            Contrastive loss for image-text similarity.\n",
    "        logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n",
    "            The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n",
    "            similarity scores.\n",
    "        logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n",
    "            The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n",
    "            similarity scores.\n",
    "        segmentation_logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels, logits_height, logits_width)`):\n",
    "            Classification scores for each pixel.\n",
    "\n",
    "            <Tip warning={true}>\n",
    "\n",
    "            The logits returned do not necessarily have the same size as the `pixel_values` passed as inputs. This is\n",
    "            to avoid doing two interpolations and lose some quality when a user needs to resize the logits to the\n",
    "            original image size as post-processing. You should always check your logits shape and resize as needed.\n",
    "\n",
    "            </Tip>\n",
    "\n",
    "        text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n",
    "            The text embeddings obtained by applying the projection layer to the pooled output of\n",
    "            [`GroupViTTextModel`].\n",
    "        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n",
    "            The image embeddings obtained by applying the projection layer to the pooled output of\n",
    "            [`GroupViTVisionModel`].\n",
    "        text_model_output (`BaseModelOutputWithPooling`):\n",
    "            The output of the [`GroupViTTextModel`].\n",
    "        vision_model_output (`BaseModelOutputWithPooling`):\n",
    "            The output of the [`GroupViTVisionModel`].\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits_per_image: torch.FloatTensor = None\n",
    "    logits_per_text: torch.FloatTensor = None\n",
    "    segmentation_logits: torch.FloatTensor = None\n",
    "    text_embeds: torch.FloatTensor = None\n",
    "    image_embeds: torch.FloatTensor = None\n",
    "    text_model_output: BaseModelOutputWithPooling = None\n",
    "    vision_model_output: BaseModelOutputWithPooling = None\n",
    "\n",
    "    def to_tuple(self) -> Tuple[Any]:\n",
    "        return tuple(\n",
    "            self[k] if k not in [\"text_model_output\", \"vision_model_output\"] else getattr(self, k).to_tuple()\n",
    "            for k in self.keys()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217562,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "8atGssmE-OgA"
   },
   "outputs": [],
   "source": [
    "class GroupViTPatchEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Image to Patch Embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int = 224,\n",
    "        patch_size: Union[int, Tuple[int, int]] = 16,\n",
    "        num_channels: int = 3,\n",
    "        embed_dim: int = 768,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        image_size = image_size if isinstance(image_size, collections.abc.Iterable) else (image_size, image_size)\n",
    "        patch_size = patch_size if isinstance(patch_size, collections.abc.Iterable) else (patch_size, patch_size)\n",
    "        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.projection = nn.Conv2d(num_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor, interpolate_pos_encoding: bool = False) -> torch.Tensor:\n",
    "        batch_size, num_channels, height, width = pixel_values.shape\n",
    "        if not interpolate_pos_encoding:\n",
    "            if height != self.image_size[0] or width != self.image_size[1]:\n",
    "                raise ValueError(\n",
    "                    f\"Input image size ({height}*{width}) doesn't match model\"\n",
    "                    f\" ({self.image_size[0]}*{self.image_size[1]}).\"\n",
    "                )\n",
    "        x = self.projection(pixel_values).flatten(2).transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217562,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "pvmCYxCX-Q5x"
   },
   "outputs": [],
   "source": [
    "class GroupViTVisionEmbeddings(nn.Module):\n",
    "    def __init__(self, config: GroupViTVisionConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embeddings = GroupViTPatchEmbeddings(\n",
    "            image_size=config.image_size,\n",
    "            patch_size=config.patch_size,\n",
    "            num_channels=config.num_channels,\n",
    "            embed_dim=config.hidden_size,\n",
    "        )\n",
    "        num_patches = self.patch_embeddings.num_patches\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches, config.hidden_size))\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.config = config\n",
    "\n",
    "    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher\n",
    "        resolution images.\n",
    "\n",
    "        Source:\n",
    "        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n",
    "        \"\"\"\n",
    "\n",
    "        npatch = embeddings.shape[1]\n",
    "        if npatch == self.position_embeddings.shape[1] and height == width:\n",
    "            return self.position_embeddings\n",
    "        patch_pos_embed = self.position_embeddings\n",
    "        num_original_pos_embed = patch_pos_embed.shape[1]\n",
    "        dim = embeddings.shape[-1]\n",
    "        feat_height = height // self.config.patch_size\n",
    "        feat_width = width // self.config.patch_size\n",
    "        # we add a small number to avoid floating point error in the interpolation\n",
    "        # see discussion at https://github.com/facebookresearch/dino/issues/8\n",
    "        feat_height, feat_width = feat_height + 0.1, feat_width + 0.1\n",
    "        original_height = original_width = math.sqrt(num_original_pos_embed)\n",
    "        reshaped_patch_pos_embed = patch_pos_embed.reshape(1, int(original_height), int(original_width), dim).permute(\n",
    "            0, 3, 1, 2\n",
    "        )\n",
    "        scale_factor = (feat_height / original_height, feat_width / original_width)\n",
    "        patch_pos_embed = nn.functional.interpolate(\n",
    "            reshaped_patch_pos_embed,\n",
    "            scale_factor=scale_factor,\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return patch_pos_embed\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor, interpolate_pos_encoding: bool = False) -> torch.Tensor:\n",
    "        batch_size, num_channels, height, width = pixel_values.shape\n",
    "        embeddings = self.patch_embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n",
    "\n",
    "        embeddings = self.layernorm(embeddings)\n",
    "\n",
    "        batch_size, seq_len, _ = embeddings.size()\n",
    "\n",
    "        # add positional encoding to each token\n",
    "        if interpolate_pos_encoding:\n",
    "            embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)\n",
    "        else:\n",
    "            embeddings = embeddings + self.position_embeddings\n",
    "\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217563,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "jZ_xSDus-ULA"
   },
   "outputs": [],
   "source": [
    "# Copied from transformers.models.clip.modeling_clip.CLIPTextEmbeddings with CLIP->GroupViT\n",
    "class GroupViTTextEmbeddings(nn.Module):\n",
    "    def __init__(self, config: GroupViTTextConfig):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(config.max_position_embeddings, embed_dim)\n",
    "\n",
    "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
    "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        seq_length = input_ids.shape[-1] if input_ids is not None else inputs_embeds.shape[-2]\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, :seq_length]\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.token_embedding(input_ids)\n",
    "\n",
    "        position_embeddings = self.position_embedding(position_ids)\n",
    "        embeddings = inputs_embeds + position_embeddings\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217563,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "WI0LJqF--anN"
   },
   "outputs": [],
   "source": [
    "class GroupViTStage(nn.Module):\n",
    "    \"\"\"This corresponds to the `GroupingLayer` class in the GroupViT implementation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: GroupViTVisionConfig,\n",
    "        depth: int,\n",
    "        num_prev_group_token: int,\n",
    "        num_group_token: int,\n",
    "        num_output_group: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.num_group_token = num_group_token\n",
    "        if num_group_token > 0:\n",
    "            self.group_token = nn.Parameter(torch.zeros(1, num_group_token, config.hidden_size))\n",
    "        else:\n",
    "            self.group_token = None\n",
    "        self.gradient_checkpointing = False\n",
    "        self.layers = nn.ModuleList([GroupViTEncoderLayer(config) for _ in range(depth)])\n",
    "\n",
    "        if num_group_token > 0:\n",
    "            self.downsample = GroupViTTokenAssign(\n",
    "                config=config,\n",
    "                num_group_token=num_group_token,\n",
    "                num_output_group=num_output_group,\n",
    "            )\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "        if num_prev_group_token > 0 and num_group_token > 0:\n",
    "            self.group_projector = nn.Sequential(\n",
    "                nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps),\n",
    "                GroupViTMixerMLP(config, num_prev_group_token, config.hidden_size // 2, num_group_token),\n",
    "            )\n",
    "        else:\n",
    "            self.group_projector = None\n",
    "\n",
    "    @property\n",
    "    def with_group_token(self):\n",
    "        return self.group_token is not None\n",
    "\n",
    "    def split_x(self, x):\n",
    "        if self.with_group_token:\n",
    "            return x[:, : -self.num_group_token], x[:, -self.num_group_token :]\n",
    "        else:\n",
    "            return x, None\n",
    "\n",
    "    def concat_x(self, x: torch.Tensor, group_token: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        if group_token is None:\n",
    "            return x\n",
    "        return torch.cat([x, group_token], dim=1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        prev_group_token: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.FloatTensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`): attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "                `(config.encoder_attention_heads,)`.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the grouping tensors of Grouping block.\n",
    "        \"\"\"\n",
    "        if self.with_group_token:\n",
    "            group_token = self.group_token.expand(hidden_states.size(0), -1, -1)\n",
    "            if self.group_projector is not None:\n",
    "                group_token = group_token + self.group_projector(prev_group_token)\n",
    "        else:\n",
    "            group_token = None\n",
    "\n",
    "        x = hidden_states\n",
    "\n",
    "        cat_x = self.concat_x(x, group_token)\n",
    "        for layer in self.layers:\n",
    "            layer_out = layer(cat_x, attention_mask=None, causal_attention_mask=None)\n",
    "            cat_x = layer_out[0]\n",
    "\n",
    "        x, group_token = self.split_x(cat_x)\n",
    "\n",
    "        attention = None\n",
    "        if self.downsample is not None:\n",
    "            x, attention = self.downsample(x, group_token)\n",
    "\n",
    "        outputs = (x, group_token)\n",
    "        if output_attentions:\n",
    "            outputs = outputs + (attention,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217563,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "lZNeThbG-bdS"
   },
   "outputs": [],
   "source": [
    "class GroupViTMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: GroupViTVisionConfig,\n",
    "        hidden_size: Optional[int] = None,\n",
    "        intermediate_size: Optional[int] = None,\n",
    "        output_size: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.activation_fn = ACT2FN[config.hidden_act]\n",
    "        hidden_size = hidden_size if hidden_size is not None else config.hidden_size\n",
    "        intermediate_size = intermediate_size if intermediate_size is not None else config.intermediate_size\n",
    "        output_size = output_size if output_size is not None else hidden_size\n",
    "        self.fc1 = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.fc2 = nn.Linear(intermediate_size, output_size)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.fc1(hidden_states)\n",
    "        hidden_states = self.activation_fn(hidden_states)\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217563,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "F5PfLQe2-dnX"
   },
   "outputs": [],
   "source": [
    "class GroupViTMixerMLP(GroupViTMLP):\n",
    "    def forward(self, x):\n",
    "        x = super().forward(x.transpose(1, 2))\n",
    "        return x.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217564,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "zIDEv8MG-fPE"
   },
   "outputs": [],
   "source": [
    "class GroupViTAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        if self.head_dim * self.num_heads != self.embed_dim:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n",
    "                f\" {self.num_heads}).\"\n",
    "            )\n",
    "        self.scale = self.head_dim**-0.5\n",
    "        self.dropout = config.attention_dropout\n",
    "\n",
    "        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
    "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        causal_attention_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
    "\n",
    "        bsz, tgt_len, embed_dim = hidden_states.size()\n",
    "        is_cross_attention = encoder_hidden_states is not None\n",
    "\n",
    "        # get query proj\n",
    "        query_states = self.q_proj(hidden_states) * self.scale\n",
    "        if is_cross_attention:\n",
    "            key_states = self._shape(self.k_proj(encoder_hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(encoder_hidden_states), -1, bsz)\n",
    "        else:\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "\n",
    "        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
    "        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
    "        key_states = key_states.view(*proj_shape)\n",
    "        value_states = value_states.view(*proj_shape)\n",
    "\n",
    "        src_len = key_states.size(1)\n",
    "        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
    "\n",
    "        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n",
    "                f\" {attn_weights.size()}\"\n",
    "            )\n",
    "\n",
    "        # apply the causal_attention_mask first\n",
    "        if causal_attention_mask is not None:\n",
    "            if causal_attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is\"\n",
    "                    f\" {causal_attention_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        if output_attentions:\n",
    "            # this operation is a bit akward, but it's required to\n",
    "            # make sure that attn_weights keeps its gradient.\n",
    "            # In order to do so, attn_weights have to reshaped\n",
    "            # twice and have to be reused in the following\n",
    "            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "        else:\n",
    "            attn_weights_reshaped = None\n",
    "\n",
    "        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "\n",
    "        attn_output = torch.bmm(attn_probs, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "        attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
    "\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights_reshaped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217564,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "MxxVauJA-ioM"
   },
   "outputs": [],
   "source": [
    "# Copied from transformers.models.clip.modeling_clip.CLIPEncoderLayer with CLIP->GroupViT\n",
    "class GroupViTEncoderLayer(nn.Module):\n",
    "    def __init__(self, config: GroupViTConfig):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.self_attn = GroupViTAttention(config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n",
    "        self.mlp = GroupViTMLP(config)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        causal_attention_mask: torch.Tensor,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.FloatTensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`): attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "                `(config.encoder_attention_heads,)`.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "        \"\"\"\n",
    "        residual = hidden_states\n",
    "\n",
    "        hidden_states = self.layer_norm1(hidden_states)\n",
    "        hidden_states, attn_weights = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            causal_attention_mask=causal_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.layer_norm2(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs += (attn_weights,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217564,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "iFAIWiAZ-k9h"
   },
   "outputs": [],
   "source": [
    "class GroupViTPreTrainedModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
    "    models.\n",
    "    \"\"\"\n",
    "\n",
    "    config_class = GroupViTConfig\n",
    "    base_model_prefix = \"groupvit\"\n",
    "    supports_gradient_checkpointing = True\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "\n",
    "        init_range = self.config.initializer_range\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=init_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "        factor = self.config.initializer_factor\n",
    "        if isinstance(module, GroupViTTextEmbeddings):\n",
    "            module.token_embedding.weight.data.normal_(mean=0.0, std=factor * 0.02)\n",
    "            module.position_embedding.weight.data.normal_(mean=0.0, std=factor * 0.02)\n",
    "        elif isinstance(module, GroupViTAttention):\n",
    "            factor = self.config.initializer_factor\n",
    "            in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n",
    "            out_proj_std = (module.embed_dim**-0.5) * factor\n",
    "            nn.init.normal_(module.q_proj.weight, std=in_proj_std)\n",
    "            nn.init.normal_(module.k_proj.weight, std=in_proj_std)\n",
    "            nn.init.normal_(module.v_proj.weight, std=in_proj_std)\n",
    "            nn.init.normal_(module.out_proj.weight, std=out_proj_std)\n",
    "        elif isinstance(module, GroupViTMLP):\n",
    "            factor = self.config.initializer_factor\n",
    "            in_proj_std = (\n",
    "                (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n",
    "            )\n",
    "            fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n",
    "            nn.init.normal_(module.fc1.weight, std=fc_std)\n",
    "            nn.init.normal_(module.fc2.weight, std=in_proj_std)\n",
    "\n",
    "    def _set_gradient_checkpointing(self, module, value=False):\n",
    "        if isinstance(module, (GroupViTTextEncoder, GroupViTVisionEncoder)):\n",
    "            module.gradient_checkpointing = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217564,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "jzru1jUe-wAJ"
   },
   "outputs": [],
   "source": [
    "GROUPVIT_START_DOCSTRING = r\"\"\"\n",
    "    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n",
    "    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n",
    "    behavior.\n",
    "\n",
    "    Parameters:\n",
    "        config ([`GroupViTConfig`]): Model configuration class with all the parameters of the model.\n",
    "            Initializing with a config file does not load the weights associated with the model, only the\n",
    "            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
    "\"\"\"\n",
    "\n",
    "GROUPVIT_TEXT_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n",
    "            it.\n",
    "\n",
    "            Indices can be obtained using [`CLIPTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
    "            [`PreTrainedTokenizer.__call__`] for details.\n",
    "\n",
    "            [What are input IDs?](../glossary#input-ids)\n",
    "        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "\n",
    "            [What are attention masks?](../glossary#attention-mask)\n",
    "        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n",
    "            config.max_position_embeddings - 1]`.\n",
    "\n",
    "            [What are position IDs?](../glossary#position-ids)\n",
    "        output_attentions (`bool`, *optional*):\n",
    "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
    "            tensors for more detail.\n",
    "        output_hidden_states (`bool`, *optional*):\n",
    "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
    "            more detail.\n",
    "        return_dict (`bool`, *optional*):\n",
    "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "\"\"\"\n",
    "\n",
    "GROUPVIT_VISION_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n",
    "            Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using\n",
    "            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details.\n",
    "        output_attentions (`bool`, *optional*):\n",
    "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
    "            tensors for more detail.\n",
    "        output_hidden_states (`bool`, *optional*):\n",
    "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
    "            more detail.\n",
    "        return_dict (`bool`, *optional*):\n",
    "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "\"\"\"\n",
    "\n",
    "GROUPVIT_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n",
    "            it.\n",
    "\n",
    "            Indices can be obtained using [`CLIPTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
    "            [`PreTrainedTokenizer.__call__`] for details.\n",
    "\n",
    "            [What are input IDs?](../glossary#input-ids)\n",
    "        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "\n",
    "            [What are attention masks?](../glossary#attention-mask)\n",
    "        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n",
    "            config.max_position_embeddings - 1]`.\n",
    "\n",
    "            [What are position IDs?](../glossary#position-ids)\n",
    "        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n",
    "            Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See\n",
    "            [`CLIPImageProcessor.__call__`] for details.\n",
    "        return_loss (`bool`, *optional*):\n",
    "            Whether or not to return the contrastive loss.\n",
    "        output_attentions (`bool`, *optional*):\n",
    "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
    "            tensors for more detail.\n",
    "        output_hidden_states (`bool`, *optional*):\n",
    "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
    "            more detail.\n",
    "        return_dict (`bool`, *optional*):\n",
    "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217564,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "diwTOUkc-yuM"
   },
   "outputs": [],
   "source": [
    "class GroupViTVisionEncoder(nn.Module):\n",
    "    def __init__(self, config: GroupViTVisionConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.stages = nn.ModuleList(\n",
    "            [\n",
    "                GroupViTStage(\n",
    "                    config=config,\n",
    "                    depth=config.depths[i],\n",
    "                    num_group_token=config.num_group_tokens[i],\n",
    "                    num_output_group=config.num_output_groups[i],\n",
    "                    num_prev_group_token=config.num_output_groups[i - 1] if i > 0 else 0,\n",
    "                )\n",
    "                for i in range(len(config.depths))\n",
    "            ]\n",
    "        )\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[tuple, BaseModelOutput]:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_groupings = () if output_attentions else None\n",
    "\n",
    "        group_tokens = None\n",
    "\n",
    "        for i, stage in enumerate(self.stages):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_outputs = stage(hidden_states, group_tokens, output_attentions)\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            group_tokens = layer_outputs[1]\n",
    "\n",
    "            if output_attentions and layer_outputs[2] is not None:\n",
    "                all_groupings = all_groupings + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, all_hidden_states, all_groupings] if v is not None)\n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=hidden_states, hidden_states=all_hidden_states, attentions=all_groupings\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217565,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "ErgnsQgL-zvh"
   },
   "outputs": [],
   "source": [
    "class GroupViTTextEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder consisting of `config.num_hidden_layers` self-attention layers. Each layer is a\n",
    "    [`GroupViTEncoderLayer`].\n",
    "\n",
    "    Args:\n",
    "        config: GroupViTTextConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: GroupViTTextConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layers = nn.ModuleList([GroupViTEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        inputs_embeds,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        causal_attention_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutput]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n",
    "                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n",
    "                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n",
    "                than the model's internal embedding lookup matrix.\n",
    "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            causal_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Causal mask for the text model. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            output_hidden_states (`bool`, *optional*):\n",
    "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
    "                for more detail.\n",
    "            return_dict (`bool`, *optional*):\n",
    "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        encoder_states = () if output_hidden_states else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "        for idx, encoder_layer in enumerate(self.layers):\n",
    "            if output_hidden_states:\n",
    "                encoder_states = encoder_states + (hidden_states,)\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(encoder_layer),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    causal_attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = encoder_layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    causal_attention_mask,\n",
    "                    output_attentions=output_attentions,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            encoder_states = encoder_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217566,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "fsgeLvGW_gB0"
   },
   "outputs": [],
   "source": [
    "# Copied from transformers.models.clip.modeling_clip.CLIPTextTransformer with CLIPText->GroupViTText, CLIPEncoder->GroupViTTextEncoder, CLIP_TEXT->GROUPVIT_TEXT\n",
    "class GroupViTTextTransformer(nn.Module):\n",
    "    def __init__(self, config: GroupViTTextConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        embed_dim = config.hidden_size\n",
    "        self.embeddings = GroupViTTextEmbeddings(config)\n",
    "        self.encoder = GroupViTTextEncoder(config)\n",
    "        self.final_layer_norm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(GROUPVIT_TEXT_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=GroupViTTextConfig)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if input_ids is None:\n",
    "            raise ValueError(\"You have to specify input_ids\")\n",
    "\n",
    "        input_shape = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, input_shape[-1])\n",
    "\n",
    "        hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n",
    "\n",
    "        bsz, seq_len = input_shape\n",
    "        # CLIP's text model uses causal mask, prepare it here.\n",
    "        # https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/clip/model.py#L324\n",
    "        causal_attention_mask = self._build_causal_attention_mask(bsz, seq_len, hidden_states.dtype).to(\n",
    "            hidden_states.device\n",
    "        )\n",
    "        # expand attention_mask\n",
    "        if attention_mask is not None:\n",
    "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "            attention_mask = _expand_mask(attention_mask, hidden_states.dtype)\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            inputs_embeds=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            causal_attention_mask=causal_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        last_hidden_state = encoder_outputs[0]\n",
    "        last_hidden_state = self.final_layer_norm(last_hidden_state)\n",
    "\n",
    "        # text_embeds.shape = [batch_size, sequence_length, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        # casting to torch.int for onnx compatibility: argmax doesn't support int64 inputs with opset 14\n",
    "        pooled_output = last_hidden_state[\n",
    "            torch.arange(last_hidden_state.shape[0], device=last_hidden_state.device),\n",
    "            input_ids.to(dtype=torch.int, device=last_hidden_state.device).argmax(dim=-1),\n",
    "        ]\n",
    "\n",
    "        if not return_dict:\n",
    "            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPooling(\n",
    "            last_hidden_state=last_hidden_state,\n",
    "            pooler_output=pooled_output,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def _build_causal_attention_mask(self, bsz, seq_len, dtype):\n",
    "        # lazily create causal attention mask, with full attention between the vision tokens\n",
    "        # pytorch uses additive attention mask; fill with -inf\n",
    "        mask = torch.empty(bsz, seq_len, seq_len, dtype=dtype)\n",
    "        mask.fill_(torch.tensor(torch.finfo(dtype).min))\n",
    "        mask.triu_(1)  # zero out the lower diagonal\n",
    "        mask = mask.unsqueeze(1)  # expand mask\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 1409,
     "status": "ok",
     "timestamp": 1677512218961,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "QAAhPken_ibc"
   },
   "outputs": [],
   "source": [
    "class GroupViTTextModel(GroupViTPreTrainedModel):\n",
    "    config_class = GroupViTTextConfig\n",
    "\n",
    "    def __init__(self, config: GroupViTTextConfig):\n",
    "        super().__init__(config)\n",
    "        self.text_model = GroupViTTextTransformer(config)\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self) -> nn.Module:\n",
    "        return self.text_model.embeddings.token_embedding\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.text_model.embeddings.token_embedding = value\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(GROUPVIT_TEXT_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=GroupViTTextConfig)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import CLIPTokenizer, GroupViTTextModel\n",
    "\n",
    "        >>> tokenizer = CLIPTokenizer.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "        >>> model = GroupViTTextModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "\n",
    "        >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n",
    "\n",
    "        >>> outputs = model(**inputs)\n",
    "        >>> last_hidden_state = outputs.last_hidden_state\n",
    "        >>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n",
    "        ```\"\"\"\n",
    "        return self.text_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1677512218961,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "pNJ4HozD_ls2"
   },
   "outputs": [],
   "source": [
    "class GroupViTVisionTransformer(nn.Module):########################\n",
    "    def __init__(self, config: GroupViTVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        embed_dim = config.hidden_size\n",
    "\n",
    "        self.embeddings = GroupViTVisionEmbeddings(config)\n",
    "        self.encoder = GroupViTVisionEncoder(config)\n",
    "        self.layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(GROUPVIT_VISION_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=GroupViTVisionConfig)\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if pixel_values is None:\n",
    "            raise ValueError(\"You have to specify pixel_values\")\n",
    "\n",
    "        hidden_states = self.embeddings(pixel_values)\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            hidden_states=hidden_states,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            output_attentions=output_attentions,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        last_hidden_state = encoder_outputs[0]\n",
    "\n",
    "        # normalize the last hidden state\n",
    "        last_hidden_state = self.layernorm(last_hidden_state)\n",
    "        pooled_output = last_hidden_state.mean(dim=1)\n",
    "\n",
    "        if not return_dict:\n",
    "            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPooling(\n",
    "            last_hidden_state=last_hidden_state,\n",
    "            pooler_output=pooled_output,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1677512218961,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "Tf5zCFfp_pS0"
   },
   "outputs": [],
   "source": [
    "class GroupViTVisionModel(GroupViTPreTrainedModel):\n",
    "    config_class = GroupViTVisionConfig\n",
    "    main_input_name = \"pixel_values\"\n",
    "\n",
    "    def __init__(self, config: GroupViTVisionConfig, projection_dim=128):\n",
    "        super().__init__(config)\n",
    "        self.vision_model = GroupViTVisionTransformer(config)\n",
    "\n",
    "        self.projection_dim = projection_dim\n",
    "        self.projection_intermediate_dim = 4096\n",
    "        self.vision_embed_dim = config.hidden_size\n",
    "\n",
    "        self.visual_projection = nn.Sequential(\n",
    "            nn.Linear(self.vision_embed_dim, self.projection_intermediate_dim, bias=True),\n",
    "            nn.BatchNorm1d(self.projection_intermediate_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.projection_intermediate_dim, self.projection_dim, bias=True),\n",
    "        )\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self) -> GroupViTPatchEmbeddings:\n",
    "        return self.vision_model.embeddings.patch_embeddings\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(GROUPVIT_VISION_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=GroupViTVisionConfig)\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from PIL import Image\n",
    "        >>> import requests\n",
    "        >>> from transformers import AutoProcessor, GroupViTVisionModel\n",
    "\n",
    "        >>> processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "        >>> model = GroupViTVisionModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "\n",
    "        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "        >>> image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "        >>> inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "        >>> outputs = model(**inputs)\n",
    "        >>> last_hidden_state = outputs.last_hidden_state\n",
    "        >>> pooled_output = outputs.pooler_output  # pooled CLS states\n",
    "        ```\"\"\"\n",
    "\n",
    "        # print(type(pixel_values), type(output_attentions), type(output_hidden_states), type(return_dict))\n",
    "        # print(pixel_values.shape, output_attentions, output_hidden_states, return_dict)\n",
    "        # print('pixel_values=', pixel_values.shape)\n",
    "        output_attentions = True\n",
    "        output_hidden_states = False\n",
    "        return_dict = True\n",
    "        # print(pixel_values.shape, output_attentions, output_hidden_states, return_dict)\n",
    "        vision_outputs = self.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        # print(vision_outputs)\n",
    "\n",
    "        attentions = vision_outputs[2]\n",
    "            \n",
    "        # [batch_size_image, num_group, height, width]\n",
    "        grouping = get_grouping_from_attentions(attentions, pixel_values.shape[2:])\n",
    "        seg_logits = grouping\n",
    "\n",
    "        pooled_output = vision_outputs[1]  # pooled_output\n",
    "        image_features = self.visual_projection(pooled_output)\n",
    "\n",
    "        # print(image_features.shape)\n",
    "        return vision_outputs, seg_logits, image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1677512218962,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "Q3lIeXoH_slB"
   },
   "outputs": [],
   "source": [
    "@add_start_docstrings(GROUPVIT_START_DOCSTRING)\n",
    "class GroupViTModel(GroupViTPreTrainedModel):\n",
    "    config_class = GroupViTConfig\n",
    "\n",
    "    def __init__(self, config: GroupViTConfig):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # if not isinstance(config.text_config, GroupViTTextConfig):\n",
    "        #     raise ValueError(\n",
    "        #         \"config.text_config is expected to be of type GroupViTTextConfig but is of type\"\n",
    "        #         f\" {type(config.text_config)}.\"\n",
    "        #     )\n",
    "\n",
    "        if not isinstance(config.vision_config, GroupViTVisionConfig):\n",
    "            raise ValueError(\n",
    "                \"config.vision_config is expected to be of type GroupViTVisionConfig but is of type\"\n",
    "                f\" {type(config.vision_config)}.\"\n",
    "            )\n",
    "\n",
    "        # text_config = config.text_config\n",
    "        vision_config = config.vision_config\n",
    "\n",
    "        self.projection_dim = config.projection_dim\n",
    "        self.projection_intermediate_dim = config.projection_intermediate_dim\n",
    "        # self.text_embed_dim = text_config.hidden_size\n",
    "        self.vision_embed_dim = vision_config.hidden_size\n",
    "        print('hidden_size', vision_config.hidden_size)\n",
    "\n",
    "        # self.text_model = GroupViTTextTransformer(text_config)\n",
    "        self.vision_model = GroupViTVisionTransformer(vision_config)\n",
    "\n",
    "        self.visual_projection = nn.Sequential(\n",
    "            nn.Linear(self.vision_embed_dim, self.projection_intermediate_dim, bias=True),\n",
    "            nn.BatchNorm1d(self.projection_intermediate_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.projection_intermediate_dim, self.projection_dim, bias=True),\n",
    "        )\n",
    "        # self.text_projection = nn.Sequential(\n",
    "        #     nn.Linear(self.text_embed_dim, self.projection_intermediate_dim, bias=True),\n",
    "        #     nn.BatchNorm1d(self.projection_intermediate_dim),\n",
    "        #     nn.ReLU(inplace=True),\n",
    "        #     nn.Linear(self.projection_intermediate_dim, self.projection_dim, bias=True),\n",
    "        # )\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * self.config.logit_scale_init_value)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    # @add_start_docstrings_to_model_forward(GROUPVIT_TEXT_INPUTS_DOCSTRING)\n",
    "    # def get_text_features(\n",
    "    #     self,\n",
    "    #     input_ids: Optional[torch.Tensor] = None,\n",
    "    #     attention_mask: Optional[torch.Tensor] = None,\n",
    "    #     position_ids: Optional[torch.Tensor] = None,\n",
    "    #     output_attentions: Optional[bool] = None,\n",
    "    #     output_hidden_states: Optional[bool] = None,\n",
    "    #     return_dict: Optional[bool] = None,\n",
    "    # ) -> torch.FloatTensor:\n",
    "    #     r\"\"\"\n",
    "    #     Returns:\n",
    "    #         text_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The text embeddings obtained by\n",
    "    #         applying the projection layer to the pooled output of [`GroupViTTextModel`].\n",
    "\n",
    "    #     Examples:\n",
    "\n",
    "    #     ```python\n",
    "    #     >>> from transformers import CLIPTokenizer, GroupViTModel\n",
    "\n",
    "    #     >>> model = GroupViTModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "    #     >>> tokenizer = CLIPTokenizer.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "\n",
    "    #     >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n",
    "    #     >>> text_features = model.get_text_features(**inputs)\n",
    "    #     ```\"\"\"\n",
    "    #     # Use GROUPVIT model's config for some fields (if specified) instead of those of vision & text components.\n",
    "    #     output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "    #     output_hidden_states = (\n",
    "    #         output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "    #     )\n",
    "    #     return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "    #     text_outputs = self.text_model(\n",
    "    #         input_ids=input_ids,\n",
    "    #         attention_mask=attention_mask,\n",
    "    #         position_ids=position_ids,\n",
    "    #         output_attentions=output_attentions,\n",
    "    #         output_hidden_states=output_hidden_states,\n",
    "    #         return_dict=return_dict,\n",
    "    #     )\n",
    "\n",
    "    #     pooled_output = text_outputs[1]\n",
    "    #     text_features = self.text_projection(pooled_output)\n",
    "\n",
    "    #     return text_features\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(GROUPVIT_VISION_INPUTS_DOCSTRING)\n",
    "    def get_image_features(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> torch.FloatTensor:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "            image_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The image embeddings obtained by\n",
    "            applying the projection layer to the pooled output of [`GroupViTVisionModel`].\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from PIL import Image\n",
    "        >>> import requests\n",
    "        >>> from transformers import AutoProcessor, GroupViTModel\n",
    "\n",
    "        >>> model = GroupViTModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "        >>> processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "\n",
    "        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "        >>> image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "        >>> inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "        >>> image_features = model.get_image_features(**inputs)\n",
    "        ```\"\"\"\n",
    "        # Use GROUPVIT model's config for some fields (if specified) instead of those of vision & text components.\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        print(pixel_values.shape, output_attentions, output_hidden_states, return_dict)\n",
    "\n",
    "        vision_outputs = self.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = vision_outputs[1]  # pooled_output\n",
    "        print('01 ', pooled_output.shape)\n",
    "\n",
    "        image_features = self.visual_projection(pooled_output)\n",
    "        print('02 ', image_features.shape)\n",
    "\n",
    "        return image_features\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(GROUPVIT_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=GroupViTModelOutput, config_class=GroupViTConfig)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        return_loss: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_segmentation: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, GroupViTModelOutput]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from PIL import Image\n",
    "        >>> import requests\n",
    "        >>> from transformers import AutoProcessor, GroupViTModel\n",
    "\n",
    "        >>> model = GroupViTModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "        >>> processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "\n",
    "        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "        >>> image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "        >>> inputs = processor(\n",
    "        ...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True\n",
    "        ... )\n",
    "\n",
    "        >>> outputs = model(**inputs)\n",
    "        >>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "        >>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n",
    "        ```\"\"\"\n",
    "        # Use GROUPVIT model's config for some fields (if specified) instead of those of vision & text components.\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_segmentation = (\n",
    "            output_segmentation if output_segmentation is not None else self.config.output_segmentation\n",
    "        )\n",
    "        if output_segmentation:\n",
    "            output_attentions = True\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # print(type(pixel_values), type(output_attentions), type(output_hidden_states), type(return_dict))\n",
    "        print(pixel_values.shape, output_attentions, output_hidden_states, return_dict)\n",
    "\n",
    "        vision_outputs = self.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        # print(vision_outputs)\n",
    "\n",
    "        # text_outputs = self.text_model(\n",
    "        #     input_ids=input_ids,\n",
    "        #     attention_mask=attention_mask,\n",
    "        #     position_ids=position_ids,\n",
    "        #     output_attentions=output_attentions,\n",
    "        #     output_hidden_states=output_hidden_states,\n",
    "        #     return_dict=return_dict,\n",
    "        # )\n",
    "\n",
    "        image_embeds = vision_outputs[1]\n",
    "        image_embeds = self.visual_projection(image_embeds)\n",
    "\n",
    "        # text_embeds = text_outputs[1]\n",
    "        # text_embeds = self.text_projection(text_embeds)\n",
    "\n",
    "        # normalized features\n",
    "        image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n",
    "        # text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        # logit_scale = self.logit_scale.exp()\n",
    "        # logits_per_text = torch.matmul(text_embeds, image_embeds.t()) * logit_scale\n",
    "        # logits_per_image = logits_per_text.t()\n",
    "\n",
    "        seg_logits = None\n",
    "        if output_segmentation:\n",
    "            # grouped features\n",
    "            # [batch_size_image, num_group, hidden_size]\n",
    "            image_group_embeds = vision_outputs[0]\n",
    "            print('image_group_embeds_01', image_group_embeds.shape, type(image_group_embeds)) # torch.Size([1, 8, 384]) <class 'torch.Tensor'>\n",
    "\n",
    "            # [batch_size_image*num_group, hidden_size]\n",
    "            image_group_embeds = self.visual_projection(image_group_embeds.reshape(-1, image_group_embeds.shape[-1]))\n",
    "            print('image_group_embeds_02', image_group_embeds.shape, type(image_group_embeds)) # torch.Size([8, 256]) <class 'torch.Tensor'>\n",
    "\n",
    "            if output_hidden_states:\n",
    "                attentions = vision_outputs[3]\n",
    "                print('attentions_01', attentions.shape, type(attentions)) # *\n",
    "\n",
    "            else:\n",
    "                attentions = vision_outputs[2]\n",
    "                print('attentions_02', attentions[0].shape, type(attentions[0]), attentions[1].shape, type(attentions[1])) # torch.Size([1, 64, 196]) torch.Size([1, 8, 64]) <class 'torch.Tensor'>\n",
    "                \n",
    "            # [batch_size_image, num_group, height, width]\n",
    "            grouping = get_grouping_from_attentions(attentions, pixel_values.shape[2:])\n",
    "            print(pixel_values.shape)\n",
    "            print(pixel_values.shape[2:])\n",
    "            print('grouping_01', grouping.shape, type(grouping)) # torch.Size([1, 8, 224, 224]) <class 'torch.Tensor'>\n",
    "            seg_logits = grouping\n",
    "\n",
    "            # # normalized features\n",
    "            # image_group_embeds = image_group_embeds / image_group_embeds.norm(dim=-1, keepdim=True)\n",
    "            # print('image_group_embeds_03', image_group_embeds.shape, type(image_group_embeds)) # torch.Size([8, 256]) <class 'torch.Tensor'>\n",
    "\n",
    "            # # [batch_size_image x num_group, batch_size_text]\n",
    "            # logits_per_image_group = torch.matmul(image_group_embeds, text_embeds.t()) * logit_scale\n",
    "            # print('logits_per_image_group_01', logits_per_image_group.shape, type(logits_per_image_group)) # torch.Size([8, 3]) <class 'torch.Tensor'>\n",
    "\n",
    "            # # [batch_size_image, batch_size_text, num_group]\n",
    "            # logits_per_image_group = logits_per_image_group.reshape(\n",
    "            #     image_embeds.shape[0], -1, text_embeds.shape[0]\n",
    "            # ).permute(0, 2, 1)\n",
    "            # print('logits_per_image_group_02', logits_per_image_group.shape, type(logits_per_image_group)) # torch.Size([1, 3, 8]) <class 'torch.Tensor'>\n",
    "\n",
    "\n",
    "            # # [batch_size_image, batch_size_text, height x width]\n",
    "            # flatten_grouping = grouping.reshape(grouping.shape[0], grouping.shape[1], -1)\n",
    "            # print('flatten_grouping_01', flatten_grouping.shape, type(flatten_grouping)) # torch.Size([1, 8, 50176]) <class 'torch.Tensor'>\n",
    "\n",
    "\n",
    "            # # [batch_size_image, batch_size_text, height, width]\n",
    "            # seg_logits = torch.matmul(logits_per_image_group, flatten_grouping) * logit_scale\n",
    "            # print('seg_logits_01', seg_logits.shape, type(seg_logits)) # torch.Size([1, 3, 50176]) <class 'torch.Tensor'>\n",
    "\n",
    "            # seg_logits = seg_logits.reshape(\n",
    "            #     seg_logits.shape[0], seg_logits.shape[1], grouping.shape[2], grouping.shape[3]\n",
    "            # )\n",
    "            # print('seg_logits_02', seg_logits.shape, type(seg_logits)) # torch.Size([1, 3, 224, 224]) <class 'torch.Tensor'>\n",
    "\n",
    "        loss = None\n",
    "        if return_loss:\n",
    "            loss = groupvit_loss(logits_per_text)\n",
    "\n",
    "        if not return_dict:\n",
    "            if seg_logits is not None:\n",
    "                output = (\n",
    "                    logits_per_image,\n",
    "                    logits_per_text,\n",
    "                    seg_logits,\n",
    "                    text_embeds,\n",
    "                    image_embeds,\n",
    "                    text_outputs,\n",
    "                    vision_outputs,\n",
    "                )\n",
    "            else:\n",
    "                output = (logits_per_image, logits_per_text, text_embeds, image_embeds, text_outputs, vision_outputs)\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return GroupViTModelOutput(\n",
    "            loss=loss,\n",
    "            # logits_per_image=logits_per_image,\n",
    "            # logits_per_text=logits_per_text,\n",
    "            segmentation_logits=seg_logits,\n",
    "            # text_embeds=text_embeds,\n",
    "            image_embeds=image_embeds,\n",
    "            # text_model_output=text_outputs,\n",
    "            vision_model_output=vision_outputs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuwerq7N9s5S"
   },
   "source": [
    "### Set Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1677512218962,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "dvuxcmejkKt8",
    "outputId": "4e6311d9-6fd0-43ed-c40a-e5541e35f105"
   },
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description='Train MoCo on CIFAR-10')\n",
    "\n",
    "# parser.add_argument('--finetune_data_dir', default=os.path.join(ROOT_PATH, '../lab_06', 'fine-tune_set', 'siim-acr-pneumothorax'))\n",
    "# parser.add_argument('--pretrain_data_dir', default=os.path.join(ROOT_PATH, '../lab_05', 'unlabel_pre-training_set'))\n",
    "\n",
    "# parser.add_argument('--image_size', default=256, type=int)\n",
    "\n",
    "# parser.add_argument('--lr', '--learning-rate', default=0.06, type=float, metavar='LR', help='initial learning rate', dest='lr')\n",
    "# parser.add_argument('--epochs', default=200, type=int, metavar='N', help='number of total epochs to run')\n",
    "# parser.add_argument('--batch-size', default=4, type=int, metavar='N', help='mini-batch size')\n",
    "\n",
    "# '''\n",
    "# args = parser.parse_args()  # running in command line\n",
    "# '''\n",
    "# args = parser.parse_args('')  # running in ipynb\n",
    "\n",
    "\n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygQeHtsngrC8"
   },
   "source": [
    "### Dataset Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_length_decode(rle, height=1024, width=1024, fill_value=1):\n",
    "    component = np.zeros((height, width), np.float32)\n",
    "    component = component.reshape(-1)\n",
    "    rle = np.array([int(s) for s in rle.strip().split(' ')])\n",
    "    rle = rle.reshape(-1, 2)\n",
    "    start = 0\n",
    "    for index, length in rle:\n",
    "        start = start+index\n",
    "        end = start+length\n",
    "        component[start: end] = fill_value\n",
    "        start = end\n",
    "    component = component.reshape(width, height).T\n",
    "    return component\n",
    "\n",
    "def run_length_encode(component):\n",
    "    component = component.T.flatten()\n",
    "    start = np.where(component[1:] > component[:-1])[0]+1\n",
    "    end = np.where(component[:-1] > component[1:])[0]+1\n",
    "    length = end-start\n",
    "    rle = []\n",
    "    for i in range(len(length)):\n",
    "        if i == 0:\n",
    "            rle.extend([start[0], length[0]])\n",
    "        else:\n",
    "            rle.extend([start[i]-end[i-1], length[i]])\n",
    "    rle = ' '.join([str(r) for r in rle])\n",
    "    return rle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(phase, size, mean, std):\n",
    "    list_transforms = []\n",
    "    if phase == \"train\":\n",
    "        list_transforms.extend(\n",
    "            [\n",
    "#                 HorizontalFlip(),\n",
    "                ShiftScaleRotate(\n",
    "                    shift_limit=0,  # no resizing\n",
    "                    scale_limit=0.1,\n",
    "                    rotate_limit=10, # rotate\n",
    "                    p=0.5,\n",
    "                    border_mode=cv2.BORDER_CONSTANT\n",
    "                ),\n",
    "#                 GaussNoise(),\n",
    "            ]\n",
    "        )\n",
    "    list_transforms.extend(\n",
    "        [\n",
    "            Resize(size, size),\n",
    "            Normalize(mean=mean, std=std, p=1),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    list_trfms = Compose(list_transforms)\n",
    "    return list_trfms\n",
    "\n",
    "def provider(\n",
    "    fold,\n",
    "    total_folds,\n",
    "    data_folder,\n",
    "    df_path,\n",
    "    phase,\n",
    "    size,\n",
    "    mean=None,\n",
    "    std=None,\n",
    "    batch_size=8,\n",
    "    num_workers=4,\n",
    "):\n",
    "    df_all = pd.read_csv(df_path)\n",
    "    df = df_all.drop_duplicates('ImageId')\n",
    "    df_with_mask = df[df[\" EncodedPixels\"] != \" -1\"]\n",
    "    df_with_mask['has_mask'] = 1\n",
    "    df_without_mask = df[df[\" EncodedPixels\"] == \" -1\"]\n",
    "    df_without_mask['has_mask'] = 0\n",
    "    df_without_mask_sampled = df_without_mask.sample(len(df_with_mask), random_state=69) # random state is imp\n",
    "    df = pd.concat([df_with_mask, df_without_mask_sampled])\n",
    "    \n",
    "    #NOTE: equal number of positive and negative cases are chosen.\n",
    "    \n",
    "    kfold = StratifiedKFold(total_folds, shuffle=True, random_state=69)\n",
    "    train_idx, val_idx = list(kfold.split(df[\"ImageId\"], df[\"has_mask\"]))[fold]\n",
    "    train_df, val_df = df.iloc[train_idx], df.iloc[val_idx]\n",
    "    df = train_df if phase == \"train\" else val_df\n",
    "    # NOTE: total_folds=5 -> train/val : 80%/20%\n",
    "    \n",
    "    fnames = df['ImageId'].values\n",
    "    \n",
    "    image_dataset = SIIMDataset(df_all, fnames, data_folder, size, mean, std, phase)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        image_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,\n",
    "        generator=torch.Generator(device='cuda')\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get from path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader = provider(\n",
    "#     fold=0,\n",
    "#     total_folds=5,\n",
    "#     data_folder=data_folder,\n",
    "#     df_path=train_rle_path,\n",
    "#     phase=\"train\",\n",
    "#     size=512,\n",
    "#     mean = (0.485, 0.456, 0.406),\n",
    "#     std = (0.229, 0.224, 0.225),\n",
    "#     batch_size=16,\n",
    "#     num_workers=4,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(dataloader)) # get a batch from the dataloader\n",
    "# images, masks = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot some random images in the `batch`\n",
    "# idx = random.choice(range(16))\n",
    "# plt.imshow(np.asarray(images[idx]).transpose(1, 2, 0))\n",
    "# plt.imshow(masks[idx][0], alpha=0.2, cmap='Reds')\n",
    "# plt.show()\n",
    "# if len(np.unique(masks[idx][0])) == 1: # only zeros\n",
    "#     print('Chosen image has no ground truth mask, rerun the cell')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set dataset & loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SIIMDataset(Dataset):\n",
    "    def __init__(self, df, fnames, data_folder, size, mean, std, phase):\n",
    "        self.df = df\n",
    "        self.root = data_folder\n",
    "        self.size = size\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.phase = phase\n",
    "        self.transforms = get_transforms(phase, size, mean, std)\n",
    "        self.gb = self.df.groupby('ImageId')\n",
    "        self.fnames = fnames\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.fnames[idx]\n",
    "        df = self.gb.get_group(image_id)\n",
    "        annotations = df[' EncodedPixels'].tolist()\n",
    "        image_path = os.path.join(self.root, image_id + \".png\")\n",
    "        image = cv2.imread(image_path)\n",
    "        mask = np.zeros([1024, 1024])\n",
    "        if annotations[0] != ' -1':\n",
    "            for rle in annotations:\n",
    "                mask += run_length_decode(rle)\n",
    "        mask = (mask >= 1).astype('float32') # for overlap cases\n",
    "        augmented = self.transforms(image=image, mask=mask)\n",
    "        image = augmented['image']\n",
    "        mask = augmented['mask']\n",
    "        return image, mask.unsqueeze(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsAVAtRoiBbG"
   },
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define base encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(\"resnet34\", encoder_weights=\"imagenet\", activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ModelBase(configuration=configuration)\n",
    "\n",
    "# t = torch.randn((32,3,512,512))\n",
    "# print(t.shape)\n",
    "# print(model(t).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define MoCo wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YXcpXBwi8KV"
   },
   "source": [
    "### Define train/test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(input, target):\n",
    "    input = torch.sigmoid(input)\n",
    "    smooth = 1.0\n",
    "    iflat = input.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    return ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if not (target.size() == input.size()):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n",
    "                             .format(target.size(), input.size()))\n",
    "        max_val = (-input).clamp(min=0)\n",
    "        loss = input - input * target + max_val + \\\n",
    "            ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class MixedLoss(nn.Module):\n",
    "    def __init__(self, alpha, gamma):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.focal = FocalLoss(gamma)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        loss = self.alpha*self.focal(input, target) - torch.log(dice_loss(input, target))\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define train & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, threshold):\n",
    "    X_p = np.copy(X)\n",
    "    preds = (X_p > threshold).astype('uint8')\n",
    "    return preds\n",
    "\n",
    "def metric(probability, truth, threshold=0.5, reduction='none'):\n",
    "    '''Calculates dice of positive and negative images seperately'''\n",
    "    '''probability and truth must be torch tensors'''\n",
    "    batch_size = len(truth)\n",
    "    with torch.no_grad():\n",
    "        probability = probability.view(batch_size, -1)\n",
    "        truth = truth.view(batch_size, -1)\n",
    "        assert(probability.shape == truth.shape)\n",
    "\n",
    "        p = (probability > threshold).float()\n",
    "        t = (truth > 0.5).float()\n",
    "\n",
    "        t_sum = t.sum(-1)\n",
    "        p_sum = p.sum(-1)\n",
    "        neg_index = torch.nonzero(t_sum == 0)\n",
    "        pos_index = torch.nonzero(t_sum >= 1)\n",
    "\n",
    "        dice_neg = (p_sum == 0).float()\n",
    "        dice_pos = 2 * (p*t).sum(-1)/((p+t).sum(-1))\n",
    "\n",
    "        dice_neg = dice_neg[neg_index]\n",
    "        dice_pos = dice_pos[pos_index]\n",
    "        dice = torch.cat([dice_pos, dice_neg])\n",
    "\n",
    "#         dice_neg = np.nan_to_num(dice_neg.mean().item(), 0)\n",
    "#         dice_pos = np.nan_to_num(dice_pos.mean().item(), 0)\n",
    "#         dice = dice.mean().item()\n",
    "\n",
    "        num_neg = len(neg_index)\n",
    "        num_pos = len(pos_index)\n",
    "\n",
    "    return dice, dice_neg, dice_pos, num_neg, num_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Meter:\n",
    "    '''A meter to keep track of iou and dice scores throughout an epoch'''\n",
    "    def __init__(self, phase, epoch):\n",
    "        self.base_threshold = 0.5 # <<<<<<<<<<< here's the threshold\n",
    "        self.base_dice_scores = []\n",
    "        self.dice_neg_scores = []\n",
    "        self.dice_pos_scores = []\n",
    "        self.iou_scores = []\n",
    "\n",
    "    def update(self, targets, outputs):\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        dice, dice_neg, dice_pos, _, _ = metric(probs, targets, self.base_threshold)\n",
    "        self.base_dice_scores.extend(dice)\n",
    "        self.dice_pos_scores.extend(dice_pos)\n",
    "        self.dice_neg_scores.extend(dice_neg)\n",
    "        preds = predict(probs, self.base_threshold)\n",
    "        iou = compute_iou_batch(preds, targets, classes=[1])\n",
    "        self.iou_scores.append(iou)\n",
    "\n",
    "    def get_metrics(self):\n",
    "        dice = np.nanmean(self.base_dice_scores)\n",
    "        dice_neg = np.nanmean(self.dice_neg_scores)\n",
    "        dice_pos = np.nanmean(self.dice_pos_scores)\n",
    "        dices = [dice, dice_neg, dice_pos]\n",
    "        iou = np.nanmean(self.iou_scores)\n",
    "        return dices, iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_log(phase, epoch, epoch_loss, meter, start):\n",
    "    '''logging the metrics at the end of an epoch'''\n",
    "    dices, iou = meter.get_metrics()\n",
    "    dice, dice_neg, dice_pos = dices\n",
    "    print(\"Loss: %0.4f | dice: %0.4f | dice_neg: %0.4f | dice_pos: %0.4f | IoU: %0.4f\" % (epoch_loss, dice, dice_neg, dice_pos, iou))\n",
    "    return dice, iou\n",
    "\n",
    "def compute_ious(pred, label, classes, ignore_index=255, only_present=True):\n",
    "    '''computes iou for one ground truth mask and predicted mask'''\n",
    "    pred[label == ignore_index] = 0\n",
    "    ious = []\n",
    "    for c in classes:\n",
    "        label_c = label == c\n",
    "        if only_present and np.sum(label_c) == 0:\n",
    "            ious.append(np.nan)\n",
    "            continue\n",
    "        pred_c = pred == c\n",
    "        intersection = np.logical_and(pred_c, label_c).sum()\n",
    "        union = np.logical_or(pred_c, label_c).sum()\n",
    "        if union != 0:\n",
    "            ious.append(intersection / union)\n",
    "    return ious if ious else [1]\n",
    "\n",
    "\n",
    "def compute_iou_batch(outputs, labels, classes=None):\n",
    "    '''computes mean iou for a batch of ground truth masks and predicted masks'''\n",
    "    ious = []\n",
    "    preds = np.copy(outputs) # copy is imp\n",
    "    labels = np.array(labels) # tensor to np\n",
    "    for pred, label in zip(preds, labels):\n",
    "        ious.append(np.nanmean(compute_ious(pred, label, classes)))\n",
    "    iou = np.nanmean(ious)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    '''This class takes care of training and validation of our model'''\n",
    "    def __init__(self, model):\n",
    "        self.fold = 1\n",
    "        self.total_folds = 5\n",
    "        self.num_workers = 6\n",
    "        self.batch_size = {\"train\": 8, \"val\": 4}\n",
    "        self.accumulation_steps = 32 // self.batch_size['train']\n",
    "        self.lr = 5e-4\n",
    "        self.num_epochs = 40\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.phases = [\"train\", \"val\"]\n",
    "        self.device = torch.device(\"cuda:0\")\n",
    "        torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "        self.net = model\n",
    "        self.criterion = MixedLoss(10.0, 2.0)\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode=\"min\", patience=3, verbose=True)\n",
    "        self.net = self.net.to(self.device)\n",
    "        cudnn.benchmark = True\n",
    "        self.dataloaders = {\n",
    "            phase: provider(\n",
    "                fold=1,\n",
    "                total_folds=5,\n",
    "                data_folder=data_folder,\n",
    "                df_path=train_rle_path,\n",
    "                phase=phase,\n",
    "                size=1024,\n",
    "                mean=(0.485, 0.456, 0.406),\n",
    "                std=(0.229, 0.224, 0.225),\n",
    "                batch_size=self.batch_size[phase],\n",
    "                num_workers=self.num_workers,\n",
    "            )\n",
    "            for phase in self.phases\n",
    "        }\n",
    "        self.losses = {phase: [] for phase in self.phases}\n",
    "        self.iou_scores = {phase: [] for phase in self.phases}\n",
    "        self.dice_scores = {phase: [] for phase in self.phases}\n",
    "        \n",
    "    def forward(self, images, targets):\n",
    "        images = images.to(self.device)\n",
    "        masks = targets.to(self.device)\n",
    "        outputs = self.net(images)\n",
    "        loss = self.criterion(outputs, masks)\n",
    "        return loss, outputs\n",
    "\n",
    "    def iterate(self, epoch, phase):\n",
    "        meter = Meter(phase, epoch)\n",
    "        start = time.strftime(\"%H:%M:%S\")\n",
    "        print(f\"Starting epoch: {epoch} | phase: {phase} | ⏰: {start}\")\n",
    "        batch_size = self.batch_size[phase]\n",
    "        self.net.train(phase == \"train\")\n",
    "        dataloader = self.dataloaders[phase]\n",
    "        running_loss = 0.0\n",
    "        total_batches = len(dataloader)\n",
    "#         tk0 = tqdm(dataloader, total=total_batches)\n",
    "        self.optimizer.zero_grad()\n",
    "        for itr, batch in enumerate(dataloader):\n",
    "            images, targets = batch\n",
    "            loss, outputs = self.forward(images, targets)\n",
    "            loss = loss / self.accumulation_steps\n",
    "            if phase == \"train\":\n",
    "                loss.backward()\n",
    "                if (itr + 1 ) % self.accumulation_steps == 0:\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "            running_loss += loss.item()\n",
    "            outputs = outputs.detach().cpu()\n",
    "            meter.update(targets, outputs)\n",
    "#             tk0.set_postfix(loss=(running_loss / ((itr + 1))))\n",
    "        epoch_loss = (running_loss * self.accumulation_steps) / total_batches\n",
    "        dice, iou = epoch_log(phase, epoch, epoch_loss, meter, start)\n",
    "        self.losses[phase].append(epoch_loss)\n",
    "        self.dice_scores[phase].append(dice)\n",
    "        self.iou_scores[phase].append(iou)\n",
    "        torch.cuda.empty_cache()\n",
    "        return epoch_loss\n",
    "\n",
    "    def start(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.iterate(epoch, \"train\")\n",
    "            state = {\n",
    "                \"epoch\": epoch,\n",
    "                \"best_loss\": self.best_loss,\n",
    "                \"state_dict\": self.net.state_dict(),\n",
    "                \"optimizer\": self.optimizer.state_dict(),\n",
    "            }\n",
    "            val_loss = self.iterate(epoch, \"val\")\n",
    "            self.scheduler.step(val_loss)\n",
    "            if val_loss < self.best_loss:\n",
    "                print(\"******** New optimal found, saving state ********\")\n",
    "                state[\"best_loss\"] = self.best_loss = val_loss\n",
    "                torch.save(state, model_DIR)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86lHkiKox3KO"
   },
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0 | phase: train | ⏰: 01:30:55\n",
      "Loss: 4.2087 | dice: 0.1510 | dice_neg: 0.0368 | dice_pos: 0.2652 | IoU: 0.1729\n",
      "Starting epoch: 0 | phase: val | ⏰: 01:39:59\n",
      "Loss: 3.8428 | dice: 0.1879 | dice_neg: 0.0735 | dice_pos: 0.3023 | IoU: 0.1974\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 1 | phase: train | ⏰: 01:41:08\n",
      "Loss: 2.5389 | dice: 0.2549 | dice_neg: 0.1524 | dice_pos: 0.3575 | IoU: 0.2416\n",
      "Starting epoch: 1 | phase: val | ⏰: 01:49:58\n",
      "Loss: 2.7111 | dice: 0.3418 | dice_neg: 0.3277 | dice_pos: 0.3558 | IoU: 0.2586\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 2 | phase: train | ⏰: 01:51:02\n",
      "Loss: 1.7938 | dice: 0.3241 | dice_neg: 0.2848 | dice_pos: 0.3634 | IoU: 0.2578\n",
      "Starting epoch: 2 | phase: val | ⏰: 01:59:36\n",
      "Loss: 2.3885 | dice: 0.4289 | dice_neg: 0.5063 | dice_pos: 0.3516 | IoU: 0.2438\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 3 | phase: train | ⏰: 02:00:39\n",
      "Loss: 1.5084 | dice: 0.3929 | dice_neg: 0.4172 | dice_pos: 0.3686 | IoU: 0.2643\n",
      "Starting epoch: 3 | phase: val | ⏰: 02:09:18\n",
      "Loss: 2.0733 | dice: 0.3893 | dice_neg: 0.4013 | dice_pos: 0.3774 | IoU: 0.2696\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 4 | phase: train | ⏰: 02:10:21\n",
      "Loss: 1.2197 | dice: 0.4898 | dice_neg: 0.5696 | dice_pos: 0.4100 | IoU: 0.3083\n",
      "Starting epoch: 4 | phase: val | ⏰: 02:19:02\n",
      "Loss: 2.2541 | dice: 0.5945 | dice_neg: 0.8361 | dice_pos: 0.3529 | IoU: 0.2647\n",
      "\n",
      "Starting epoch: 5 | phase: train | ⏰: 02:20:03\n",
      "Loss: 1.2105 | dice: 0.5085 | dice_neg: 0.6122 | dice_pos: 0.4047 | IoU: 0.3029\n",
      "Starting epoch: 5 | phase: val | ⏰: 02:28:45\n",
      "Loss: 2.0743 | dice: 0.5694 | dice_neg: 0.7836 | dice_pos: 0.3551 | IoU: 0.2609\n",
      "\n",
      "Starting epoch: 6 | phase: train | ⏰: 02:29:48\n",
      "Loss: 1.3726 | dice: 0.3966 | dice_neg: 0.4057 | dice_pos: 0.3876 | IoU: 0.2827\n",
      "Starting epoch: 6 | phase: val | ⏰: 02:38:23\n",
      "Loss: 1.9503 | dice: 0.5253 | dice_neg: 0.6387 | dice_pos: 0.4120 | IoU: 0.3029\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 7 | phase: train | ⏰: 02:39:26\n",
      "Loss: 1.2058 | dice: 0.4902 | dice_neg: 0.5539 | dice_pos: 0.4265 | IoU: 0.3188\n",
      "Starting epoch: 7 | phase: val | ⏰: 02:48:14\n",
      "Loss: 1.7422 | dice: 0.6076 | dice_neg: 0.8004 | dice_pos: 0.4147 | IoU: 0.3114\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 8 | phase: train | ⏰: 02:49:17\n",
      "Loss: 0.9669 | dice: 0.5753 | dice_neg: 0.7047 | dice_pos: 0.4460 | IoU: 0.3445\n",
      "Starting epoch: 8 | phase: val | ⏰: 02:57:54\n",
      "Loss: 2.0167 | dice: 0.6324 | dice_neg: 0.8676 | dice_pos: 0.3971 | IoU: 0.2949\n",
      "\n",
      "Starting epoch: 9 | phase: train | ⏰: 02:58:55\n",
      "Loss: 1.0566 | dice: 0.5338 | dice_neg: 0.6400 | dice_pos: 0.4275 | IoU: 0.3262\n",
      "Starting epoch: 9 | phase: val | ⏰: 03:07:42\n",
      "Loss: 2.0176 | dice: 0.6455 | dice_neg: 0.9244 | dice_pos: 0.3667 | IoU: 0.2728\n",
      "\n",
      "Starting epoch: 10 | phase: train | ⏰: 03:08:46\n",
      "Loss: 1.1998 | dice: 0.4745 | dice_neg: 0.5302 | dice_pos: 0.4188 | IoU: 0.3122\n",
      "Starting epoch: 10 | phase: val | ⏰: 03:17:36\n",
      "Loss: 1.9157 | dice: 0.5560 | dice_neg: 0.7122 | dice_pos: 0.3999 | IoU: 0.2898\n",
      "\n",
      "Starting epoch: 11 | phase: train | ⏰: 03:18:39\n",
      "Loss: 1.0569 | dice: 0.5387 | dice_neg: 0.6301 | dice_pos: 0.4474 | IoU: 0.3400\n",
      "Starting epoch: 11 | phase: val | ⏰: 03:27:27\n",
      "Loss: 2.2388 | dice: 0.3829 | dice_neg: 0.3845 | dice_pos: 0.3814 | IoU: 0.2656\n",
      "Epoch    12: reducing learning rate of group 0 to 5.0000e-05.\n",
      "\n",
      "Starting epoch: 12 | phase: train | ⏰: 03:28:28\n",
      "Loss: 0.9507 | dice: 0.5891 | dice_neg: 0.7031 | dice_pos: 0.4751 | IoU: 0.3673\n",
      "Starting epoch: 12 | phase: val | ⏰: 03:37:13\n",
      "Loss: 1.6833 | dice: 0.6431 | dice_neg: 0.8445 | dice_pos: 0.4416 | IoU: 0.3309\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 13 | phase: train | ⏰: 03:38:16\n",
      "Loss: 0.8217 | dice: 0.6504 | dice_neg: 0.8008 | dice_pos: 0.5000 | IoU: 0.3920\n",
      "Starting epoch: 13 | phase: val | ⏰: 03:47:04\n",
      "Loss: 1.5718 | dice: 0.6375 | dice_neg: 0.7962 | dice_pos: 0.4787 | IoU: 0.3672\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 14 | phase: train | ⏰: 03:48:05\n",
      "Loss: 0.8075 | dice: 0.6610 | dice_neg: 0.8150 | dice_pos: 0.5069 | IoU: 0.3967\n",
      "Starting epoch: 14 | phase: val | ⏰: 03:56:32\n",
      "Loss: 1.6327 | dice: 0.6397 | dice_neg: 0.7878 | dice_pos: 0.4916 | IoU: 0.3838\n",
      "\n",
      "Starting epoch: 15 | phase: train | ⏰: 03:57:33\n",
      "Loss: 0.7425 | dice: 0.6690 | dice_neg: 0.8135 | dice_pos: 0.5245 | IoU: 0.4127\n",
      "Starting epoch: 15 | phase: val | ⏰: 04:06:15\n",
      "Loss: 1.6048 | dice: 0.6535 | dice_neg: 0.8256 | dice_pos: 0.4813 | IoU: 0.3569\n",
      "\n",
      "Starting epoch: 16 | phase: train | ⏰: 04:07:19\n",
      "Loss: 0.7702 | dice: 0.6899 | dice_neg: 0.8523 | dice_pos: 0.5275 | IoU: 0.4170\n",
      "Starting epoch: 16 | phase: val | ⏰: 04:16:06\n",
      "Loss: 1.3836 | dice: 0.6664 | dice_neg: 0.8487 | dice_pos: 0.4840 | IoU: 0.3835\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 17 | phase: train | ⏰: 04:17:06\n",
      "Loss: 0.8083 | dice: 0.6822 | dice_neg: 0.8334 | dice_pos: 0.5310 | IoU: 0.4153\n",
      "Starting epoch: 17 | phase: val | ⏰: 04:25:39\n",
      "Loss: 1.5263 | dice: 0.6481 | dice_neg: 0.8235 | dice_pos: 0.4726 | IoU: 0.3700\n",
      "\n",
      "Starting epoch: 18 | phase: train | ⏰: 04:26:38\n",
      "Loss: 0.7525 | dice: 0.6861 | dice_neg: 0.8308 | dice_pos: 0.5414 | IoU: 0.4298\n",
      "Starting epoch: 18 | phase: val | ⏰: 04:35:13\n",
      "Loss: 1.5863 | dice: 0.6612 | dice_neg: 0.8403 | dice_pos: 0.4820 | IoU: 0.3686\n",
      "\n",
      "Starting epoch: 19 | phase: train | ⏰: 04:36:17\n",
      "Loss: 0.7691 | dice: 0.7042 | dice_neg: 0.8628 | dice_pos: 0.5456 | IoU: 0.4284\n",
      "Starting epoch: 19 | phase: val | ⏰: 04:45:03\n",
      "Loss: 1.6736 | dice: 0.6730 | dice_neg: 0.8782 | dice_pos: 0.4678 | IoU: 0.3580\n",
      "\n",
      "Starting epoch: 20 | phase: train | ⏰: 04:46:05\n",
      "Loss: 0.7023 | dice: 0.7008 | dice_neg: 0.8586 | dice_pos: 0.5430 | IoU: 0.4310\n",
      "Starting epoch: 20 | phase: val | ⏰: 04:54:43\n",
      "Loss: 1.4032 | dice: 0.6720 | dice_neg: 0.8697 | dice_pos: 0.4742 | IoU: 0.3834\n",
      "Epoch    21: reducing learning rate of group 0 to 5.0000e-06.\n",
      "\n",
      "Starting epoch: 21 | phase: train | ⏰: 04:55:46\n",
      "Loss: 0.6700 | dice: 0.7127 | dice_neg: 0.8655 | dice_pos: 0.5599 | IoU: 0.4410\n",
      "Starting epoch: 21 | phase: val | ⏰: 05:04:26\n",
      "Loss: 1.4069 | dice: 0.6755 | dice_neg: 0.8697 | dice_pos: 0.4812 | IoU: 0.3659\n",
      "\n",
      "Starting epoch: 22 | phase: train | ⏰: 05:05:28\n",
      "Loss: 0.6936 | dice: 0.7164 | dice_neg: 0.8776 | dice_pos: 0.5553 | IoU: 0.4439\n",
      "Starting epoch: 22 | phase: val | ⏰: 05:14:16\n",
      "Loss: 1.6441 | dice: 0.6738 | dice_neg: 0.8655 | dice_pos: 0.4820 | IoU: 0.3644\n",
      "\n",
      "Starting epoch: 23 | phase: train | ⏰: 05:15:17\n",
      "Loss: 0.6751 | dice: 0.7104 | dice_neg: 0.8592 | dice_pos: 0.5615 | IoU: 0.4466\n",
      "Starting epoch: 23 | phase: val | ⏰: 05:24:01\n",
      "Loss: 1.6014 | dice: 0.6761 | dice_neg: 0.8782 | dice_pos: 0.4740 | IoU: 0.3697\n",
      "\n",
      "Starting epoch: 24 | phase: train | ⏰: 05:25:00\n",
      "Loss: 0.6752 | dice: 0.7172 | dice_neg: 0.8781 | dice_pos: 0.5564 | IoU: 0.4381\n",
      "Starting epoch: 24 | phase: val | ⏰: 05:33:50\n",
      "Loss: 1.4281 | dice: 0.6750 | dice_neg: 0.8634 | dice_pos: 0.4865 | IoU: 0.3808\n",
      "Epoch    25: reducing learning rate of group 0 to 5.0000e-07.\n",
      "\n",
      "Starting epoch: 25 | phase: train | ⏰: 05:34:53\n",
      "Loss: 0.7014 | dice: 0.7175 | dice_neg: 0.8755 | dice_pos: 0.5596 | IoU: 0.4447\n",
      "Starting epoch: 25 | phase: val | ⏰: 05:43:33\n",
      "Loss: 1.5387 | dice: 0.6732 | dice_neg: 0.8782 | dice_pos: 0.4683 | IoU: 0.3667\n",
      "\n",
      "Starting epoch: 26 | phase: train | ⏰: 05:44:37\n",
      "Loss: 0.6912 | dice: 0.7167 | dice_neg: 0.8739 | dice_pos: 0.5596 | IoU: 0.4458\n",
      "Starting epoch: 26 | phase: val | ⏰: 05:53:15\n",
      "Loss: 1.4417 | dice: 0.6748 | dice_neg: 0.8634 | dice_pos: 0.4861 | IoU: 0.3815\n",
      "\n",
      "Starting epoch: 27 | phase: train | ⏰: 05:54:16\n",
      "Loss: 0.6756 | dice: 0.7193 | dice_neg: 0.8786 | dice_pos: 0.5600 | IoU: 0.4462\n",
      "Starting epoch: 27 | phase: val | ⏰: 06:03:05\n",
      "Loss: 1.4706 | dice: 0.6744 | dice_neg: 0.8782 | dice_pos: 0.4706 | IoU: 0.3754\n",
      "\n",
      "Starting epoch: 28 | phase: train | ⏰: 06:04:09\n",
      "Loss: 0.6707 | dice: 0.7182 | dice_neg: 0.8760 | dice_pos: 0.5604 | IoU: 0.4503\n",
      "Starting epoch: 28 | phase: val | ⏰: 06:12:55\n",
      "Loss: 1.5288 | dice: 0.6733 | dice_neg: 0.8592 | dice_pos: 0.4873 | IoU: 0.3732\n",
      "Epoch    29: reducing learning rate of group 0 to 5.0000e-08.\n",
      "\n",
      "Starting epoch: 29 | phase: train | ⏰: 06:13:57\n",
      "Loss: 0.7258 | dice: 0.7141 | dice_neg: 0.8734 | dice_pos: 0.5549 | IoU: 0.4388\n",
      "Starting epoch: 29 | phase: val | ⏰: 06:22:31\n",
      "Loss: 1.5767 | dice: 0.6747 | dice_neg: 0.8782 | dice_pos: 0.4713 | IoU: 0.3630\n",
      "\n",
      "Starting epoch: 30 | phase: train | ⏰: 06:23:31\n",
      "Loss: 0.7055 | dice: 0.7211 | dice_neg: 0.8818 | dice_pos: 0.5605 | IoU: 0.4477\n",
      "Starting epoch: 30 | phase: val | ⏰: 06:32:09\n",
      "Loss: 1.4907 | dice: 0.6736 | dice_neg: 0.8697 | dice_pos: 0.4775 | IoU: 0.3624\n",
      "\n",
      "Starting epoch: 31 | phase: train | ⏰: 06:33:11\n",
      "Loss: 0.7223 | dice: 0.7131 | dice_neg: 0.8676 | dice_pos: 0.5585 | IoU: 0.4436\n",
      "Starting epoch: 31 | phase: val | ⏰: 06:42:01\n",
      "Loss: 1.6087 | dice: 0.6766 | dice_neg: 0.8887 | dice_pos: 0.4645 | IoU: 0.3561\n",
      "\n",
      "Starting epoch: 32 | phase: train | ⏰: 06:43:03\n",
      "Loss: 0.7091 | dice: 0.7160 | dice_neg: 0.8724 | dice_pos: 0.5595 | IoU: 0.4473\n",
      "Starting epoch: 32 | phase: val | ⏰: 06:51:46\n",
      "Loss: 1.5447 | dice: 0.6754 | dice_neg: 0.8803 | dice_pos: 0.4705 | IoU: 0.3635\n",
      "Epoch    33: reducing learning rate of group 0 to 5.0000e-09.\n",
      "\n",
      "Starting epoch: 33 | phase: train | ⏰: 06:52:50\n",
      "Loss: 0.6399 | dice: 0.7190 | dice_neg: 0.8770 | dice_pos: 0.5609 | IoU: 0.4445\n",
      "Starting epoch: 33 | phase: val | ⏰: 07:01:36\n",
      "Loss: 1.5925 | dice: 0.6759 | dice_neg: 0.8824 | dice_pos: 0.4694 | IoU: 0.3616\n",
      "\n",
      "Starting epoch: 34 | phase: train | ⏰: 07:02:38\n",
      "Loss: 0.6492 | dice: 0.7229 | dice_neg: 0.8854 | dice_pos: 0.5603 | IoU: 0.4414\n",
      "Starting epoch: 34 | phase: val | ⏰: 07:11:26\n",
      "Loss: 1.3424 | dice: 0.6737 | dice_neg: 0.8697 | dice_pos: 0.4776 | IoU: 0.3731\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 35 | phase: train | ⏰: 07:12:27\n",
      "Loss: 0.6329 | dice: 0.7218 | dice_neg: 0.8870 | dice_pos: 0.5566 | IoU: 0.4429\n",
      "Starting epoch: 35 | phase: val | ⏰: 07:21:15\n",
      "Loss: 1.7056 | dice: 0.6777 | dice_neg: 0.8845 | dice_pos: 0.4709 | IoU: 0.3601\n",
      "\n",
      "Starting epoch: 36 | phase: train | ⏰: 07:22:18\n",
      "Loss: 0.6647 | dice: 0.7194 | dice_neg: 0.8781 | dice_pos: 0.5607 | IoU: 0.4450\n",
      "Starting epoch: 36 | phase: val | ⏰: 07:30:59\n",
      "Loss: 1.7503 | dice: 0.6778 | dice_neg: 0.8866 | dice_pos: 0.4690 | IoU: 0.3544\n",
      "\n",
      "Starting epoch: 37 | phase: train | ⏰: 07:32:03\n",
      "Loss: 0.6657 | dice: 0.7196 | dice_neg: 0.8812 | dice_pos: 0.5580 | IoU: 0.4454\n",
      "Starting epoch: 37 | phase: val | ⏰: 07:40:52\n",
      "Loss: 1.5135 | dice: 0.6708 | dice_neg: 0.8655 | dice_pos: 0.4761 | IoU: 0.3694\n",
      "\n",
      "Starting epoch: 38 | phase: train | ⏰: 07:41:57\n",
      "Loss: 0.6995 | dice: 0.7190 | dice_neg: 0.8760 | dice_pos: 0.5620 | IoU: 0.4416\n",
      "Starting epoch: 38 | phase: val | ⏰: 07:50:45\n",
      "Loss: 1.4683 | dice: 0.6765 | dice_neg: 0.8845 | dice_pos: 0.4686 | IoU: 0.3641\n",
      "\n",
      "Starting epoch: 39 | phase: train | ⏰: 07:51:50\n",
      "Loss: 0.6523 | dice: 0.7176 | dice_neg: 0.8755 | dice_pos: 0.5597 | IoU: 0.4446\n",
      "Starting epoch: 39 | phase: val | ⏰: 08:00:38\n",
      "Loss: 1.4750 | dice: 0.6719 | dice_neg: 0.8571 | dice_pos: 0.4867 | IoU: 0.3792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_trainer = Trainer(model)\n",
    "model_trainer.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA34AAAFNCAYAAABfWL0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAABzFklEQVR4nO3dd3hUVf7H8fdJLySQhFATqkhHhICg0kQRK2BX7L2v666rrv7WXXVXXcu6Kra1F5oiig1UBAFFpUrvNbQEQnrPnN8fZ4AAAQPMZFI+r+eZZ2bu3Ln3O7mZZD5zzj3HWGsRERERERGR2iso0AWIiIiIiIiIfyn4iYiIiIiI1HIKfiIiIiIiIrWcgp+IiIiIiEgtp+AnIiIiIiJSyyn4iYiIiIiI1HIKfiIiIjWQMWaDMeb0QNchIiI1g4KfiIjUGJUNO8aYgcaY1AqWTzfG3Oif6qqnQ/0sRESkblHwExERqQLGmJBA1yAiInWXgp+IiNRIxpggY8zDxpiNxpg0Y8x7xpj6x7C9s40xy4wxOcaYLcaYP5d7bJgxZqExJtsYs9YYM9S7vJkxZpIxJsMYs8YYc1O55/zdGPOxMeYDY0w2cK0xpr4x5k1jzDbvPh43xgQfop49zx/nrWm+MeaEQ6wbbox53hiz1Xt53rssGvgaaGaMyfVemh3tz0hERGouBT8REamprvVeBgFtgHrAS8ewvTeBW6y1MUAX4HsAY0xv4D3gPqAB0B/Y4H3OWCAVaAZcBPzLGHNauW0OAz72Pu9D4B2gFDgOOBEYAhyu6+kw4CMgHhgNfGqMCa1gvYeAPkB34ASgN/CwtTYPOAvYaq2t571srcTPQkREahkFPxERqalGAs9Za9dZa3OBB4HLjqFLZQnQyRgTa63dba2d711+A/CWtfZba63HWrvFWrvCGJMMnALcb60ttNYuBN4Ari63zdnW2k+ttR4gFjgbuMdam2etTQP+A1x2mJrmWWs/ttaWAM8BEbiAV9HP4lFrbZq1Nh34B3DVUf4cRESkFlLwExGRmqoZsLHc/Y1ACNAY16pWUctYKC7gVeRCXDDbaIz5wRjT17s8GVh7iP1nWGtzDqihebn7m8vdbund/zZjTKYxJhN4DWh0iHr2e743PO5pXayolgN/FurSKSIie+lEcxERqam24sLUHi1wgW8HEAw0NMbU87YGYowx3vU3HrghAGvtHGCYtyvlncB4XOjbDLQ9xP7jjTEx5cJfC2BL+c2Wu70ZKAIaWmtLK/kak/fcMMYEAUne/VZUS0tgabk69qxnK1hfRETqGLX4iYhITTUG+KMxprUxph7wL2CctbbUWrsJ+AV4yhhTzxgTjjtHrwT4+cANGWPCjDEjjTH1vd0qswGP9+E3geuMMYO9A8o0N8Z0sNZuBn4CnjDGRBhjuuG6hX5QUbHW2m3AN8CzxphY77baGmMGHOY19jTGXODtvnoPLjgeVL/3Z/GwMSbRGNMQ+Fu5OnYACccy8I2IiNR8Cn4iIlJTvQW8D8wA1gOFwF3lHr8U141yDa4VbjBwjrW28BDbuwrY4B2B81bceXNYa38FrsOdj5cF/MC+lsbLgVa41rWJwCPW2u8OU/PVQBiwDNiNG/il6WHW/8z7OnZ767vAG0wP9DgwF1gELAbme5dhrV2BC4brvF1M1QVURKQOMtaqB4iIiEh1Y4z5O3CctfbKQNciIiI1n1r8REREREREajkFPxERERERkVpOXT1FRERERERqObX4iYiIiIiI1HIKfiIiIiIiIrVcrZnAvWHDhrZVq1aBLkNERERERCQg5s2bt9Nam1jRY7Um+LVq1Yq5c+cGugwREREREZGAMMZsPNRj6uopIiIiIiJSyyn4iYiIiIiI1HIKfiIiIiIiIrVcrTnHryIlJSWkpqZSWFgY6FKkEiIiIkhKSiI0NDTQpYiIiIiI1Cq1OvilpqYSExNDq1atMMYEuhw5DGstu3btIjU1ldatWwe6HBERERGRWqVWd/UsLCwkISFBoa8GMMaQkJCg1lkRERERET+o1cEPUOirQXSsRERERET8w6/Bzxgz1Biz0hizxhjzwGHWu9AYY40xKd77rYwxBcaYhd7Lq/6s018yMzN5+eWXj+q5Z599NpmZmZVe/+9//zvPPPPMYdcZOHDgfnMdbtiwgS5duhxVfSIiIiIiUnP4LfgZY4KBUcBZQCfgcmNMpwrWiwH+APxywENrrbXdvZdb/VWnPx0u+JWWlh72uV999RUNGjTwQ1VV7/deq4iIiIiI+Jc/W/x6A2usteustcXAWGBYBes9BjwF1LqTux544AHWrl1L9+7due+++5g+fTr9+vXj/PPPp1Mnl4GHDx9Oz5496dy5M6+//vre57Zq1YqdO3eyYcMGOnbsyE033UTnzp0ZMmQIBQUFh93vwoUL6dOnD926dWPEiBHs3r37iOretm0b/fv3p3v37nTp0oWZM2cCMHnyZHr06MEJJ5zA4MGDAcjIyGD48OF069aNPn36sGjRIsC1QF511VWccsopXHXVVaSnp3PhhRfSq1cvevXqxY8//nhENYmIiEjllZR5WLk9hy8WbWXjrrxAlyMi1YA/R/VsDmwudz8VOKn8CsaYHkCytfZLY8x9Bzy/tTFmAZANPGytnenHWv3iySefZMmSJSxcuBCA6dOnM3/+fJYsWbJ35Mq33nqL+Ph4CgoK6NWrFxdeeCEJCQn7bWf16tWMGTOG//3vf1xyySVMmDCBK6+88pD7vfrqq3nxxRcZMGAAf/vb3/jHP/7B888/X+m6R48ezZlnnslDDz1EWVkZ+fn5pKenc9NNNzFjxgxat25NRkYGAI888ggnnngin376Kd9//z1XX3313te7bNkyZs2aRWRkJFdccQV//OMfOfXUU9m0aRNnnnkmy5cvr/wPU0RERCqUXVjCim05LNuaxfJtOSzbls3KHTkUl3oACA4yXJKSzN2Dj6Np/cgAVysigRKw6RyMMUHAc8C1FTy8DWhhrd1ljOkJfGqM6WytzT5gGzcDNwO0aNHisPv7x+dLWbY1+7DrHKlOzWJ55LzOR/Sc3r177zddwQsvvMDEiRMB2Lx5M6tXrz4o+LVu3Zru3bsD0LNnTzZs2HDI7WdlZZGZmcmAAQMAuOaaa7j44ouBigdPqWhZr169uP766ykpKWH48OF0796d6dOn079//721x8fHAzBr1iwmTJgAwGmnncauXbvIznY/5/PPP5/ISPcP5rvvvmPZsmV795GdnU1ubi716tU75GsRERGRfay1bM0qZNnWbHfZ5oLepoz8vevER4fRqWks157cio5NY2jdsB6fLtjCh79sZML8VK7q05LbB7YloV54AF+JU1hSRkiQISS41o81KFIt+DP4bQGSy91P8i7bIwboAkz3ho8mwCRjzPnW2rlAEYC1dp4xZi1wPDC33POx1r4OvA6QkpJi/fQ6fCo6Onrv7enTp/Pdd98xe/ZsoqKiGDhwYIXTGYSH7/vjHBwc/LtdPQ8lISFhv26fGRkZNGzY8KD1+vfvz4wZM/jyyy+59tpruffee4mLizvi/ZV/rR6Ph59//pmIiIijql1ERGqWnMISPB6oHxUa6FJqpOJSD2vSclm2bf+Ql1VQAoAx0Dohmq7N63Npr2Q6NY2lU7NYGsWEH/SlbvfkBtxwamtemLqat39cz9hfN3H9qa25sV8b6kdW7fGx1jJ/Uyajf9nEF4u2AtC+SQwdm8TSsWkMnZrVp0PTGGIjqrau4lIPG3blsXpHLqt25LA2PZewkCCS4qJIjot01/GRNImNUFCVGsufwW8O0M4Y0xoX+C4DrtjzoLU2C9ibOowx04E/W2vnGmMSgQxrbZkxpg3QDlh3LMUcacucL8TExJCTk3PIx7OysoiLiyMqKooVK1bw888/H/M+69evT1xcHDNnzqRfv368//77e1v/Bg4cyAcffMDpp5+OMYZ3332XQYMGHbSNjRs3kpSUxE033URRURHz58/noYce4vbbb2f9+vV7u3rGx8fTr18/PvzwQ/7v//6P6dOn07BhQ2JjYw/a5pAhQ3jxxRe57z7Xo3fhwoV7WzFFRKRmsdaSXVDK5t35bMksIHV3AVt2F5Ba7n5WQQlBBga2b8QlKckM7tiI0Br6gdlaS1Gph9yiUnILS8ktKiWvqJS84lJyi8rILyqluMxDSZmltMxDqcdSWmYp83go8ey/rNTj8V57L97nlXncOiVlHjLzS1ibnktJmftOOyI0iA5NYjmnW1M6No2lU9NYOjSJITq88h/jkuOjePriE7hlQFv+890qXvx+De/N3sitA9pyzcktiQrzbyew7MISPluwhQ9/2cSK7TlEhwVzYc8kosOCWb4th2+X72Dc3H1nCCXFRdKpaSwdvZfOzWJJios85qmfikrLWJeex+q0XNbsyGF1mgt6G3blU+ZxP29j3P5Lyyzbs7dgyzUthAQZmjaIIKlBFElxkSTH73/dOCaCoCBNT3UsrLXuveTxEGQMEaHBgS6p1vDbu9xaW2qMuROYAgQDb1lrlxpjHgXmWmsnHebp/YFHjTElgAe41Vqb4a9a/SUhIYFTTjmFLl26cNZZZ3HOOefs9/jQoUN59dVX6dixI+3bt6dPnz4+2e+7777LrbfeSn5+Pm3atOHtt98G4Oabb2bFihWccMIJGGNISUnhiSeeOOj506dP5+mnnyY0NJR69erx3nvvkZiYyOuvv84FF1yAx+OhUaNGfPvtt/z973/n+uuvp1u3bkRFRfHuu+9WWNMLL7zAHXfcQbdu3SgtLaV///68+mqNnKVDRKTWs9aSkVfsAl2mN9DtLih3v4Dcov1HbI4KCyYpLpLmDSLp0SKO5nGRZBWUMGFeKt+vSKNhvTAu7JHEJb2SaZsYuG7+Ho/lt9RMFqVmuSDnDXF7w1xRWbnb3uXFZXtDwZEKDTaEBAV5uzS6bo17b+9dvm9ZaFAQTetHMKhDo72teK0Sogn2UZg4rlE9Rl3Rg9sGZPHsNyt5avIK3vpxPXcOOo7LeicTHuLbD9mLUl3r3mcLt1JQUkbnZrH8a0RXzu/ejHrlgqu1lrScor0tnMu3ZbNsWzbfLt+xN3jFhIfQoWnMfoGwfZOYCoNBYUkZa9NzWeMNdqt3uNsbduWx51AGGWiZEE27RvUY2qUJ7RrFcFyjehzXqN7ebRaXetjq/Z1P3Z3P5t35pO4uYHNGPj+sSictp2i//YYGG5o32NdCmBS3Lxi2jI8iPjqsxs9bbK0lM7+EnblFpOcUkX7AdUFxGSV7vgjxuOuSMveFh1u+74uQvffLLCXlvhQpr3mDSNo3ieH4xjF08F63bRTt89/VyijzWLZmFrApI5+Nu/K5oEfzGhVMjbU1oofk70pJSbHl56gDWL58OR07dgxQRXI0dMxERKpGblEpmzPy2ZSRv/d6U0b+3ta7gpKy/daPiQghKS7K+6F236W5t+WjQVRohR9oS8s8/LAqnbFzNvP9ijTKPJZereK4JCWZc7o19XtLE0BeUSkzV+9k6vIdTFuZxs7c4r2PhQYbosNDiA4LoV54CNHhwUSH77ldwbKw/ZfvWS8sJIjQoCBCgg3BQYbQ4CCfhTV/mrshg39PWcmv6zNo3iCSe05vx4gTmx9Td8a8olIm/baV0b9sYvGWLCJDgzn/hGZccVILuiXVP6LgU1BcxsodOXvD4J5LXrH7/Qwy0CaxHh2bxtIkNpz1O/NZk+bOe9yTH4KDDK0SomjXKIZ2jevRrnEM7RrVo3XD6GP+0F5YUrb3y5DU3flsztgTEAvYsjt/v981cOG1ZcMoWiZE0yohipbx0bRMiKJVw+gKu+lWFWstuUWlpOcUsTO32AW5nMJ9t3OL9ga9nblFe1ujywsLDqJhvTCiw0MIDQ5yX3p4v9QIC9n3JUdYcNDeLz7CQrxfgASb/ZaHBrv3UEmZh9VpuazcnrNfK3hwkKF1w2jaN47ZLxQmx0cd8/uuqLSM1N0FbNyVx8Zd+d6Lu715d/5+r33yPf3o0OTgnm6BZIyZZ61NqfAxBT+pTnTMRER8o8xj2Z5dyKZd+we7PUFvV97+H0hjI0JokRBFcrlw19zbWtE8LtIn51yl5RQyYd4Wxs/dzPqdedQLD+G8E5pxaa9kTjjCQPB7Unfn8/2KNL5bnsbPa3dRXOYhJiKEge0bcXrHRvRpk0CDqNCAtBpUN9ZaZq7eydNTVrJ4SxZtEqP50xntOatLkyPqtrh8WzYf/rKRTxdsJbeolPaNYxjZpwXDT2zu03P2PB7L5t355VoGc1i+LZu0nEJaJUS7cLcn5DWKoXXDaMJCAtPNuKC4jNTd+XtbiDbuymOj9/bmjPz9WrciQ4NpmRBFi3gXBFsmRNEqwV03rR9Z6UBTWFJGZn4JmQXFZOWXkFlQQlZ+CVkFblmm93ZWQQmZ+SXszi9mZ24RhSWeg7YVHGRIiA4jMSachvXCSYwJ3/92uevYyBC/BteSMg8bduaxYnsOq3bk7L3elJG/t1U4IjSI4xvv3zrYvknMQaE6v7h0v0C3YVc+mzLy2LAzn61ZBft1740OC6al9zjsCewtvMemSWz169qr4Cc1ho6ZiEjllZZ5WLkj54Bg57qhpR7wzXRwkOuC1iI+iuR49+Gy/KUqB2Gx1jJnw27GzdnMl4u3UljioUOTGC5JSWbEic2Jiw474m2WeSwLN2fy/YodTF2exort7hz7Ng2jOa1DIwZ3bExKq7gae55hVbDWMmXpdp79ZhWr03Lp1DSW+85sz8D2iYf8QF9YUsYXi7Yx+peNzN+USVhIEOd2bcrIPi3o0SKuSluwrLU1qhtlaZmHrZmFbNiVt18A2RMO90zHAa41LSk+klYJ0bTwtmrtCW5ZBcV7b2cWlOz3vAMFBxnqR4bSIDKU2MhQGkS52wcFOu/tuKiwat9ynV9cyqoduazanrNfKNyZu68bboOoUI5vHIPHY9mYkU/6AV1046PDXOBOiKLFntZYb9BLqGHdcxX8pMbQMRMR+X15RaWMm7OZN2etZ0vmvpGe60eG7gtzCfsHu6b1q+dohNmFJXz+21bGzdnMotQswoKDGNK5MZf2SuaUtg0P+216blEpM1elM3VFGtNWpLErr5jgIEOvVnEM7tCYwR0b0SaA5xPWVGUey2cLt/Cf71axOaOAlJZx3Hdme05qs2+6qTVpOXz4yyYmzEslu7CUNonRXNG7BRf1TKJB1JEHd9mfx9tiv2FXHpvKBcINu/LZtCsPcO/3+lFhNIgMdWEuKpT6Ud7bkWH7lpV7vF64f1vlqpNduUWs3JHDqu05rPSO1rqn2+/eFrz4aFokRFX56Lb+pOAnNYaOmYjIoaXlFPLuTxv44OdNZBWU0KtVHFec1IJ2jWJIjqvaVjt/WL4tm3FzNjNxwRayCkpIiovk4p7JXJySRLMGbl7YzRn5TF2+g6kr0vhlXQbFZR7qR4YysH0ip3VoxMDjG9X4n0N1UVzqYfzczbwwdTVpOUX0a9eQoV2a8NnCrfy6PoPQYMPQLk25oncL+rSJrzOBQqQ6U/CTGkPHTETkYGvScvjfjPVMXLCFEo+HoZ2bcFP/NvRoceRzrNYEhSVlfLNsB+PnbGbWmp0YAye3TWBnTjErd3i7cCZGM3hPF86WcdWyNbO2KCwp473ZG3hl+lp255fQMiGKy72tew2rwUTwIrLP4YKf/4fSEhERkSNmreXX9Rn8b+Y6vlueRnhIEJf0SuLGU9vQqmF0oMvzqwjvKJDnn9CMzRn5fDR3M18s2kaj2HAePqcjgzs2pnUt/xlUJxGhwdzcvy2X927Bxl35dGoaW+0GtBCR36fgV83Uq1eP3NzcSi/fY/r06TzzzDN88cUXe5dde+21nHvuuVx00UV+qVVERHyvzGOZvGQ7r89cx2+bM4mPDuOe09txVZ+WJNTB1pXk+CjuHdKee4e0D3QpdV5MRChdmtcPdBkicpQU/OSYlZWVERys4bBFRI5FQXEZH83bzBsz17MpI59WCVE8NrwLF/VIIjJMf2NFROTYqEO8Hz3wwAOMGjVq7/2///3vPPPMM+Tm5jJ48GB69OhB165d+eyzzyq9TWst9913H126dKFr166MGzfuiOt64YUX6NSpE926deOyyy4DIDc3l+uuu46uXbvSrVs3JkyYAMCYMWPo2rUrXbp04f7779+7jXr16vGnP/2JE044gdmzZ/PBBx/Qu3dvunfvzi233EJZWVmF+xYRkf3tzC3iuW9WcvKTU/nbZ0tJqBfGq1f2YOqfBnJVn5YKfSIi4hNq8fOjSy+9lHvuuYc77rgDgPHjxzNlyhQiIiKYOHEisbGx7Ny5kz59+nD++edXajSsTz75hIULF/Lbb7+xc+dOevXqRf/+/Y+orieffJL169cTHh5OZmYmAI899hj169dn8eLFAOzevZutW7dy//33M2/ePOLi4hgyZAiffvopw4cPJy8vj5NOOolnn32W5cuX89RTT/Hjjz8SGhrK7bffzocffsjVV199ZD8wEZE6ZF16Lm/MWs+EeakUlXo4vWNjbhnQhpSWVTv3mYiI1A11J/h9/QBsX+zbbTbpCmc9eciHTzzxRNLS0ti6dSvp6enExcWRnJxMSUkJf/3rX5kxYwZBQUFs2bKFHTt20KRJk9/d5axZs7j88ssJDg6mcePGDBgwgDlz5lC/fsV97iv68NCtWzdGjhzJ8OHDGT58OADfffcdY8eO3btOXFwcM2bMYODAgSQmJgIwcuRIZsyYwfDhwwkODubCCy8EYOrUqcybN49evXoBUFBQQKNGjX73tYiI1CUejyUtp4g1abm8N3sD3y7fQWhwEBf2aM6N/drQVvPNiYiIH9Wd4BcgF198MR9//DHbt2/n0ksvBeDDDz8kPT2defPmERoaSqtWrSgsLDym/SQkJLB79+79lmVkZNCwYcOD1v3yyy+ZMWMGn3/+Of/85z/3tvIdiYiIiL3n9Vlrueaaa3jiiSeOrngRER/LLizB47HERIQSXEWjD5aUedieVUjq7gJSd+ezJbOALbsL3HVmAVszCygpc1Mo1Y8M5c5Bx3F131YkxtS9AVtERKTq1Z3gd5iWOX+69NJLuemmm9i5cyc//PADAFlZWTRq1IjQ0FCmTZvGxo0bK729fv368dprr3HNNdeQkZHBjBkzePrpp4mLi2Pr1q1758HbuHEjv/32G927d9/v+R6Ph82bNzNo0CBOPfVUxo4dS25uLmeccQajRo3i+eefB1xXz969e3P33Xezc+dO4uLiGDNmDHfddddBNQ0ePJhhw4bxxz/+kUaNGpGRkUFOTg4tW7Y86p+biMjRKCnz8Mr0tbz4/eq9ISsmPITYyFB3iQihvvd2/chQYiNCqR/pfTwilPpRe5aFEhsZQmRo8N6eE4UlZaTuCXK7C9iSmc+W3QV7l+3ILsRzwNS4jWLCaR4XSdfm9TmrS1Oax0WS1CCS3q3jiQ6vO/+CRUQk8PRfx886d+5MTk4OzZs3p2nTpoDrMnneeefRtWtXUlJS6NChQ6W3N2LECGbPns0JJ5yAMYZ///vfe7uIfvDBB1x33XUUFhYSGhrKG2+8cVAX0LKyMq688kqysrKw1nL33XfToEEDHn74Ye644w66dOlCcHAwjzzyCBdccAFPPvkkgwYNwlrLOeecw7Bhww6qqVOnTjz++OMMGTIEj8dDaGgoo0aNUvATkSq1ZEsW9328iOXbsjm3W1N6tIgju7CErIISsgtK3XVhCZsy8skuKCG7sJTcotLDbjM02BAbEQrArrzi/R4LDjI0rR9B8waR9G2bQFKDSBfs4qJo3iCSpg0iCA/RwCwiIlI9GGvt769VA6SkpNi5c+fut2xP65fUHDpmInKkikrLeHHqGl75YS3x0WH8c3gXhnT+/XOmAUrLPOQU7guFB4ZEd78Ej4XmDSJoHhdJ8wZRNI+LpHFMOCHBGhxbRESqD2PMPGttSkWPqcVPRERqrIWbM/nLx7+xakcuF/ZI4m/ndqJ+VGilnx8SHERcdBhx0WF+rFJERCTwFPxERKTGKSwp4z/fruJ/M9fRODaCt6/rxaD2Gk1YRETkUBT8RESkRpm3MYP7PlrEup15XN47mQfP7rj3PDwRERGpWK0PftZaTYRbQ9SW801FxD/yi0t5espK3vlpA80bRPLBDSdxaruDp6wRERGRg9Xq4BcREcGuXbtISEhQ+KvmrLXs2rWLiIiIQJciItXQ7LW7uH/CIjZl5HN135bcP7SDpkMQERE5ArX6v2ZSUhKpqamkp6cHuhSphIiICJKSkgJdhohUI7lFpTz19Qre/3kjLROiGHdzH05qkxDoskRERGqcWh38QkNDad26daDLEBGRozBzdToPTFjM1qwCbjy1NX8a0p7IMM2LJyIicjRqdfATEZGaJ7uwhH99uZyxczbTNjGaj289mZ4t4wJdloiISI2m4CciIpVS5rGsS89l8ZYslmzJZuWObEKDg4iPCqNBVBjx0aHERYcRH+XmxYuLCiMuOpS4qDBCKznR+bQVaTz4yWLScgq5bWBb/jC4HRGhauUTERE5Vgp+IiJykNIyD2vT87whz12Wbcsmv7gMgIjQINo3jsECa9Jy2Z1XTJ73sYrERIR4g2AY8VEHB8T46FC+WbaDT+ZvoX3jGF6/uifdkhpUzYsVERGpAxT8RETquNIyD6vTXEve0i1ZLPaGvMISDwCRocF0bhbLJSnJdGlen67N69M2MZqQA1rxikrLyMwvISOvmN15xWTku+vde5blF5ORV0x6bhGrduSyO794b5AECAky3D24HXcMakt4iFr5REREfEnBT0SkDikp87BqR463FS+bxVuyWL4tm6JSF/KiwlzIu7x3C7p6Q16bxHoEB/3+lDjhIcE0jg2mcWzlp2UpLCljd34xu/NKqB8VSvMGkUf92kREROTQFPxEROqAguIyHv9yGR/NS6XYG/LqhYfQqVksV/ZpSdfm9enSvD6tG0ZXKuT5SkRoME3rR9K0vgKfiIiIPyn4iYjUciu2Z3PX6AWsSc/l0pRk+rZNoGvz+rRKiCaoCkOeiIiIBI6Cn4hILWWt5YNfNvH4F8uIjQzl/etP4tR2DQNdloiIiASAgp+ISC2UlV/C/RMWMXnpdgYcn8izl5xAw3rhgS5LREREAkTBT0SklpmzIYM/jFlAWk4RD53dkRtOba0unSIiInWcgp+ISC1R5rG8PG0N//luFUlxUUy47WROSG4Q6LJERESkGlDwExGpBbZnFXLPuAX8vC6DYd2b8fjwLsREhAa6LBEREakmFPxERGq4qct38OePfqOwxMPTF3Xjop5JGKOunSIiIrJPkD83bowZaoxZaYxZY4x54DDrXWiMscaYlHLLHvQ+b6Ux5kx/1ikiUhMVlZbxj8+XcsO7c2lSP5LP7zqVi1OSFfpERETkIH5r8TPGBAOjgDOAVGCOMWaStXbZAevFAH8Afim3rBNwGdAZaAZ8Z4w53lpb5q96RURqknXpudw1ZgFLt2Zz7cmteOCsDkSEBge6LBEREamm/Nni1xtYY61dZ60tBsYCwypY7zHgKaCw3LJhwFhrbZG1dj2wxrs9EZE6b8K8VM59cRZbMgt4/aqe/P38zgp9IiIiclj+DH7Ngc3l7qd6l+1ljOkBJFtrvzzS54qI1DW5RaX8cdxC/vTRb3RpXp+v/9CPIZ2bBLosERERqQECNriLMSYIeA649hi2cTNwM0CLFi18U5iISDW0ODWLu8bMZ1NGPvec3o67TmtHsObmExERkUryZ/DbAiSXu5/kXbZHDNAFmO4diKAJMMkYc34lnguAtfZ14HWAlJQU68viRUSqA2stb85az1OTV5AQHc6Ym/pwUpuEQJclIiIiNYw/g98coJ0xpjUutF0GXLHnQWttFtBwz31jzHTgz9baucaYAmC0MeY53OAu7YBf/ViriEi1UVLmYeX2HBZuzmTyku3MWrOT0zs25umLuhEXHRbo8kRERKQG8lvws9aWGmPuBKYAwcBb1tqlxphHgbnW2kmHee5SY8x4YBlQCtyhET1FpDay1pK6u4CFmzNZuDmT3zZnsmRrFoUlHgASosP4x/mdubpvS03TICIiIkfNWFs7ekimpKTYuXPnBroMEZHDyswv5rfULBZuyuS3VBf0duUVAxAeEkSX5vXpntyAE5IbcGJyA5LiIhX4REREpFKMMfOstSkVPRawwV1ERGq7otIylm3N5rc9rXmpWazfmQeAMXBcYj1O69CIE5Ib0D25Ae2bxBAa7M/BlkVERKSuUvATEfGhFduzGfPLJhZuzmTZtmxKylyvisax4XRPbsDFKUl0T2pA16T6xESEBrhaERERqSsU/EREfGRtei6Xvf4zxaUeuiXV54ZT29A9uT7dk+NoUj8i0OWJiIhIHabgJyLiA2k5hVzz1q+EBBkm/aE/LRKiAl2SiIiIyF4KfiIixyi3qJTr3p5DRl4xY2/uo9AnIiIi1Y6Cn4jIMSgp83D7h/NZsT2HN65JoVtSg0CXJCIiInIQDR8nInKUrLU8MGExM1al88SIrgxq3yjQJYmIiIhUSMFPROQoPfftKibMT+We09txSa/kQJcjIiIickgKfiIiR2H0L5t48fs1XNYrmT8Mbue/HRXl+G/bIiIiUmco+ImIHKGpy3fw8KeLGdQ+kceHd8EY4/udeDww8zl4siV8/7jvt3+sdm+EV/vBt3+D4rxAV1Nz5GfA2JEw//1AVyIiInWMgp+IyBFYuDmTO0cvoEvz+rx0RQ9Cgv3wZzQ/A8ZcClP/AfWTYMbTsOQT3+/naBXlwpjLYecq+PG/MKoPrJwc6Kqqv9IiGHclrPgCJt0JX/wRSosDXZX4QlmJ+7JGRKQaU/ATEamkDTvzuP6dOSTGhPPmNb2IDvfDwMib57iWtHXT4exn4M65kNwHPr0dti3y/f6OlMcDE2+B9OVw2Ydw3dcQFuWC6tiRkLUl0BVWT9bCpLtg448w4nU45Q8w9y149zzI2RHo6qq/4nxYNB7SVwa6kv1Z647jU63g6bbw8fWwcDTkbA90ZSIiBzHW2kDX4BMpKSl27ty5gS5DRGqpnblFXPjKT2QXlDDhtpNpk1jPtzuwFn5+2XWdjG0Ol7wLzU50j+WmwesDwQTBTdOgXqJv930kvv8nzPg3nPkv6HuHW1ZaDLNfgh/+DUHBMOiv0PsWCNaMQXtNewJ+eBIGPQwD7nPLlkyAz+6EiPpw6QeQlBLYGqujkgKY+zbM+g/kpUFQKPT7E/S7F0LCA1tbVqo7fuumQev+7n27ZqqrE6BxF2h7Ghx3OrToE/h6RcS3dq2FhLaBruIgxph51toK/6Eo+ImI/I784lIuf/1nVu7IYfRNfejRIs63OyjIhM/ucF0AO5wLw0ZBZIP919m6AN4aCs16wNWfQUiYb2uojCWfwMfXQfcrYdhLcOC5jbs3wFf3wepvoElXOPe/kNSz6uusbhaOgU9vhe4j3bEt/3PbvgTGXgE52+CcZ6HH1YGrszopLYL578HMZ93PpnV/OPlu1+q3eDwkdoDzX4Tk3lVfm7Ww4AOY8lfwlMGQRyHlBndcPR7YsQTWTnUhcNPP4CmB0Cho1Q+OG+yCYHybg98/IlIzFGa79//C0XDzNGh6QqAr2o+Cn4jIUSot83Dz+/OYvjKN165K4YxOjX27g60LYPw1kL0FzngM+tx26A+Eiz+GCTdAyvVw7n98W8fv2fYbvHmmC3TXfnHo1gtrYfkk+Pp+190t5XoY/LeDg2xdsX4mvD8CWvaFkRMqDuz5Ga6L4LppLkAMfTIwwb46KC2GhR/AjGfce6LFya4FuXW/feus+sadH5m9BXrf7H6/wn3cAn8o2dvg8z/A6inQ8hQX5ONbH3r9olzYMNOFwLVTIWOdW96g5b4Q2Lo/hMdUTf0icmw2/Oi+yMtKhVPugYEPVLvWfAU/EZGjYK3lrxMXM+bXzTw+vAtX9mnpy43DnDfct4bRjeDidyC51+8/79tH4Mfn4ZznoNcNvqvncHLT4PVBgHVdTWMqEX4Ls2Hav+DX1yA60XUN7XJh3WrlSF8Fb54O9ZrADd8cPvyWlbrBfH56AVr0hYvfrdzPubYoK4HfxrpuxJmbIKm3C3xtBlb8O1OUA1Mfg19fdwMgnfs8tDvdf/VZC4s/ci3apUVw+iOuO3PQEQ6VkLHOGwK/h/UzoDgXgkIg+aR93UKbdDvy7YqIf5UUwvePwexRENcKRrwGLU4KdFUVUvATETkKL0xdzXPfruKOQW2578wOvttwYTZ8fjcsnQjthrh/IFHxlXuupwzGXOY+OF49CVqd4ru6KlJa5AYg2bYIrp8Mzbof2fO3LnCtM1sXQJtBrjtjNTwnwudy0+GNwVCSDzdOhbhKfmmw+GN33lhknPe8v1reVbas1AWqH56C3etdV+ZBD7nWsMp8SbDpFzdozs6V0PUS11oaneDbGnPT3O/wii9cIB3+CjQ87ti3W1oMm3/Z1y10u3fwpqiG0PJkqJ8MMU0gpqn7EiCmqbuv1kGRqrV1oXdQsxWuV8YZj1ZdL4OjoOAnInKExs/dzF8+XsQFPZrz7MUn+G6uvu2LXdfO3RvgtIddV5Ej/Xa/MAv+NxgKdrvzCxq08E1tB7LWhZCFH8BFb0OXC45uO54ymPMmTH0Uyoqh/5/dqJbVrHuMz5QUuLC8fQlc++WRh7fti73n/W13Lbs9rvJPnYHkKXNffEx/Enatdq1cgx6C48888lbh0iI35+XMZyEiFoY+BV0v8k3r8tKJ8OWfXJfN0x6Cvne6AYz8ITfNfaGzZipsmeuOf0n+weuF1YN65YLg3kvT/ZdX4w+mIjVCWakbWOqHJ90XMsNG+bdngY8o+ImIHIHpK9O44d25nNw2gTev6UVYiA+6XVkLC953XcUiGsBFbx1ba93O1S78xbWA679xUyr42uyXYcqD0P8v7kPvscre5ra3dCIktINzn3PnN9UmHg98dA0s/xwufR86nnd028nPcAPprJsOvW6EM5+oHef9eTyw/DMX+NJXQKPOMOhBN6jRsQa1Hctc69+Wua4l/ZznoEHy0W0rbxd89WdY+okbXXf4q9DIh63+lWEtFGW76T5ytrkgmLvdXe+9eJeXFhz8/LCYfS2Fx53uzreNiK3a1yC+l5vmvkhr0AJOuFzdgv1l52qYeKv7e9LlIjj76cr3zAkwBT8RkUpanJrFpa/PplVCNONv7Us9X8zVV5wHX9wLi8a6c5YueMM3UzKs/hY+vBg6j3BB0pfnz62ZCh9eBO3Phkve9+2Hi9XfwVd/cq2eJ1zuBrUJ5BQVvvTt39yk9kP+CSffeWzbKiuFqX+Hn1505/1d8h7Ua+STMqucta6r5LQnIG0pNGzvAl/HYb793fKUufP+pj7qpj8Z/IgLzkeyjxVfwuf3uBb1AffDqX+s3lOT7A2IBwTCXG9g3L3BdbUOr+/OC+5zW839ParLCjLd34KfX4GSPLeseU8332vzHgEtrVbxeNz599/+zfVKOfc5d356DaLgJyJSCZsz8hnx8k+EhwQx8faTaRQbcewbTVvhWoDSV7rRv/rf59uuYrOeh+8ecSMb9vuTb7a5cw28cRrEJrlBSfzRZaykwI3c+ON/ISwazvgHnHh1zf72eu7b8MU97hyQc571XRCvyef9WQurpsC0f7pz2BKOg4EPui8r/NVlEmD3Rncs1n7vzss7/8Xfb7Er2A1fP+C+oGncFUa84kaxrQ22LnB/K5Z9BsFhcOJIOPkuN62EVG/F+e7LjFn/gcJM6HyBG/hoy3z49v9cC2CPq92XHL4+v7WuyUp1Uyutmw7HneH+bsQ2DXRVR0zBT0Tkd2TkFXPRKz+xK6+YCbf15bhGPhhA4bdx7sNnWDRc+IZr7fM1a+GTm1w4uHwMtD/r2La39/zBDDeCZ2UHJTlaaSvgy3th449uiPvYZm5C872XBgfc914ivcvDY/0bICpr9Xcw+hI3KMllY3zfQrRtEYwb6br9nfscnHilb7fvS9a6Lzo2/ujmu9s6342CN+AB6Hpx1bWeWQuLxsHkB1yre78/u9a7irrMrv7WdRPNTXNfoPS/r3Z0rT3QrrVu5NiFo8FTCp2Gw6n3VLt5yAQ30u38d+GHp10X33ZD3Hnh5Y9VYbYbGOnnV9ygP4P/D3peVz3+JvpCSaH7+5HYwb/dLK11c4R+dZ97X5z5uPs51tBRqBX8REQOo6C4jJFv/MySrdmMvvEkUlod4z+YkgL4+i9uAuqWp8CFb/r3W8OSAnjrTNi1Dm6aContj247njIXXtZNd5PEtzrVp2UekrXw2xjXxa4w6+ALv/N/Kjz24LAYGQcdz4Xjh/r/n/f2JfDWUIhvBdd97b9RF/Mz4KNrYf0P0OsmGPoEBIf6Z19HwlPmBqTZ+JMLe5tmQ/4u91hcKxe4TrgscLXmprvwt+RjSOzonfjdO3XKnomYF7zvHhvxijunr7bL2e7Cwty3XDfRtqe5gaZa96+xH3ZrDY/H/a5O+6frptuir+vR0fLkQz8nbbkLLRtmumB49jOQ3LvKSvaLLfNg4m1uxF5w78+Wfd3cni37umlcfCFvpxu1d/kkSO7j/gbU8JZwBT8RkUMo81hu+2Ae3y7fwSsjezC0yzEGtIx1MO4q2LEETr3XjVRYFS0cWalurr3wenDT9y74HKlvHnbnkJz7PKRc5/MSj4rHA8U5FQfCgsyKlxdmunOb8ndB4y6uBafTMP98C569zU3bYK0L3bHNfL+P8spKXdfe2S+5D0CXvFv152uVFruugxt/dGFv8y8uPIALei1PcR9SW54Mca2rT5BYOdm1LmdvhZNudS3wX/3ZTQR/8t2u+1xtHWn2UAqzXPib/TLkpbnpNE69xw22U1tajWoKa2Hl1/D94+482MZdXeBrd0bl3kPWusGIpjwMOVuh+5Vw+t9r3vnTpcUw42k3Sm9MEzjt/9x7dNNsN31LcY5br34LbxDs6/7WNDz+yP/WrPwaJt3tunmf9pD7O1ALfu8V/EREDuHpKSsYNW0tj5zXietOaX1sGysrhdcHQtZm17Wz3Rk+qbHSNv0M75wLrfvBFR8dWeBcOAY+vdW1JJ3zjP9qrCplpe5b85nPws5VbhTRfvd6uxr6qOWpKBfePsuF/eu+hqbdfLPdylj0keuaGBXvhhhPbO/Cfmik7/dVnA+pc/a16KXOgdJC91hiB2/IO8V9AKvf3Pf796XCbDfwy5z/ufsJ7dy8fHtaAOuqkkLX6v7jf918ignHuQ/BJ1xW98JwIKyf6X4vU3+F+LYuhHQacXTnPBflwox/uzAfGuW2lXJD9R6gaI8dy9x8edsXuYG/hj7puvXvsad3wabZ7u/RptmQl+4ei2oILfq4v0ct+ropYg71msu39Dfu4ubSbdLF7y+vqij4iYhU4KvF27j9w/lc3juZf43oeuxz9f3ymuviefG70Hm4T2o8YvPedZPDn3wXDHm8cs/ZPAfeORuST4KrJlaP7oO+4ilzXXhmPAs7Frsh0E+5x50jdywfaD1lbq691d/A5ePg+CE+K7nSti2CsSMha9O+ZSER+7q67r002P+6osfD6+/7kFmY5b5Z39Oit3W+O+/FBLnBTva06LXoC9ENq/xl+8Smn2Hzr9D7Jv+E5ZrKU+YGgPnxedj2G9RrAn1vd+c7aSoI39u6wAW+td9DTDMYeD90H+mbv8Hpq9z/o3XTXLg5++nDdxcNJE+Z620y7Z+uu/65z7uu+r/HWnfe6qafYONsd717g3ssrB4k9dr3tyopxb3XN8yCT29zvWROuccNulbLvtxQ8BMROcDK7TmMePlHOjSJYczNfQgPOcbuHblp8GJPN7z2VRMD273tq/vcKHAjXnPf2B9O1hb43yD3D/GmaTVmnqIjtmd0yRlPu3mZYpq6cNzzWjf4zpFu6+u/uJ/xOc+66QICpSDTnZNZsHv/S2Gme6xg977rPUPAV8i4ABgW41qssRAU6n6fW/Z1YS+5t/tQJrWftS4wzHrenVNaF6eCKPHOjeiPLwbSV8H3j7kvpSLjXW+EXjf6fl/Wun1M/itkp0K3S+GMR10Xyupi11oXxDb/4uY9Pff5Y/tCKXurt0VwtrvesZS9f88ad3JfmMW1ghGvuhbCWkjBT0SknKz8Es4fNYv84jK+uOtUGvti2oZPboElE+D2n6Hhcce+vWNRVgLvj3AtGtd/7T68V6SkwA1Ksmst3PgtNOpYtXUGgrXug+yMZ9xACFEJ0PcO96GrsqFmz8T2fe+EM//p33p9qbTIe15k5gFBsdz9wizXza/lye73JiwqwEVLwG2Z71oAl01yU0F0PM99QRQc5lqm9rsufzv8EMvL3Q4Jh/rJ1WcE1aIc9wXRkk9gzXdQVuRajqIT3aVeIxdKoht5lzX0LvM+Hhl3+C/9MjfD9Cfht9GuG2bfO93fH3+3phbnwczn3IiuweGuleukWwLbu8PjgblvuvnygkPdgDRdL/b9l6YFu10Phk0/ud4tTbu58wb9MU1RNaHgJyLiVeaxXPfOHGav3cnYm/vQs6UPWrg2/Oi6Svb7kzsZvzrI2wX/G+hC4M3TD/6G11qYcKMLq76YBqIm2vSzC4BrvnUtGifd4lo0DtfqueJL172ywzm+n9hepDrbuQZ++i+s+sYForISKCt2l2MRVs8NtNNuiDsv2t8DJB2oOM+FvaUTXdft0kLXI6DTMBfq8na6Hh156e52XpobOMp6Dt5WUMi+QHhgOMxKhXnvAMZ90dTv3qrvKr1rLXx9v/ubl9jRdf9s3a9qawAXgCfd6XortB0Mw16q+uNeiyn4iYh4PTV5Ba9MX8u/RnTlipNaHPsGy0rgtf7um+I7fq1eLSTbl8CbZ0DjznDtl/ufxzDzWXduyeBH3AeQumzrAvfzWP45hEZDr+uh710Q03j/9bbMh3fOcS2j13xRvY61SKBYu38IPOTtCpaVFLieCau/dV0RwY1m2e4MFwSTevlnUJKSArfPpZ+40FeS74Ja5+HQeYQb1v9wX+p4ytz0KnnpLgjm7XS39wbEcpfcdCgtcOfIdh/pWtt8NRXB0dgzeujk+yFzE3S50J0PXhXBy1o3h+TkB9zP8Mx/uu721WXk31pCwU9EBPhi0VbuHL2AK05qwb9GdPXNRmePcqODXfph5U5Gr2rLPoPxV7uhvYe95P7BrvjKDUzS9SK44H/6p7tH2nLXHWrJx+58kB5Xwyl/gAbJ7gPS/wZDaATcOLXunOckUhWsde+/1d+4QLZpNtgy1/267WAXAo87/dimJigphLVTXcveyq+hONeNBNnpfOh8geve7I+h/K11rYqe0v1HqAy0kgJ3Dues/wAWknq7eRxb94PmKb7vfpuzA764B1Z+5aaiGf4yxB/jSNpSIQU/Eanzlm/L5oKXf6JTs1jG3NSHsBAfdNHL3gYv9XIniI/8qPoGqGn/gh+egqFPQZsB8Mbp0LCdm4JAIxoebNda92Hot7GAdQPkpM5zgwbc8A006hDoCkVqtz2DFq3+1nVLzN3hljfrsa81sNmJvx/USovdIDVLPnGBoyjbnYfX8XzXsteqX82Y5sCfMta7c+3Wz3ADn2Dd+Yct+rifT+sBblL4Y/k5Lf3UTZJenOdOh+hzW62YL6+6UvATkTotM7+Y816aRXGph8/vPJVGvhjMBeDjG1z3wDt+hvg2vtmmP3g8MO5KWDXZtVRZjzvvT+dUHF7mZjcYwrx3XevDlZ+44CwiVcfjcfO6rf7WtQimzgGsG5jpuNNdCGx72r5zc8tKYN0PrmVvxeduwKKI+tDhPOgywgWZ2jRljS8V7HbnrK+f4Qa/SlvmlofHuhbR1v1dGGzcpXLnN+dnuBGQF38ETbu7kab1xZnfKfiJSJ1VWubhunfm8Mu6DMbe0oceLeJ8s+H1M+Dd82DA/TDor77Zpj8V5cAbZ3gnG//KzWkklZPrHcyhLox6KlLd5e1y896t+daFwYIMd/5cUi+Iaw2rp7gAEx7rBmHqPALaDKo+I4fWJLlpLgCun+n+52Wsdcsj46HVqd6uof2h4fEH93hZ9Q1Mugvyd0L/v7hzyRW4q4SCn4jUWU98tZzXZqzjyQu6cllvHwzmAu4b5VdOcaO/3fFLzekuWbDbDTSQeHygKxEROXaeMjc40+pv3CVjnWsB7HyBawUM9VHvDnGytniD4Ax3ydrsltdr7O0W2t/N9/nzKzD/XTdy6IhXoVn3gJZd1yj4iUidNOm3rdw9ZgFX9mnB48N9NJgLwI//dXMPXT4O2g/13XZFRERqAmth94Z9IXDDzH3nYmLglLth4F8VvgPgcMHPr2e0GmOGAv8FgoE3rLVPHvD4rcAdQBmQC9xsrV1mjGkFLAdWelf92Vp7qz9rFZHaZdnWbP7y8W/0ahXH387t7LsNZ22B6U/B8Wcp9ImISN1kjBuVM7419LzGBcGdq9yIrI27QlLPQFcoFfBb8DPGBAOjgDOAVGCOMWaStXZZudVGW2tf9a5/PvAcsOeT1FprbXd/1ScitdfuvGJufn8uDSLDGDWyh29G8Nzjm4fcQB9nPfn764qIiNQFxkBie3eRasuHn4YO0htYY61dZ60tBsYCw8qvYK3NLnc3Gqgd/U5FJGBKyzzcOWY+adlFvHJlDxrF+LCbydppbqS4fn+CuFa+266IiIiIn/kz+DUHNpe7n+pdth9jzB3GmLXAv4G7yz3U2hizwBjzgzGmnx/rFJFa5KnJK/hxzS4eH9GFE301gidAaRF89Wc3bcPJd//++iIiIiLViD+DX6VYa0dZa9sC9wMPexdvA1pYa08E7gVGG2NiD3yuMeZmY8xcY8zc9PT0qitaRKqlzxZu4X8z13N135ZckpLs243Pfgl2rYGzntbJ6iIiIlLj+DP4bQHKf/JK8i47lLHAcABrbZG1dpf39jxgLXDQ+OPW2tettSnW2pTExERf1S0iNdCSLVncP2ERvVvF83/ndvLtxjM3ww9PQ4dzod3pvt22iIiISBXwZ/CbA7QzxrQ2xoQBlwGTyq9gjGlX7u45wGrv8kTv4DAYY9oA7YB1fqxVRGqwjLxibnl/HnFRbjCX0GAf/2mb8qC7HqoBXURERKRm8tuontbaUmPMncAU3HQOb1lrlxpjHgXmWmsnAXcaY04HSoDdwDXep/cHHjXGlAAe4FZrbYa/ahWpLTLyiomLCsUYE+hSqkxpmYc7R88nPbeIj27pS2JMuG93sPo7WP45DP4bNPBx91ERERGRKqIJ3EVqic8WbuGecQsZcWJznrigK+EhwYEuqUo89sUy3py1nmcuPoGLeib5duMlhfByHwgKgdt+gpAw325fRERExIcON4F7wAd3EZFjN3dDBvd9tIikuEg+mb+Fkf/7hV25RYEuy+8mLkjlzVnrufbkVr4PfQA/vQi718PZ/1boExERkRpNwU+khtu0K5+b359H87hIJt1xKqOu6MGSrVkMG/UjK7fnBLo8v1myJYsHJizmpNbxPHROR9/vYPcGmPkMdBoObU/z/fZFREREqpCCn0gNllVQwnXv/IrHWt66thdx0WGc060p42/pS3Gphwte/pHvV+wIdJk+tyu3iFven0dCtJ8GcwGY/CCYYDjzX77ftoiIiEgVU/ATqaFKyjzc/uE8NmXk8+qVPWndMHrvY92SGjDpzlNpnRjNje/O5Y2Z66gt5/O6wVwWsDO3iNeuSqFhPR8P5gKwcjKs/AoG3g/1m/t++yIiIiJVTMFPpAay1vLwxCX8uGYXT1zQjT5tEg5ap0n9CMbf0pchnZrw+JfL+evExRSXegJQrW/9e8pKZq/bxT9HdKVrUn3f76CkAL7+CyR2gD63+377IiIiIgGg4CdSA70+Yx3j5m7mzkHHHXZQk6iwEF4e2YM7Bx3HmF83c/Vbv7A7r7gKK/WtLxZt5fUZ67i6b0v/DOYCMOt5yNwIZz8NwaH+2YeIiIhIFVPwE6lhJi/ZxpOTV3BOt6bce8bxv7t+UJDhz2e25/lLuzN/UyYjXv6RNWm5VVCpb63cnsNfPl5Ez5ZxPHxOJ//sJGMdzPoPdLkIWvf3zz5EREREAuB3g59xrjTG/M17v4Uxprf/SxORAy1KzeSecQs5IakBz158AkFBlZ+offiJzRlzUx9yi0oZ8fKPzFyd7sdKfSuroIRbP5i3twUzLMQP31lZC1/9BYLDYMjjvt++iIiISABV5tPTy0Bf4HLv/RxglN8qEpEKbc0s4IZ355IQHc7/rk4hIvTIJ2jv2TKOT+84heYNIrn27Tm8N3uD7wv1MY/H8qfxC9mckc/LI3vQODbCPzta+RWs+RYGPQixTf2zDxEREZEAqUzwO8laewdQCGCt3Q1oJmORKpRbVMr178yhsLiMt6/rRWLM0Y9kmRQXxce3nczA4xP522dL+dtnSygtq76Dvoyatobvlqfx8Dkd6d063j87Kc6Hrx+ARp2h9y3+2YeIiIhIAFUm+JUYY4IBC2CMSQSq76dEkVqmtMzDXaPnszotl5dG9uD4xjHHvM164SG8fnUKt/Rvw3uzN3LdO3PIKijxQbW+NW1lGs99t4rh3Ztxzcmt/Lejmc9C1iY45xkIDvHffkREREQCpDLB7wVgItDIGPNPYBagGY1FqsjjXy5n2sp0/n5+ZwYcn+iz7QYHGR48uyP/vrAbP6/bxQUv/8iGnXk+2/6x2rgrjz+MWUCHJrE8cUE3jKn8+YxHJHsb/PQCdLsMWp7sn32IiIiIBNhhg58xJghYD/wFeALYBgy31n5UBbWJ1Hnv/Lied37awA2ntuaqPi39so9LeiXz/g0nkZFXzPCXf2T22l1+2c+RKCgu49YP5gPw2pU9iQw78vMZK23ZZ1BWDP3/7L99iIiIiATYYYOftdYDjLLWrrDWjrLWvmStXV5FtYnUadNWpPHoF8s4vWNj/np2R7/uq0+bBD694xQa1gvnqjd/Yeyvm/y6v8Ox1vLgJ4tYsT2b/15+Ii0Sovy7w6UToXEXaNjOv/sRERERCaDKdPWcaoy50Pitn5WIHGj5tmzuHD2fjk1j+e9l3Qk+gmkbjlbLhGg+uf1kTj6uIQ98spjHv1hGmcf6fb8HevenDXy6cCv3nn48g9o38u/OsrbA5p+h83D/7kdEREQkwCoT/G4BPgKKjTE53ku2n+sSqbPSsgu54Z051IsI4c1rehEdXnWDjcRGhPLWNSlce3Ir3pi1nmvf/pUtmQVVtv9f12fw+JfLOb1jI+4YdJz/d7jsM3fdaYT/9yUiIiISQL8b/Ky1MdbaIGttqPd2jLU2tiqKE6lrCorLuPG9uezOL+HNa3rRpL6f5qw7jJDgIP5+fmeeuKAr8zbu5oznfuCtWev93vq3I7uQ2z+cT3J8FM9d2v2IJqc/aksnQpOu0LAKQqaIiIhIAFWmxQ9jzPnGmGe8l3P9XZRIXeTxWP44biGLt2TxwuUn0qV5/YDWc3nvFnzzx/70bh3Po18s44JXfmLFdv809heXerjtg3nkF5fy6pU9iY0I9ct+9pOVCqm/Qme19omIiEjt97vBzxjzJPAHYJn38gdjzBP+LkykrnlqygomL93OQ2d35IxOjQNdDuAme3/72l7897LubM7I59wXZvH0lBUUlpT5dD+Pf7mM+Zsy+fdF3Wjf5NjnKayUvd08h1fN/kREREQCqDItfmcDZ1hr37LWvgUMBc7xb1kidcvYXzfx2g/rGHlSC244tXWgy9mPMYZh3Zsz9d4BDOvenFHT1nLWf2fy8zrfTPvw8bxU3pu9kZv7t+Hcbs18ss1KWToRmnSDhLZVt08RERGRAKlUV0+gQbnbge1/JlLL/LhmJw9/uoR+7Rry9/M7+2+i8mMUFx3Gs5ecwPs39KbU4+Gy13/mgQmLyMovOeptLtmSxUMTF9O3TQJ/ObO9D6v9HZmbIHWOunmKiIhInVGZ4PcEsMAY844x5l1gHvBP/5YlUjesScvl1g/m0bphNKNG9iA0uLLfxQROv3aJfHPPAG7p34bxczdz+n9+4KvF27D2yAZ/2Z1XzC3vzyM+OowXrziRkKp87Xu6eWoaBxEREakjKjOq5xigD/AJMAHoa60d5+/CRGq70jIPd41ZQFhwEG9d26tqBjTxkciwYB48uyOT7jyVRjHh3P7hfG56bx7bsio39UOZx3L32AWk5xTxypU9aVgv3M8VH2DpRGjaHeLbVO1+RURERAKkMoO7jADyrbWTrLWTgEJjzHC/VyZSy703eyPLt2Xz2PAuJMdHBbqco9KleX0+u+MUHjyrA7PWpHPGczN4f/YGPL8z9cNz365k5uqdPDqsM92TG1RNsXvs3ghb5qmbp4iIiNQplelb9Yi1NmvPHWttJvCI3yoSqQPSsgt57ttV9D8+kbO6NAl0OcckJDiIWwa0Zco9/eme3ID/+2wpF782m9U7cipcf/KS7YyatpbLeiVzWe8WVVwt6uYpIiIidVJlgl9F64T4uhCRuuTxL5dTXOrhH9V4MJcj1TIhmvdv6M0zF5/A2vRczn5hJv/5dhVFpfumfliTlsufP/qNE5Lq8/fzOwem0KUTodmJENcqMPsXERERCYDKBL+5xpjnjDFtvZf/4AZ4EZGj8NOanUz6bSu3DmhD64bRgS7Hp4wxXNQzie/uHcBZXZry36mrOeeFWczdkEFuUSm3vD+X8JAgXrmyJxGhwVVf4O4NsHW+unmKiIhInVOZ4HcXUAyM814KgTv8WZRIbVVc6uH/PltCcnwktw86LtDl+E3DeuG8cPmJvH1dLwqKy7jo1dmc/+Is1u/M48UrTqRZg8jAFLb0U3etSdtFRESkjvndLpvW2jzgAQBjTDAQ7V0mIkfozVnrWZuex5vXpBCRvQEWjoawaDj5LgiuOaN6Vtag9o345o/9efabVbzz03r+enZHTm7bMHAFLZ0IzXtCXMvA1SAiIiISAL8b/Iwxo4FbgTJgDhBrjPmvtfZpfxcnUptsySzg9alLeDhpGYN/fhk2zgITBNYDK7+Ci96GBsmBLtPnosND+Nt5nfjzmccTFRbA04Mz1sO2hTDk8cDVICIiIhIglenq2clamw0MB74GWgNX+bMokVrFWtgynzVv3cSMoFu4ceeTkL0FTvs/+ONSuPgdSFsBr/WDVVMCXa3fBDT0ASz71F13GhbQMkREREQCoTKfxEKNMaG44PeStbbEGHP4SbpEjsHa9FyufftXWiVE06dNAn3axNO1eQPCQirzPUU1kp8Biz+C+e/DjsWcZEPZ2PgM2p91O7Q8BYK8r6fzCGjSDT66BkZfAqfc40JhsAbP9amlE6F5CjQIwBQSIiIiIgFWmU+WrwEbgN+AGcaYlkC2P4uSuu2/361mZ04x0WEhPD1lJQCRocGktIrzBsEEuiXVJzS4GgZBjwfW/wAL3oflX0BZEZ4m3Xk+7Bamhfbn45vPgpAKRrNMaAs3fAeTH4Afn4fNv8BFb0Fssyp/CbXSrrWw7TcY8s9AVyIiIiISEJUZ3OUF4IU9940xm4BB/ixK6q41abl8vmgrt/RvywNndSAjr5hf1+/i53UZzF67a28QjAoLJqVVPH3axNOnTQJdmwc4CGaluoFaFrwPmZsgogH0vAZOvIoXlkbwwner+eCGkwivKPTtERoB5z3vWgM//wO8eipc8D84bnBVvYraS908RUREpI474r5k1loLlPqhFhFe+n41ESHB3NSvNQDx0WEM7dKUoV2aArArt4hf12fw87pdzF63i39PdkEwem8Q3NM1tD4h/g6CpcWw6muY/x6smQpYaD0ABj8CHc6F0Ag27srj5ekzOKdbU05tV8nRLLtdDE1PcF0/P7gQ+v8ZBj4IQQGY9662WPopJPWulYPniIiIiFSGX08iMsYMBf4LBANvWGufPODxW3FzApYBucDN1tpl3sceBG7wPna3tbb2jnohgDu3b9JvW7mpXxsS6oVXuE5CvXDO6tqUs7q6ILizfBBcu4unJq8AXBDs1Tp+b9fQLs1ifRcE01e6sPfbWMjfCTHNXDjrPhLiW+9dzVrL3yctJTTI8H/ndDqyfSQeDzdOha/ugxlPw6af4cI3IKaJb15DXbJrLWxfBGc+EehKRERERALGb8HPO+ffKOAMIBWYY4yZtCfYeY221r7qXf984DlgqDGmE3AZ0BloBnxnjDneWlvmr3ol8F76fg3hIcHc1L9NpZ/TsF44Z3dtytneIJies3+L4JNfuyDYoUkM71zXmyb1I46tyM2/wltD3TQM7c+CHldD29MqbI37ZtkOpq1M56GzOx7dfsOiYPgoaHUKfHEvvNrPhb82A47tNdQ1Sye6a3XzFBERkTrskMHPGHPBAYsssBNYaK3NqcS2ewNrrLXrvNsbCwwD9gY/7zQRe0R794F3vbHW2iJgvTFmjXd7syuxX6mB1qXn8tnCLdzYrw0ND9HaVxmJMeGc060p53TbFwSnr0zjH58v48JXfuKDG0+idcPoo9u4tfDdPyAqAW6dBTGND7lqfnEpj36+jPaNY7j2lFZHt789ul8BzU6E8dfAe8Nct8/+f1bXz8pa+ikknwT1mwe6EhEREZGAOVzft/MOuJwP/BlYZIw5rRLbbg5sLnc/1btsP8aYO4wxa4F/A3cfyXOl9nhp2hrCQoK4qV/lW/sqIzEmnItTkhl7cx8KS8q46JWfWLIl6+g2tm6am3S9/58PG/rAtV5uySzgseFdfDPoTKOOcNP30O1SmP4v+OACyE0/9u3WdjtXw47FbsoMERERkTrskJ9IrbXXVXAZBgwEfHayjLV2lLW2LXA/8PCRPNcYc7MxZq4xZm56uj4E11Qbdubx2cKtXHlSSxJjjr6173C6NK/PR7f2JSI0mMte/5nZa3cd2QashamPQf1k6HntYVddk5bL/2au44IezendOv7oiz5QeD0Y8Sqc94I75+/VU2HDj77bfm209FN3rW6eIiIiUscdcVOEtXYjEFqJVbcA5YfQS/IuO5SxuEniK/1ca+3r1toUa21KYmJiJUqS6ujF79cQEmS4eYBvW/sO1CaxHh/f1pem9SO45u1f+Wbp9so/ecWXsHU+DLgfQg4dTq21/O2zJUSEBvPgWR19UPUBjHHTRNw4FcKi4d1zYeazbv5AOdiyT6FFX82HKCIiInXeEQc/Y0x7oKgSq84B2hljWhtjwnCDtUw6YFvtyt09B1jtvT0JuMwYE26MaQ20A3490lql+tu4K49PF27hyj4taRRzjAOvVELT+pGMv6UvnZrGcusH8xg/d/PvP8lTBtP+CQnHwQmXH3bVzxdt46e1u7jvzPZ+a70EoEkXuOUH14Vx6qMw+hLIO8JWzNoufRXsWAKdhge6EhEREZGAO9zgLp+zb7CVPeKBpsCVv7dha22pMeZOYApuOoe3rLVLjTGPAnOttZOAO40xpwMlwG7gGu9zlxpjxuMGgikF7tCInrXTS97Wvlv83NpXXlx0GB/eeBK3fjCPv3y8iMz8Ym7u3/bQT1gyAdKWwUVvQfChB8LNKSzh8S+W0aV5LCNPaumHyg8QHgMXvgktT4bJD8Jr/eCit6HFSf7fd02w7FPAQKfzA12JiIiISMAZNx97BQ8Yc+CY8RbYBay21hb7u7AjlZKSYufOnRvoMuQIbNyVx2nP/sDVfVvyyHmdq3z/xaUe7h2/kC8WbePWAW25f2h7jDH7r1RWAi/1grB6cMsMCDp0I/ljXyzjrR/X88ltJ3Niizg/V3+ArQvdhO9ZqTD8Feh2SdXuvzp6uS9ENIDrvw50JSIiIiJVwhgzz1qbUtFjh2y+sNb+UG4DjYFeQCyQDqT5ukipe0ZNW0NwkOHWAYdpbfOjsJAg/nvZiTSICuXVH9ayO6+Yf47osv9E7ws+gN3r4fJxhw19K7Zn885PG7isV4uqD30Azbq7YDp2JEy8BTDQ7eKqr6O6SF/pWmnPejrQlYiIiIhUC797jp8x5hLc+XUXA5cAvxhjLvJ3YVK7bc7I55P5W7iidwsax/r/3L5DCQ4yPDasC3cPbse4uZu5Y/R8Cku8vYpLCmHG05DUC44/85Db8HgsD09cQmxECH85s30VVV6BiPpwxThoeQpMvBkWfxy4WgJt6aeom6eIiIjIPpUZ3OUhoJe19hpr7dW4idT/z79lSW03atoagoIMtw0MTGtfecYY7j3jeB45rxNTlu7gurfnkFNYAnPfguwtMPhvbjTNQ5gwP5W5G3fzwFkdiIsOq8LKKxAW7cJfi5Phk5vc+Yl10dKJLgDHNAl0JSIiIiLVQmWCX5C1tnzXzl2VfJ5IhTZn5PPxvFQu75Uc0Na+A113Smuev7Q7czZkcP3r0/HMeBZaD4DW/Q/5nKz8Ep78egU9WjTg4p7Jh1yvSoVFw8jxbhqDCTfBkk8CXRGUFsOij6Awy//7SlsO6cuh83D/70tERESkhjj0EIX7TDbGTAHGeO9fCnzlv5Kktnt5+hqCjOHWatDad6DhJzanfmQoC0c/TFDQTtJ630+jw6z/9Dcr2J1fzHs39CYo6NCtglUuLBquGA8fXgQTbgQTFLgglLXFDTyTOse1RF71CYRG+m9/e7p5dlQ3TxEREZE9frflzlp7H/A60M17ed1ae7+/C5PaKXV3Ph/NTeWy3sk0re/HD//HYFDLUO4K/4pppHD+xEJW78ipcL1FqZl8+Msmru7bis7N6ldxlZUQXg9GfuTOUfz4elj2WdXXsG66m2YibTn0uQM2zXZB1OOn2Vmsdd08W50KMY39sw8RERGRGqhSXTattROstfd6LxP9XZTUXi9PX0uQqR7n9h3Sjy8QUpJLy4v+RZm1XPzabBZs2r3fKmUey/99uoSG9cK5d8jxASq0EsJj4MqPISnFG/4mVc1+PR6Y+Sy8PwKiGsJN02Dov2Dok7DiC/jyTy6k+Vracti5Ut08RURERA5wyOBnjMkxxmRXcMkxxmRXZZFSO2zJLOCjuZu5pFdStW3tI2cH/PIqdLmQNl1OYsKtJxMbEcrIN35hxqr0vauNnbOJ31KzeOjsjsRGhAaw4EoIj4GRH0OzHvDxdbD8C//uryATxo2EqY9Cp+Fw0/eQ6A3HfW6FU/8I896GH57y/b6Xfeq6taqbp4iIiMh+Dhn8rLUx1trYCi4x1trYqixSaoeXp60B4LaBxwW4ksOY9RyUFsGgvwLQIiGKj2/rS8uEaG54dw5fLNrKrtwi/j15JX3axDOse7MAF1xJEbFw5QRodqI7327Fl/7Zz/Yl8PpAWP2Na9276C3X5bS8wY9A95Ew/Qk3cqqv7Onm2fIUqHe4MzNFRERE6h6NzilVYmtmAePnbuaSlGSaN6imrX2Zm10QOXEkJOzritooJoKxN/ehe3ID7hqzgKvf+pW8olIeG9YFc5hpHqqdPeGv6Qkw/hpY+bVvt79wDLxxOpQWwrVfQp/bKp4Gwxg477/Qbojr8umrFsi0ZbBzFXQe4ZvtiYiIiNQiCn5SJV6e7lr7bh9UjVv79nQ97P+Xgx6qHxnKe9efxGntG7F0azY3nNqado1jqrhAH4ioD1d+Ak26wrirYOXkY99maRF88Uf49FZ3LuEtM6BFn8M/JzgULn7H2/30etj407HXsXSiunmKiIiIHIKCn/jdtqwCxs9J5aKe1bi1b+caWDgaUm6ABhXPxxcZFsyrV/Xk9at6Vu8BXX5PZAO4aiI06QLjr4JVU45+W5mb4a2hrqX0lD/AVZ9WvpvlniknGrSAMZfBjmVHX8fe0Tz7Qb3Eo9+OiIiISC2l4Cd+98r0tXis5fbqPJLn9H9BSDj0u/ewq4UGBzGkcxPCQ4KrqDA/2RP+GnWCcVfC6m+PfBtrpsJr/WHnarj0AzjjUQiuzNSg5UQneOf1i4IPLnRB8mjsWAq71qibp4iIiMghKPiJX23PKmTsr5u5OCWJ5PioQJdTse2LYckEOOnWujUoSGQcXP0pNOoIY0fC6u8q9zyPB374twtqMU3g5unQ8byjr6NBC3fuYXEefHAB5Gcc+TaWTgQTfGx1iIiIiNRiCn7iV69MX+Nt7avG5/Z9/08Irw+n3B3oSqpeZJzrnpl4PIy9Atb8Tvgr2O26ZU77J3S9GG78Dhr64Ng27gyXj4HdG2H0JVCcX/nn7unm2bofRDc89lpEREREaiEFP/GbHdmFjJmzmQt7VOPWvs1zYNXXcMpdLgTVRVHxcPUkF/7GXOG6cFZk22/w2gBY+z2c/Qxc8Lo7T89XWp0CF74BW+a5+QbLSiv3vO2LIWOtunmKiIiIHIaCn/jNK9PXUuax3FGdR/L8/lGIaggn3RboSgJrT/hr6G35Wztt/8fnvw9vnAGeUrjua+h9U8VTNRyrTue7ULlqMnzxB9ea93v2dPPsoG6eIiIiIoei4Cd+kZZdyJhfN3Fhj+a0SKimrX3rfoD1M6Dfnw6eZLwuioqHqz+D+LauO+e66VBSCJPugkl3uikabpkByb38W0evG2DA/bDgA/j+scOvu6ebZ5sBbqAYEREREanQEQ7BJ1I5r/ywllKP5c5B7QJdSsWsdaEitjmkXB/oaqqP6AS4ZhK8ex6Mvgzi20DaUheOBz0EQVU0munAByF3B8x8Fuo1hpNuqXi97Ytg9/rfHY1VREREpK5T8BOfS8suZPQvmxhxYjVu7Vs1GVLnwHn/hdCIQFdTvUQ3dN0+3z0PslLhsjHQ4eyqrcEYOPtZyE2Hr++H6ETocsHB6+3t5nlu1dYnIiIiUsMo+InPvTZjnbe1r5qe2+fxwPePQ1xr6D4y0NVUT/US4eZpUFoYuEFvgkPgojfh/REw8RYXSFv33/f43m6eA103VRERERE5JJ3jJ76RtgJWfk36js18+MtGhndvTquGPhzx0ZeWfgI7lriui8Ghga6m+gqNDPxIp6GRbpqH+LZuxNFti/Y9tm0h7N6g0TxFREREKkEtfnLsdq6Bt86EwkwSgclBjWlYdir82heSekHjLq71pjooK4Vp/4JGnaDLhYGuRiojMs5N8P7mEPjwIrjhG4hr5Vr7gkKgwzmBrlBERESk2qsmn8alxsrPgNEXQ1AIWcPf57VPJjO0/iZabZkFKye4dUKjoFkPNxpkUm9I7h24ibZ/G+3mfLtsNASpwbvGqN8crvrEhb/3L4Drp8DST6HNIHXzFBEREakEBT85eqVFMO5KyNoC13zOqCWxvFESzEVXD4CG0ZC5yQ2gsvlXd/3Ti24eOHDn1yX3di2Cyb2hUWf/twqWFsH0p6B5T2hfxYOVyLFLbA9XjIf3hsEbgyFzo5v2QURERER+l4KfHB1r4fN7YOOPcOGb7Izvznuzv2dY9+a0SfTOiRfX0l26XuTulxTA1oWQ+qsLg+umw6Jx7rHQaGjeY18QTOrl+1bBuW9DdioMe8k/k4+L/7U4CS5+G8aOhKDQqh9tVERERKSGUvCTozPzWfhtNCX9H+Cn8AG889FvFJd6uPO0w4zkGRoJLfu6C7jwuF+r4K/w0wv7WgXrNYFGHd35eHuuE9sf3WTrxXkw8xlo1c+NAik1V/uz4NIP3Dx/gR58RkRERKSGUPCTI5Y5dzwNvn+Mn+sN5vrp3ckv/pWI0CDuOq0dbROPIJQZc+hWwS1zIW05pC2DuW9BacG+5zVouX8YbNQRGraDkPBD7+uX1yAvHS79UK19tYFa+kRERESOiIKf/C6Px7J4SxZTV6SRungG/8p6gDn2eB4suZkLeyRxWsdG9G2TQERo8LHv7MBWQQBPmTufa08QTFvuLmu+3dc6aIIh4biDWwjjW0NRDvz4PLQ703UVFBERERGpYxT8pEK5RaXMWp3O1OVpTFuZzs7cIpJMOl9EPkZRZCPiLv2I71u1xFRF61lQMMS3cZfyQ/eXFrsROsuHwe2LYNlngHXrBIe7cwULs+C0h/1fq4iIiIhINaTgJ3tt3JXH1OVpfL8ijV/W76KkzBIbEcKA9o04s20UZ/36GME5Fm6YSP3EVoEuF0LCvC17HfdfXpwPO1fu30J4wuXQtFtg6hQRERERCTAFv5pk+eeQsx1SrnetYMeopMzD3A27+X7FDqauSGNdeh4A7RrV4/pTWnNah0b0bBlHCB4YcynsWuUm0k5sf8z79quwKGh2oruIiIiIiIiCX41RVgKf/wHyd8Gi8TD8FWh4mBE0f8fEBan84/NlZOaXEBYcRJ+2CVzdpyWndWhMi4So/Vf+6n5Y8x2c919oO+gYX4iIiIiIiFQ1Bb+aYu00F/pSboAlE+DVU+H0R6D3LRAUVOnN5BeX8shnS/loXiq9WsVxY782nHpcQ6LDD/Gr8Mtr8Ovr0PdO6Hmtb16LiIiIiIhUKQW/mmLRODdn2dAnYcBfXOvf5AdgxZduQvK4Vr+7iRXbs7lz9ALWpudy9+B23H3acYQEHyY0rvrG7aP9OXDGo757LSIiIiIiUqUq31QkgVOU4wJe5wvcgCYxTeDysTBsFGz7DV45Bea+7SZEr4C1ljG/bmLYSz+SVVDCBzecxL1nHH/40Ld9CXx8HTTuAhf+zyfnFIqIiIiISGD4NfgZY4YaY1YaY9YYYx6o4PF7jTHLjDGLjDFTjTEtyz1WZoxZ6L1M8med1d6KL90E5t0u2bfMGDjxSrjtJ0hKgS/ugQ8uhKwt+z01p7CEu8Ys4MFPFtO7dTxf3d2PU45rePj95WyH0ZdCeAxcMQ7Con3/mkREREREpMr4LfgZY4KBUcBZQCfgcmNMpwNWWwCkWGu7AR8D/y73WIG1trv3cr6/6qwRFo2DBi0guYLJxxskw1WfwjnPwqaf4eW+sHAMWMui1EzOeWEWXy/Zzl+Gtufd63qTGBN++H0V58OYy6Egw7Uqxjbzy0sSEREREZGq489z/HoDa6y16wCMMWOBYcCyPStYa6eVW/9n4Eo/1lMz5eyAddPh1HtdK19FjIFeN0Lb0+DTO+DTW9k4ayy3bLsUU68x427uQ0qr+N/fl8cDE2+BrQvgsg+hWXdfvhIREREREQkQf3b1bA5sLnc/1bvsUG4Avi53P8IYM9cY87MxZrgf6qsZlkwA69m/m+ehxLch89KJjIu/jcbpP/JN+P18e+auyoU+gO8fg+WTYMhj0OGcY6tbRERERESqjWoxqqcx5kogBRhQbnFLa+0WY0wb4HtjzGJr7doDnnczcDNAixYtqqzeKrV4PDQ9oVKTps/dkMHdYxaQntufkP5DuGDT45hJN8Har+DsZyE64dBPXvABzHrOTdnQ907f1S8iIiIiIgHnzxa/LUByuftJ3mX7McacDjwEnG+tLdqz3Fq7xXu9DpgOnHjgc621r1trU6y1KYmJib6tvjpIX+W6XXa79LCreTyWUdPWcOnrPxMSHMSE207mwjNPw1z/DQz+Gyz/Al7uAyu+qngD62fC5/dAm4Fw9jOH7lIqIiIiIiI1kj+D3xygnTGmtTEmDLgM2G90TmPMicBruNCXVm55nDEm3Hu7IXAK5c4NrDMWjwcTBF0uPOQq6TlFXPP2rzw9ZSVndWnCF3efSrekBu7B4BDo9ye4eTrENIaxl8PEW6Egc98Gdq6BcVdCfGu4+F0IDvXnKxIRERERkQDwW1dPa22pMeZOYAoQDLxlrV1qjHkUmGutnQQ8DdQDPjKulWmTdwTPjsBrxhgPLpw+aa2tW8HPWlg0HloPcPP2VeDHNTu5Z9xCsgtKeOKCrlzWKxlTUWtdky5w4/cw8xmY8Qys+wGGvQjNesDoS9wcfVeMh8gG/n1NIiIiIiISEMYeYtLvmiYlJcXOnTs30GX4zqZf4K0hMPxV6H75fg+Vlnn479TVvDRtDW0aRjNqZA86NImt3Ha3zIdPb4P0FRCbBHlpcM3n0KKPH16EiIiIiIhUFWPMPGttSkWPVYvBXaQCi8dDSCR0PHe/xduyCvjDmIX8uiGDi3sm8Y9hnYkKO4LD2LwH3PwDTP8X/PwqDHtZoU9EREREpJZT8KuOykpgySfQ4WwIj9m7+Kc1O7lj9HyKSj08d8kJXNAj6ei2HxoBZzwKp/3NnQcoIiIiIiK1mj71V0drpkJBBnTdN3eftZa/TlxMg6gw3rgmhbaJ9Y59Pwp9IiIiIiJ1gj9H9ZSjtWgcRMbDcYP3Llq1I5cNu/K5sV9r34Q+ERERERGpMxT8qpvCbFj5FXS5YL+pFSYv2Y4xcEanxgEsTkREREREaiIFv+pmxRdQWnjQpO2Tl26nZ4s4GsVEBKgwERERERGpqRT8qptF4yCuFST12rto0658lm/LZmiXiufzExERERERORwFv+okexusn+EGdSk3EfuUpdsBOLOzgp+IiIiIiBw5Bb/qZMkEsB7odsl+iycv3U6nprEkx0cFqDAREREREanJFPyqk8XjodmJ0LDd3kVp2YXM37Rb3TxFREREROSoKfhVF2krYNtvBw3q8s2yHViLgp+IiIiIiBw1Bb/qYvF4MEHQ+YL9Fk9Zup3WDaNp10hz94mIiIiIyNFR8KsOPB5Y/BG0GQQx++bpy8ovYfbaXZzZuQmm3GAvIiIiIiIiR0LBrzrY/Atkbjqom+fUFTso9Vh18xQRERERkWOi4FcdLB4PoVHQ4Zz9Fk9Zup0msRF0a14/QIWJiIiIiEhtoOAXaKXFsHSiC33h+87jyy8u5YdV6ZzZuTFBQermKSIiIiIiR0/BL9DWfAcFu92k7eXMWJVOYYmHM9XNU0REREREjpGCX6AtGgdRDaHtoP0WT1m6g7ioUHq3ig9QYSIiIiIiUlso+AVSYRas/Bq6XADBoXsXF5d6+G75Dk7v2JiQYB0iERERERE5NkoVgbT8cygrOmg0z9nrdpFTWKrRPEVERERExCcU/AJp0TiIaw3Ne+63eMrS7USHBXPKcQ0DVJiIiIiIiNQmCn6Bkr0V1s90rX3lJmcv81i+WbqDgR0aEREaHMACRURERESktlDwC5TFHwMWuu0/muf8TbvZmVvE0M7q5ikiIiIiIr6h4Bcoi8a7Lp4JbfdbPGXJdsKCgxjYPjFAhYmIiIiISG2j4BcIO5bBjsUHDepirWXy0u2c2q4hMRGhh3iyiIiIiIjIkVHwC4TF48EEQ+cL9lu8dGs2qbsL1M1TRERERER8SsGvqnk87vy+tqdBvf27c36zdDtBBgZ3bBSg4kREREREpDZS8Ktqm2ZD1uaDBnUBmLx0O71bx5NQLzwAhYmIiIiISG2l4FfVFo+H0GjocM5+i9el57JqR666eYqIiIiIiM8p+FWl0iJYOhE6ngth0fs9NGXpDgCGKPiJiIiIiIiPKfhVpdXfQGEWdK24m+cJSfVp1iAyAIWJiIiIiEhtpuBXlRaNh+hEaDNwv8Xbsgr4bXMmZ3ZRa5+IiIiIiPiegl9VKciEVZOhy4UQHLLfQ994u3meqW6eIiIiIiLiBwp+VWX5JCgrrng0zyXbadeoHm0T6wWgMBERERERqe0U/KrKovEQ3xaa9dhvcUZeMb9uyGCounmKiIiIiIifKPhVhaxU2DALul0Kxuz30HfLd1DmsermKSIiIiIifuPX4GeMGWqMWWmMWWOMeaCCx+81xiwzxiwyxkw1xrQs99g1xpjV3ss1/qzT7xZ/DFjodvFBD01Zsp3mDSLp3Cy26usSEREREZE6wW/BzxgTDIwCzgI6AZcbYzodsNoCIMVa2w34GPi397nxwCPASUBv4BFjTJy/avW7ReMhqRfEt9lvcW5RKTPX7GRolyaYA1oCRUREREREfMWfLX69gTXW2nXW2mJgLDCs/ArW2mnW2nzv3Z+BJO/tM4FvrbUZ1trdwLfAUD/W6j/bl0DaUtfN8wDTV6ZRXOpRN08REREREfErfwa/5sDmcvdTvcsO5Qbg6yN5rjHmZmPMXGPM3PT09GMs108WjwcTDJ1HHPTQ5CXbaVgvjJ4ta25jpoiIiIiIVH/VYnAXY8yVQArw9JE8z1r7urU2xVqbkpiY6J/ijoXH487vO+50iG6430OFJWVMW5HGGZ2aEBykbp4iIiIiIuI//gx+W4DkcveTvMv2Y4w5HXgION9aW3Qkz632Nv4I2VsqnLvvp7U7ySsu48zOjQNQmIiIiIiI1CX+DH5zgHbGmNbGmDDgMmBS+RWMMScCr+FCX1q5h6YAQ4wxcd5BXYZ4l9Usie1hyOPQ/qyDHpq8ZDsx4SGc3LZhBU8UERERERHxnRB/bdhaW2qMuRMX2IKBt6y1S40xjwJzrbWTcF076wEfeUe13GStPd9am2GMeQwXHgEetdZm+KtWv6nXCE6+66DFpWUevluexuCOjQgLqRa9bUVEREREpBbzW/ADsNZ+BXx1wLK/lbt9+mGe+xbwlv+qC5w5G3aTkVes0TxFRERERKRKqLkpAKYs3U54SBAD2lfDAWlERERERKTWUfCrYtZapizdzoDjE4kK82uDq4iIiIiICKDgV+UWpWaxLatQ3TxFRERERKTKKPhVsclLtxMSZBjcsVGgSxERERERkTpCwa8KWWuZsmQ7fdsm0CAqLNDliIiIiIhIHaHgV4XWpOWybmceQ9TNU0REREREqpCCXxWasnQ7xsCZnRoHuhQREREREalDFPyq0OSl2+nRIo5GsRGBLkVEREREROoQBb8qsjkjnyVbsjmzs1r7RERERESkain4VZFvlu0A0DQOIiIiIiJS5RT8qsiUJdvp2DSWlgnRgS5FRERERETqGAW/KpCeU8ScjRnq5ikiIiIiIgGh4FcFvlu+A2thaBd18xQRERERkaqn4FcFJi/ZTquEKNo3jgl0KSIiIiIiUgcp+PlZdmEJP63dyZmdm2CMCXQ5IiIiIiJSByn4+dm0FWmUlFnOVDdPEREREREJEAU/P5u8ZDuNY8PpntQg0KWIiIiIiEgdpeDnR4UlZUxfmc6QTk0IClI3TxERERERCQwFPz+atXonBSVlGs1TREREREQCKiTQBdRmgzo04qNb+9I9uUGgSxERERERkTpMwc+PgoMMvVrFB7oMERERERGp49TVU0REREREpJZT8BMREREREanlFPxERERERERqOQU/ERERERGRWk7BT0REREREpJZT8BMREREREanlFPxERERERERqOQU/ERERERGRWk7BT0REREREpJZT8BMREREREanljLU20DX4hDEmHdgY6Doq0BDYGegiRMehmtBxCDwdg+pBxyHwdAyqBx2HwNMxqB58dRxaWmsTK3qg1gS/6soYM9damxLoOuo6HYfqQcch8HQMqgcdh8DTMagedBwCT8egeqiK46CuniIiIiIiIrWcgp+IiIiIiEgtp+Dnf68HugABdByqCx2HwNMxqB50HAJPx6B60HEIPB2D6sHvx0Hn+ImIiIiIiNRyavETERERERGp5RT8/MgYM9QYs9IYs8YY80Cg66mrjDEbjDGLjTELjTFzA11PXWCMecsYk2aMWVJuWbwx5ltjzGrvdVwga6wLDnEc/m6M2eJ9Pyw0xpwdyBprO2NMsjFmmjFmmTFmqTHmD97lej9UocMcB70fqogxJsIY86sx5jfvMfiHd3lrY8wv3s9K44wxYYGutTY7zHF4xxizvtx7oXuAS631jDHBxpgFxpgvvPf9/l5Q8PMTY0wwMAo4C+gEXG6M6RTYquq0Qdba7hquuMq8Aww9YNkDwFRrbTtgqve++Nc7HHwcAP7jfT90t9Z+VcU11TWlwJ+stZ2APsAd3v8Fej9UrUMdB9D7oaoUAadZa08AugNDjTF9gKdwx+A4YDdwQ+BKrBMOdRwA7iv3XlgYqALrkD8Ay8vd9/t7QcHPf3oDa6y166y1xcBYYFiAaxKpEtbaGUDGAYuHAe96b78LDK/KmuqiQxwHqULW2m3W2vne2zm4f/LN0fuhSh3mOEgVsU6u926o92KB04CPvcv1XvCzwxwHqULGmCTgHOAN731DFbwXFPz8pzmwudz9VPRPJlAs8I0xZp4x5uZAF1OHNbbWbvPe3g40DmQxddydxphF3q6g6mJYRYwxrYATgV/Q+yFgDjgOoPdDlfF2bVsIpAHfAmuBTGttqXcVfVaqAgceB2vtnvfCP73vhf8YY8IDV2Gd8DzwF8DjvZ9AFbwXFPykLjjVWtsD1+32DmNM/0AXVNdZN5ywvmEMjFeAtrguPtuAZwNaTR1hjKkHTADusdZml39M74eqU8Fx0PuhCllry6y13YEkXM+oDoGtqG468DgYY7oAD+KORy8gHrg/cBXWbsaYc4E0a+28qt63gp//bAGSy91P8i6TKmat3eK9TgMm4v7ZSNXbYYxpCuC9TgtwPXWStXaH95++B/gfej/4nTEmFBc2PrTWfuJdrPdDFavoOOj9EBjW2kxgGtAXaGCMCfE+pM9KVajccRjq7Q5trbVFwNvoveBPpwDnG2M24E4FOw34L1XwXlDw8585QDvvCD1hwGXApADXVOcYY6KNMTF7bgNDgCWHf5b4ySTgGu/ta4DPAlhLnbUnbHiNQO8Hv/Ket/EmsNxa+1y5h/R+qEKHOg56P1QdY0yiMaaB93YkcAbuXMtpwEXe1fRe8LNDHIcV5b6IMrhzy/Re8BNr7YPW2iRrbStcPvjeWjuSKngvaAJ3P/IOC/08EAy8Za39Z2ArqnuMMW1wrXwAIcBoHQf/M8aMAQYCDYEdwCPAp8B4oAWwEbjEWquBR/zoEMdhIK5bmwU2ALeUO9dMfMwYcyowE1jMvnM5/oo7v0zvhypymONwOXo/VAljTDfcgBXBuIaH8dbaR73/p8fiuhcuAK70tjqJHxzmOHwPJAIGWAjcWm4QGPETY8xA4M/W2nOr4r2g4CciIiIiIlLLqauniIiIiIhILafgJyIiIiIiUssp+ImIiIiIiNRyCn4iIiIiIiK1nIKfiIiIiIhILafgJyIicgBjTJkxZmG5ywM+3HYrY4zmyBIRkSoV8vuriIiI1DkF1trugS5CRETEV9TiJyIiUknGmA3GmH8bYxYbY341xhznXd7KGPO9MWaRMWaqMaaFd3ljY8xEY8xv3svJ3k0FG2P+Z4xZaoz5xhgTGbAXJSIidYKCn4iIyMEiD+jqeWm5x7KstV2Bl4DnvcteBN611nYDPgRe8C5/AfjBWnsC0ANY6l3eDhhlre0MZAIX+vXViIhInWestYGuQUREpFoxxuRaa+tVsHwDcJq1dp0xJhTYbq1NMMbsBJpaa0u8y7dZaxsaY9KBJGttUblttAK+tda2896/Hwi11j5eBS9NRETqKLX4iYiIHBl7iNtHoqjc7TJ0zr2IiPiZgp+IiMiRubTc9Wzv7Z+Ay7y3RwIzvbenArcBGGOCjTH1q6pIERGR8vQNo4iIyMEijTELy92fbK3dM6VDnDFmEa7V7nLvsruAt40x9wHpwHXe5X8AXjfG3IBr2bsN2Obv4kVERA6kc/xEREQqyXuOX4q1dmegaxERETkS6uopIiIiIiJSy6nFT0REREREpJZTi5+IiIiIiEgtp+AnIiIiIiJSyyn4iYiIiIiI1HIKfiIiIiIiIrWcgp+IiIiIiEgtp+AnIiIiIiJSy/0/TFjd51kO+5MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PLOT TRAINING\n",
    "losses = model_trainer.losses\n",
    "dice_scores = model_trainer.dice_scores # overall dice\n",
    "iou_scores = model_trainer.iou_scores\n",
    "\n",
    "def plot(scores, name):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(range(len(scores[\"train\"])), scores[\"train\"], label=f'train {name}')\n",
    "    plt.plot(range(len(scores[\"train\"])), scores[\"val\"], label=f'val {name}')\n",
    "    plt.title(f'{name} plot'); plt.xlabel('Epoch'); plt.ylabel(f'{name}');\n",
    "    plt.legend(); \n",
    "    plt.show()\n",
    "\n",
    "plot(losses, \"BCE loss\")\n",
    "plot(dice_scores, \"Dice score\")\n",
    "plot(iou_scores, \"IoU score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, root, df, size, mean, std, tta=4):\n",
    "        self.root = root\n",
    "        self.size = size\n",
    "        self.fnames = list(df[\"ImageId\"])\n",
    "        self.num_samples = len(self.fnames)\n",
    "        self.transform = Compose(\n",
    "            [\n",
    "                Normalize(mean=mean, std=std, p=1),\n",
    "                Resize(size, size),\n",
    "                ToTensorV2(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.fnames[idx]\n",
    "        path = os.path.join(self.root, fname + \".png\")\n",
    "        image = cv2.imread(path)\n",
    "        images = self.transform(image=image)[\"image\"]\n",
    "        return images\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "def post_process(probability, threshold, min_size):\n",
    "    mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]\n",
    "    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n",
    "    predictions = np.zeros((1024, 1024), np.float32)\n",
    "    num = 0\n",
    "    for c in range(1, num_component):\n",
    "        p = (component == c)\n",
    "        if p.sum() > min_size:\n",
    "            predictions[p] = 1\n",
    "            num += 1\n",
    "    return predictions, num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_submission_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-3c9681cfedf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmin_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_submission_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m testset = DataLoader(\n\u001b[1;32m     11\u001b[0m     \u001b[0mTestDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_submission_path' is not defined"
     ]
    }
   ],
   "source": [
    "size = 512\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "num_workers = 8\n",
    "batch_size = 16\n",
    "best_threshold = 0.5\n",
    "min_size = 3500\n",
    "device = torch.device(\"cuda:0\")\n",
    "df = pd.read_csv(sample_submission_path)\n",
    "testset = DataLoader(\n",
    "    TestDataset(test_data_folder, df, size, mean, std),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "model = model_trainer.net # get the model from model_trainer object\n",
    "model.eval()\n",
    "state = torch.load('./model.pth', map_location=lambda storage, loc: storage)\n",
    "model.load_state_dict(state[\"state_dict\"])\n",
    "encoded_pixels = []\n",
    "for i, batch in enumerate(tqdm(testset)):\n",
    "    preds = torch.sigmoid(model(batch.to(device)))\n",
    "    preds = preds.detach().cpu().numpy()[:, 0, :, :] # (batch_size, 1, size, size) -> (batch_size, size, size)\n",
    "    for probability in preds:\n",
    "        if probability.shape != (1024, 1024):\n",
    "            probability = cv2.resize(probability, dsize=(1024, 1024), interpolation=cv2.INTER_LINEAR)\n",
    "        predict, num_predict = post_process(probability, best_threshold, min_size)\n",
    "        if num_predict == 0:\n",
    "            encoded_pixels.append('-1')\n",
    "        else:\n",
    "            r = run_length_encode(predict)\n",
    "            encoded_pixels.append(r)\n",
    "df['EncodedPixels'] = encoded_pixels\n",
    "df.to_csv('submission.csv', columns=['ImageId', 'EncodedPixels'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/facebookresearch/moco/blob/colab-notebook/colab/moco_cifar10_demo.ipynb",
     "timestamp": 1677256490794
    }
   ]
  },
  "environment": {
   "name": "pytorch-gpu.1-4.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06768cfdf690415e9affdf2a74e59a30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "17032952ca804b558931b93c0fde1540": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cc091016bb8d423f80dddca00096ba0b",
       "IPY_MODEL_6bbbd0d7f0f749939610ccc1aa47bfec",
       "IPY_MODEL_eb9b54187a9b4404b93ce96cb5297a1e"
      ],
      "layout": "IPY_MODEL_48854cfeccd84183a1819d703353a4fa"
     }
    },
    "1d94bafa0c204861869e3c8656f88338": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20faf3fda02c4257b5ac68099de45581": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "23596ead25e842da821eee9281ddd44b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce5a59adc10e4ad3bbd5ee949704a493",
      "placeholder": "​",
      "style": "IPY_MODEL_2e92aabde375495f880ae9752b55e057",
      "value": " 4.64k/4.64k [00:00&lt;00:00, 115kB/s]"
     }
    },
    "2e92aabde375495f880ae9752b55e057": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "31ea43544fcd44a19f86c920afaa12ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3deab35d86db420c963d21fde4f4f770": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1d94bafa0c204861869e3c8656f88338",
      "placeholder": "​",
      "style": "IPY_MODEL_06768cfdf690415e9affdf2a74e59a30",
      "value": "Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"
     }
    },
    "3e187d79da6f477180cac4b95d949388": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "48854cfeccd84183a1819d703353a4fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5528a5ca02bc4cde9fa1bad142519e35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_79cc1507b29d4125ae5e3c704b2b8b75",
      "placeholder": "​",
      "style": "IPY_MODEL_31ea43544fcd44a19f86c920afaa12ab",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "55606bf393b940278f76baf65170eeab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5bffaeb6f389499c958f4a12dca192e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5d806cd2fe7e4424b7fb90371815f4d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3deab35d86db420c963d21fde4f4f770",
       "IPY_MODEL_e3214b7f823d4a12bed90f36b6cd3bbe",
       "IPY_MODEL_6bec28962d4740aabe8e9896e00134b7"
      ],
      "layout": "IPY_MODEL_5da754b21ef34dc9a7eff14bdc3a34bc"
     }
    },
    "5da754b21ef34dc9a7eff14bdc3a34bc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6bbbd0d7f0f749939610ccc1aa47bfec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4d8fcd96c2b44c1a21e6a0cf46d736d",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5bffaeb6f389499c958f4a12dca192e4",
      "value": 170498071
     }
    },
    "6bec28962d4740aabe8e9896e00134b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d083420df95d42ecb16cedd5037dc2a3",
      "placeholder": "​",
      "style": "IPY_MODEL_55606bf393b940278f76baf65170eeab",
      "value": " 223M/223M [00:02&lt;00:00, 86.7MB/s]"
     }
    },
    "73487b45ea444f8e81410a9627c85b40": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79cc1507b29d4125ae5e3c704b2b8b75": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7cdb20bd656249fe85674962d2a4ba7e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9cf2f5d1a7894a73861ce8bd69675ed1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a4d8fcd96c2b44c1a21e6a0cf46d736d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aeabbf27ff5e4ef5a0c3de652c86d632": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5528a5ca02bc4cde9fa1bad142519e35",
       "IPY_MODEL_c3b291f042fe422aa3a2b22f1208d0ca",
       "IPY_MODEL_23596ead25e842da821eee9281ddd44b"
      ],
      "layout": "IPY_MODEL_d74ac37d81974a4780581b554f3e7d23"
     }
    },
    "c3b291f042fe422aa3a2b22f1208d0ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73487b45ea444f8e81410a9627c85b40",
      "max": 4642,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3e187d79da6f477180cac4b95d949388",
      "value": 4642
     }
    },
    "cc091016bb8d423f80dddca00096ba0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7cdb20bd656249fe85674962d2a4ba7e",
      "placeholder": "​",
      "style": "IPY_MODEL_20faf3fda02c4257b5ac68099de45581",
      "value": "100%"
     }
    },
    "ce5a59adc10e4ad3bbd5ee949704a493": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf90228e48af46809d35402d94eb568e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d083420df95d42ecb16cedd5037dc2a3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d74ac37d81974a4780581b554f3e7d23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9070ee51e9c4ebe868777345a25a62d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3214b7f823d4a12bed90f36b6cd3bbe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f48ad91e88b340258d9fbe66bc58696b",
      "max": 223137427,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9cf2f5d1a7894a73861ce8bd69675ed1",
      "value": 223137427
     }
    },
    "eb9b54187a9b4404b93ce96cb5297a1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d9070ee51e9c4ebe868777345a25a62d",
      "placeholder": "​",
      "style": "IPY_MODEL_cf90228e48af46809d35402d94eb568e",
      "value": " 170498071/170498071 [00:14&lt;00:00, 14802420.28it/s]"
     }
    },
    "f48ad91e88b340258d9fbe66bc58696b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
