{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MXcbCW8f7Eh"
   },
   "source": [
    "### Some Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.9.0+cu111 in /opt/conda/lib/python3.7/site-packages (1.9.0+cu111)\n",
      "Requirement already satisfied: torchvision==0.10.0+cu111 in /opt/conda/lib/python3.7/site-packages (0.10.0+cu111)\n",
      "Requirement already satisfied: torchaudio==0.9.0 in /opt/conda/lib/python3.7/site-packages (0.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.9.0+cu111) (4.4.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision==0.10.0+cu111) (1.18.1)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision==0.10.0+cu111) (9.5.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: segmentation_models_pytorch in /opt/conda/lib/python3.7/site-packages (0.3.2)\n",
      "Requirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from segmentation_models_pytorch) (0.10.0+cu111)\n",
      "Requirement already satisfied: pretrainedmodels==0.7.4 in /opt/conda/lib/python3.7/site-packages (from segmentation_models_pytorch) (0.7.4)\n",
      "Requirement already satisfied: efficientnet-pytorch==0.7.1 in /opt/conda/lib/python3.7/site-packages (from segmentation_models_pytorch) (0.7.1)\n",
      "Requirement already satisfied: timm==0.6.12 in /opt/conda/lib/python3.7/site-packages (from segmentation_models_pytorch) (0.6.12)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from segmentation_models_pytorch) (4.65.0)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from segmentation_models_pytorch) (9.5.0)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.9.0+cu111)\n",
      "Requirement already satisfied: munch in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (2.5.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from timm==0.6.12->segmentation_models_pytorch) (5.3.1)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.7/site-packages (from timm==0.6.12->segmentation_models_pytorch) (0.14.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.18.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (4.4.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm==0.6.12->segmentation_models_pytorch) (3.0.12)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm==0.6.12->segmentation_models_pytorch) (2023.1.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm==0.6.12->segmentation_models_pytorch) (2.22.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm==0.6.12->segmentation_models_pytorch) (20.9)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm==0.6.12->segmentation_models_pytorch) (4.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from munch->pretrainedmodels==0.7.4->segmentation_models_pytorch) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.9->huggingface-hub->timm==0.6.12->segmentation_models_pytorch) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub->timm==0.6.12->segmentation_models_pytorch) (3.4.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm==0.6.12->segmentation_models_pytorch) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm==0.6.12->segmentation_models_pytorch) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm==0.6.12->segmentation_models_pytorch) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm==0.6.12->segmentation_models_pytorch) (2020.4.5.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping opencv-python as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: opencv-contrib-python in /opt/conda/lib/python3.7/site-packages (4.7.0.72)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.7/site-packages (from opencv-contrib-python) (1.18.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.18.1)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (3.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: albumentations in /opt/conda/lib/python3.7/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from albumentations) (1.18.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from albumentations) (1.7.3)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in /opt/conda/lib/python3.7/site-packages (from albumentations) (0.19.3)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from albumentations) (5.3.1)\n",
      "Requirement already satisfied: qudida>=0.0.4 in /opt/conda/lib/python3.7/site-packages (from albumentations) (0.0.4)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from albumentations) (4.7.0.72)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from qudida>=0.0.4->albumentations) (1.0.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from qudida>=0.0.4->albumentations) (4.4.0)\n",
      "Requirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2.6.3)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (9.5.0)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2.28.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2021.11.2)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (1.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (20.9)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->scikit-image>=0.16.1->albumentations) (2.4.7)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.28.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.18.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.0.1)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.4.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2020.4.5.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "Hit:1 http://free.nchc.org.tw/ubuntu bionic InRelease\n",
      "Hit:2 http://free.nchc.org.tw/ubuntu bionic-updates InRelease                  \n",
      "Hit:3 http://free.nchc.org.tw/ubuntu bionic-backports InRelease                \n",
      "Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]    \n",
      "Ign:4 https://deb.obspy.org xenial InRelease                                   \n",
      "Err:6 https://deb.obspy.org xenial Release                                     \n",
      "  Certificate verification failed: The certificate is NOT trusted. The certificate chain uses expired certificate.  Could not handshake: Error in the certificate verification. [IP: 95.217.194.95 443]\n",
      "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1581 B]\n",
      "Err:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC\n",
      "Ign:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
      "Reading package lists... Done                                                  \n",
      "E: The repository 'http://deb.obspy.org xenial Release' does not have a Release file.\n",
      "N: Updating from such a repository can't be done securely, and is therefore disabled by default.\n",
      "N: See apt-secure(8) manpage for repository creation and user configuration details.\n",
      "W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC\n",
      "E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease' is not signed.\n",
      "N: Updating from such a repository can't be done securely, and is therefore disabled by default.\n",
      "N: See apt-secure(8) manpage for repository creation and user configuration details.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "ffmpeg is already the newest version (7:3.4.11-0ubuntu0.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 137 not upgraded.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "libsm6 is already the newest version (2:1.2.2-1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 137 not upgraded.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "libxext6 is already the newest version (2:1.3.3-1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 137 not upgraded.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "unzip is already the newest version (6.0-21ubuntu1.2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 137 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install segmentation_models_pytorch\n",
    "!pip uninstall opencv-python\n",
    "!pip install opencv-contrib-python\n",
    "!pip install scikit-learn\n",
    "!pip install albumentations\n",
    "!pip install transformers\n",
    "!apt-get update\n",
    "!apt-get install ffmpeg -y\n",
    "!apt-get install libsm6 -y\n",
    "!apt-get install libxext6 -y\n",
    "!apt-get install unzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May  8 07:31:55 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.76       Driver Version: 515.76       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:20:00.0 Off |                  N/A |\n",
      "| 35%   53C    P8    30W / 200W |      0MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# ### for_multi_GPU\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5122,
     "status": "ok",
     "timestamp": 1677512171549,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "CvAFrPlS4bLU",
    "outputId": "2900b98f-10a0-48ae-ae6a-0aff7f787c20"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pdb\n",
    "import time\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader, Dataset, sampler\n",
    "from matplotlib import pyplot as plt\n",
    "from albumentations import (HorizontalFlip, ShiftScaleRotate, Normalize, Resize, Compose, GaussNoise)\n",
    "# from albumentations.torch import ToTensorV2\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_rtoBGhgSae"
   },
   "source": [
    "### Set Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30443,
     "status": "ok",
     "timestamp": 1677512201987,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "UKCi74HiH1A9",
    "outputId": "779d592f-9214-477e-c922-233d7845cb7c"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# ROOT_PATH = '/home/yasaisen/Desktop/11_research/11_research_main/lab_08'\n",
    "ROOT_PATH = '/home'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_submission_path = '../input/siim-acr-pneumothorax-segmentation/sample_submission.csv'\n",
    "# sample_submission_path = '/content/drive/My Drive/11_research_main/lab_01/CXR_Dataset/stage_2_sample_submission.csv'\n",
    "\n",
    "train_rle_path = os.path.join(ROOT_PATH, 'SIIM_ACR_dataset', 'train-rle.csv')\n",
    "data_folder = os.path.join(ROOT_PATH, 'SIIM_ACR_dataset', 'step_set_segmix_v7.0')\n",
    "test_data_folder = os.path.join(ROOT_PATH, 'SIIM_ACR_dataset', 'test_png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1677512201988,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "Uh-pffjlIDDw"
   },
   "outputs": [],
   "source": [
    "def checkpath(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2263,
     "status": "ok",
     "timestamp": 1677512204231,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "SRDsG25wIKf-"
   },
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "Version = '230508_v0.0.2'\n",
    "\n",
    "root_folder = os.path.abspath(os.path.join(ROOT_PATH, Version))\n",
    "\n",
    "# model_DIR = os.path.abspath(os.path.join(root_folder, 'model'))\n",
    "# checkpath(root_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupVit Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2463,
     "status": "ok",
     "timestamp": 1677512217559,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "1tdXYOBI7W7Q"
   },
   "outputs": [],
   "source": [
    "import collections.abc\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "\n",
    "# from ...activations import ACT2FN\n",
    "from transformers.activations import ACT2FN\n",
    "\n",
    "# from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n",
    "from transformers.modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n",
    "# from ...modeling_utils import PreTrainedModel\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "# from ...utils import (\n",
    "#     ModelOutput,\n",
    "#     add_start_docstrings,\n",
    "#     add_start_docstrings_to_model_forward,\n",
    "#     logging,\n",
    "#     replace_return_docstrings,\n",
    "# )\n",
    "from transformers.utils import (\n",
    "    ModelOutput,\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "# from .configuration_groupvit import GroupViTConfig, GroupViTTextConfig, GroupViTVisionConfig\n",
    "from transformers.models.groupvit.configuration_groupvit import GroupViTConfig, GroupViTTextConfig #, GroupViTVisionConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from typing import TYPE_CHECKING, Any, Mapping, Optional, Union\n",
    "\n",
    "# from ...configuration_utils import PretrainedConfig\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "# from ...onnx import OnnxConfig\n",
    "from transformers.onnx import OnnxConfig\n",
    "# from ...utils import logging\n",
    "from transformers.utils import logging\n",
    "\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    # from ...processing_utils import ProcessorMixin\n",
    "    from transformers.processing_utils import ProcessorMixin\n",
    "    # from ...utils import TensorType\n",
    "    from transformers.utils import TensorType\n",
    "\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "GROUPVIT_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n",
    "    \"nvidia/groupvit-gcc-yfcc\": \"https://huggingface.co/nvidia/groupvit-gcc-yfcc/resolve/main/config.json\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupViTVisionConfig(PretrainedConfig):\n",
    "    r\"\"\"\n",
    "    This is the configuration class to store the configuration of a [`GroupViTVisionModel`]. It is used to instantiate\n",
    "    an GroupViT model according to the specified arguments, defining the model architecture. Instantiating a\n",
    "    configuration with the defaults will yield a similar configuration to that of the GroupViT\n",
    "    [nvidia/groupvit-gcc-yfcc](https://huggingface.co/nvidia/groupvit-gcc-yfcc) architecture.\n",
    "\n",
    "    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n",
    "    documentation from [`PretrainedConfig`] for more information.\n",
    "\n",
    "    Args:\n",
    "        hidden_size (`int`, *optional*, defaults to 384):\n",
    "            Dimensionality of the encoder layers and the pooler layer.\n",
    "        intermediate_size (`int`, *optional*, defaults to 1536):\n",
    "            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n",
    "        depths (`List[int]`, *optional*, defaults to [6, 3, 3]):\n",
    "            The number of layers in each encoder block.\n",
    "        num_group_tokens (`List[int]`, *optional*, defaults to [64, 8, 0]):\n",
    "            The number of group tokens for each stage.\n",
    "        num_output_groups (`List[int]`, *optional*, defaults to [64, 8, 8]):\n",
    "            The number of output groups for each stage, 0 means no group.\n",
    "        num_attention_heads (`int`, *optional*, defaults to 6):\n",
    "            Number of attention heads for each attention layer in the Transformer encoder.\n",
    "        image_size (`int`, *optional*, defaults to 224):\n",
    "            The size (resolution) of each image.\n",
    "        patch_size (`int`, *optional*, defaults to 16):\n",
    "            The size (resolution) of each patch.\n",
    "        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n",
    "            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n",
    "            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` ``\"quick_gelu\"` are supported.\n",
    "        layer_norm_eps (`float`, *optional*, defaults to 1e-5):\n",
    "            The epsilon used by the layer normalization layers.\n",
    "        dropout (`float`, *optional*, defaults to 0.0):\n",
    "            The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.\n",
    "        attention_dropout (`float`, *optional*, defaults to 0.0):\n",
    "            The dropout ratio for the attention probabilities.\n",
    "        initializer_range (`float`, *optional*, defaults to 0.02):\n",
    "            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
    "        initializer_factor (`float`, *optional*, defaults to 1.0):\n",
    "            A factor for initializing all weight matrices (should be kept to 1, used internally for initialization\n",
    "            testing).\n",
    "\n",
    "    Example:\n",
    "\n",
    "    ```python\n",
    "    >>> from transformers import GroupViTVisionConfig, GroupViTVisionModel\n",
    "\n",
    "    >>> # Initializing a GroupViTVisionModel with nvidia/groupvit-gcc-yfcc style configuration\n",
    "    >>> configuration = GroupViTVisionConfig()\n",
    "\n",
    "    >>> model = GroupViTVisionModel(configuration)\n",
    "\n",
    "    >>> # Accessing the model configuration\n",
    "    >>> configuration = model.config\n",
    "    ```\"\"\"\n",
    "\n",
    "    model_type = \"groupvit_vision_model\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=384,\n",
    "        intermediate_size=1536,\n",
    "        depths=[6, 3, 3],\n",
    "        num_hidden_layers=12,\n",
    "        num_group_tokens=[64, 8, 0],\n",
    "        num_output_groups=[64, 8, 8],\n",
    "        num_attention_heads=6,\n",
    "        image_size=1024,\n",
    "        patch_size=16,\n",
    "        num_channels=3,\n",
    "        hidden_act=\"gelu\",\n",
    "        layer_norm_eps=1e-5,\n",
    "        dropout=0.0,\n",
    "        attention_dropout=0.0,\n",
    "        initializer_range=0.02,\n",
    "        initializer_factor=1.0,\n",
    "        assign_eps=1.0,\n",
    "        assign_mlp_ratio=[0.5, 4],\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.depths = depths\n",
    "        if num_hidden_layers != sum(depths):\n",
    "            logger.warning(\n",
    "                f\"Manually setting num_hidden_layers to {num_hidden_layers}, but we expect num_hidden_layers =\"\n",
    "                f\" sum(depth) = {sum(depths)}\"\n",
    "            )\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_group_tokens = num_group_tokens\n",
    "        self.num_output_groups = num_output_groups\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_channels = num_channels\n",
    "        self.hidden_act = hidden_act\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.dropout = dropout\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.initializer_range = initializer_range\n",
    "        self.initializer_factor = initializer_factor\n",
    "        self.assign_eps = assign_eps\n",
    "        self.assign_mlp_ratio = assign_mlp_ratio\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n",
    "        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
    "\n",
    "        # get the vision config dict if we are loading from GroupViTConfig\n",
    "        if config_dict.get(\"model_type\") == \"groupvit\":\n",
    "            config_dict = config_dict[\"vision_config\"]\n",
    "\n",
    "        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n",
    "            logger.warning(\n",
    "                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n",
    "                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n",
    "            )\n",
    "\n",
    "        return cls.from_dict(config_dict, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217559,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "M97ik37v9rbD"
   },
   "outputs": [],
   "source": [
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "_CHECKPOINT_FOR_DOC = \"nvidia/groupvit-gcc-yfcc\"\n",
    "\n",
    "GROUPVIT_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
    "    \"nvidia/groupvit-gcc-yfcc\",\n",
    "    # See all GroupViT models at https://huggingface.co/models?filter=groupvit\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217560,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "TfjSCoDn9utr"
   },
   "outputs": [],
   "source": [
    "# Copied from transformers.models.bart.modeling_bart._expand_mask\n",
    "def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n",
    "    \"\"\"\n",
    "    bsz, src_len = mask.size()\n",
    "    tgt_len = tgt_len if tgt_len is not None else src_len\n",
    "\n",
    "    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
    "\n",
    "    inverted_mask = 1.0 - expanded_mask\n",
    "\n",
    "    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217560,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "6PRDe_fu9xoT"
   },
   "outputs": [],
   "source": [
    "# contrastive loss function, adapted from\n",
    "# https://sachinruk.github.io/blog/pytorch/pytorch%20lightning/loss%20function/gpu/2021/03/07/CLIP.html\n",
    "def contrastive_loss(logits: torch.Tensor) -> torch.Tensor:\n",
    "    return nn.functional.cross_entropy(logits, torch.arange(len(logits), device=logits.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217560,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "NTvFTrtG9z1z"
   },
   "outputs": [],
   "source": [
    "# Copied from transformers.models.clip.modeling_clip.clip_loss with clip->groupvit\n",
    "def groupvit_loss(similarity: torch.Tensor) -> torch.Tensor:\n",
    "    caption_loss = contrastive_loss(similarity)\n",
    "    image_loss = contrastive_loss(similarity.t())\n",
    "    return (caption_loss + image_loss) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217560,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "iOvAJP0L91ns"
   },
   "outputs": [],
   "source": [
    "def hard_softmax(logits: torch.Tensor, dim: int):\n",
    "    y_soft = logits.softmax(dim)\n",
    "    # Straight through.\n",
    "    index = y_soft.max(dim, keepdim=True)[1]\n",
    "    y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(dim, index, 1.0)\n",
    "    ret = y_hard - y_soft.detach() + y_soft\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217560,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "6-b2H_fE93b1"
   },
   "outputs": [],
   "source": [
    "def gumbel_softmax(logits: torch.Tensor, tau: float = 1, hard: bool = False, dim: int = -1) -> torch.Tensor:\n",
    "    # more stable https://github.com/pytorch/pytorch/issues/41663\n",
    "    gumbel_dist = torch.distributions.gumbel.Gumbel(\n",
    "        torch.tensor(0.0, device=logits.device, dtype=logits.dtype),\n",
    "        torch.tensor(1.0, device=logits.device, dtype=logits.dtype),\n",
    "    )\n",
    "    gumbels = gumbel_dist.sample(logits.shape)\n",
    "\n",
    "    gumbels = (logits + gumbels) / tau  # ~Gumbel(logits,tau)\n",
    "    y_soft = gumbels.softmax(dim)\n",
    "\n",
    "    if hard:\n",
    "        # Straight through.\n",
    "        index = y_soft.max(dim, keepdim=True)[1]\n",
    "        y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(dim, index, 1.0)\n",
    "        ret = y_hard - y_soft.detach() + y_soft\n",
    "    else:\n",
    "        # Reparametrization trick.\n",
    "        ret = y_soft\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217561,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "sh78Xa0q9564"
   },
   "outputs": [],
   "source": [
    "def resize_attention_map(attentions, height, width, align_corners=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        attentions (`torch.Tensor`): attention map of shape [batch_size, groups, feat_height*feat_width]\n",
    "        height (`int`): height of the output attention map\n",
    "        width (`int`): width of the output attention map\n",
    "        align_corners (`bool`, *optional*): the `align_corner` argument for `nn.functional.interpolate`.\n",
    "\n",
    "    Returns:\n",
    "        `torch.Tensor`: resized attention map of shape [batch_size, groups, height, width]\n",
    "    \"\"\"\n",
    "\n",
    "    scale = (height * width // attentions.shape[2]) ** 0.5\n",
    "    if height > width:\n",
    "        feat_width = int(np.round(width / scale))\n",
    "        feat_height = attentions.shape[2] // feat_width\n",
    "    else:\n",
    "        feat_height = int(np.round(height / scale))\n",
    "        feat_width = attentions.shape[2] // feat_height\n",
    "\n",
    "    batch_size = attentions.shape[0]\n",
    "    groups = attentions.shape[1]  # number of group token\n",
    "    # [batch_size, groups, height*width, groups] -> [batch_size, groups, height, width]\n",
    "    attentions = attentions.reshape(batch_size, groups, feat_height, feat_width)\n",
    "    attentions = nn.functional.interpolate(\n",
    "        attentions, size=(height, width), mode=\"bilinear\", align_corners=align_corners\n",
    "    )\n",
    "    return attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217561,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "KyZtU92l98ix"
   },
   "outputs": [],
   "source": [
    "def get_grouping_from_attentions(attentions, hw_shape):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        attentions (`tuple(torch.FloatTensor)`: tuple of attention maps returned by `GroupViTVisionTransformer`\n",
    "        hw_shape (`tuple(int)`): height and width of the output attention map\n",
    "    Returns:\n",
    "        `torch.Tensor`: the attention map of shape [batch_size, groups, height, width]\n",
    "    \"\"\"\n",
    "\n",
    "    attn_maps = []\n",
    "    with torch.no_grad():\n",
    "        prev_attn_masks = None\n",
    "        for attn_masks in attentions:\n",
    "            # [batch_size, num_groups, height x width] -> [batch_size, height x width, num_groups]\n",
    "            attn_masks = attn_masks.permute(0, 2, 1).contiguous()\n",
    "            if prev_attn_masks is None:\n",
    "                prev_attn_masks = attn_masks\n",
    "            else:\n",
    "                prev_attn_masks = prev_attn_masks @ attn_masks\n",
    "            # [batch_size, heightxwidth, num_groups] -> [batch_size, num_groups, heightxwidth] -> [batch_size, num_groups, height, width]\n",
    "            cur_attn_map = resize_attention_map(prev_attn_masks.permute(0, 2, 1).contiguous(), *hw_shape)\n",
    "            attn_maps.append(cur_attn_map)\n",
    "\n",
    "    # [batch_size, num_groups, height, width]\n",
    "    final_grouping = attn_maps[-1]\n",
    "\n",
    "    return final_grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217561,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "sX37CrAn9_Mw"
   },
   "outputs": [],
   "source": [
    "class GroupViTCrossAttentionLayer(nn.Module):\n",
    "    def __init__(self, config: GroupViTVisionConfig):\n",
    "        super().__init__()\n",
    "        self.attn = GroupViTAttention(config)\n",
    "        self.norm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.mlp = GroupViTMLP(config)\n",
    "        self.norm_post = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, query, key):\n",
    "        x = query\n",
    "        x = x + self.attn(query, encoder_hidden_states=key)[0]\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        x = self.norm_post(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217561,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "LfMDcjbu-BFh"
   },
   "outputs": [],
   "source": [
    "class GroupViTAssignAttention(nn.Module):\n",
    "    def __init__(self, config: GroupViTVisionConfig):\n",
    "        super().__init__()\n",
    "        self.scale = config.hidden_size**-0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.proj = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.assign_eps = config.assign_eps\n",
    "\n",
    "    def get_attn(self, attn, gumbel=True, hard=True):\n",
    "        if gumbel and self.training:\n",
    "            attn = gumbel_softmax(attn, dim=-2, hard=hard)\n",
    "        else:\n",
    "            if hard:\n",
    "                attn = hard_softmax(attn, dim=-2)\n",
    "            else:\n",
    "                attn = nn.functional.softmax(attn, dim=-2)\n",
    "\n",
    "        return attn\n",
    "\n",
    "    def forward(self, query, key):\n",
    "        value = key\n",
    "        # [batch_size, query_length, channels]\n",
    "        query = self.q_proj(query)\n",
    "\n",
    "        # [batch_size, key_length, channels]\n",
    "        key = self.k_proj(key)\n",
    "\n",
    "        # [batch_size, key_length, channels]\n",
    "        value = self.v_proj(value)\n",
    "\n",
    "        # [batch_size, query_length, key_length]\n",
    "        raw_attn = (query @ key.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        attn = self.get_attn(raw_attn)\n",
    "        soft_attn = self.get_attn(raw_attn, gumbel=False, hard=False)\n",
    "\n",
    "        attn = attn / (attn.sum(dim=-1, keepdim=True) + self.assign_eps)\n",
    "\n",
    "        out = attn @ value\n",
    "\n",
    "        out = self.proj(out)\n",
    "\n",
    "        return out, soft_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217562,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "yOJBrOGt-GKD"
   },
   "outputs": [],
   "source": [
    "class GroupViTTokenAssign(nn.Module):\n",
    "    def __init__(self, config: GroupViTVisionConfig, num_group_token, num_output_group):\n",
    "        super().__init__()\n",
    "        self.num_output_group = num_output_group\n",
    "        # norm on group_tokens\n",
    "        self.norm_tokens = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        assign_mlp_ratio = (\n",
    "            config.assign_mlp_ratio\n",
    "            if isinstance(config.assign_mlp_ratio, collections.abc.Iterable)\n",
    "            else (config.assign_mlp_ratio, config.assign_mlp_ratio)\n",
    "        )\n",
    "        tokens_dim, channels_dim = [int(x * config.hidden_size) for x in assign_mlp_ratio]\n",
    "        self.mlp_inter = GroupViTMixerMLP(config, num_group_token, tokens_dim, num_output_group)\n",
    "        self.norm_post_tokens = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        # norm on x\n",
    "        self.norm_x = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.pre_assign_attn = GroupViTCrossAttentionLayer(config)\n",
    "\n",
    "        self.assign = GroupViTAssignAttention(config)\n",
    "        self.norm_new_x = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.mlp_channels = GroupViTMLP(config, config.hidden_size, channels_dim, config.hidden_size)\n",
    "\n",
    "    def project_group_token(self, group_tokens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            group_tokens (torch.Tensor): group tokens, [batch_size, num_group_tokens, channels]\n",
    "\n",
    "        Returns:\n",
    "            projected_group_tokens (torch.Tensor): [batch_size, num_output_groups, channels]\n",
    "        \"\"\"\n",
    "        # [B, num_output_groups, C] <- [B, num_group_tokens, C]\n",
    "        projected_group_tokens = self.mlp_inter(group_tokens)\n",
    "        projected_group_tokens = self.norm_post_tokens(projected_group_tokens)\n",
    "        return projected_group_tokens\n",
    "\n",
    "    def forward(self, image_tokens, group_tokens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_tokens (`torch.Tensor`): image tokens, of shape [batch_size, input_length, channels]\n",
    "            group_tokens (`torch.Tensor`): group tokens, [batch_size, num_group_tokens, channels]\n",
    "        \"\"\"\n",
    "\n",
    "        group_tokens = self.norm_tokens(group_tokens)\n",
    "        image_tokens = self.norm_x(image_tokens)\n",
    "        # [batch_size, num_output_groups, channels]\n",
    "        projected_group_tokens = self.project_group_token(group_tokens)\n",
    "        projected_group_tokens = self.pre_assign_attn(projected_group_tokens, image_tokens)\n",
    "        new_image_tokens, attention = self.assign(projected_group_tokens, image_tokens)\n",
    "        new_image_tokens += projected_group_tokens\n",
    "\n",
    "        new_image_tokens = new_image_tokens + self.mlp_channels(self.norm_new_x(new_image_tokens))\n",
    "\n",
    "        return new_image_tokens, attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217562,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "1eBLeUct-JCq"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GroupViTModelOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n",
    "            Contrastive loss for image-text similarity.\n",
    "        logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n",
    "            The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n",
    "            similarity scores.\n",
    "        logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n",
    "            The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n",
    "            similarity scores.\n",
    "        segmentation_logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels, logits_height, logits_width)`):\n",
    "            Classification scores for each pixel.\n",
    "\n",
    "            <Tip warning={true}>\n",
    "\n",
    "            The logits returned do not necessarily have the same size as the `pixel_values` passed as inputs. This is\n",
    "            to avoid doing two interpolations and lose some quality when a user needs to resize the logits to the\n",
    "            original image size as post-processing. You should always check your logits shape and resize as needed.\n",
    "\n",
    "            </Tip>\n",
    "\n",
    "        text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n",
    "            The text embeddings obtained by applying the projection layer to the pooled output of\n",
    "            [`GroupViTTextModel`].\n",
    "        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n",
    "            The image embeddings obtained by applying the projection layer to the pooled output of\n",
    "            [`GroupViTVisionModel`].\n",
    "        text_model_output (`BaseModelOutputWithPooling`):\n",
    "            The output of the [`GroupViTTextModel`].\n",
    "        vision_model_output (`BaseModelOutputWithPooling`):\n",
    "            The output of the [`GroupViTVisionModel`].\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits_per_image: torch.FloatTensor = None\n",
    "    logits_per_text: torch.FloatTensor = None\n",
    "    segmentation_logits: torch.FloatTensor = None\n",
    "    text_embeds: torch.FloatTensor = None\n",
    "    image_embeds: torch.FloatTensor = None\n",
    "    text_model_output: BaseModelOutputWithPooling = None\n",
    "    vision_model_output: BaseModelOutputWithPooling = None\n",
    "\n",
    "    def to_tuple(self) -> Tuple[Any]:\n",
    "        return tuple(\n",
    "            self[k] if k not in [\"text_model_output\", \"vision_model_output\"] else getattr(self, k).to_tuple()\n",
    "            for k in self.keys()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217562,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "8atGssmE-OgA"
   },
   "outputs": [],
   "source": [
    "class GroupViTPatchEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Image to Patch Embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int = 224,\n",
    "        patch_size: Union[int, Tuple[int, int]] = 16,\n",
    "        num_channels: int = 3,\n",
    "        embed_dim: int = 768,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        image_size = image_size if isinstance(image_size, collections.abc.Iterable) else (image_size, image_size)\n",
    "        patch_size = patch_size if isinstance(patch_size, collections.abc.Iterable) else (patch_size, patch_size)\n",
    "        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.projection = nn.Conv2d(num_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor, interpolate_pos_encoding: bool = False) -> torch.Tensor:\n",
    "        batch_size, num_channels, height, width = pixel_values.shape\n",
    "        if not interpolate_pos_encoding:\n",
    "            if height != self.image_size[0] or width != self.image_size[1]:\n",
    "                raise ValueError(\n",
    "                    f\"Input image size ({height}*{width}) doesn't match model\"\n",
    "                    f\" ({self.image_size[0]}*{self.image_size[1]}).\"\n",
    "                )\n",
    "        x = self.projection(pixel_values).flatten(2).transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217562,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "pvmCYxCX-Q5x"
   },
   "outputs": [],
   "source": [
    "class GroupViTVisionEmbeddings(nn.Module):\n",
    "    def __init__(self, config: GroupViTVisionConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embeddings = GroupViTPatchEmbeddings(\n",
    "            image_size=config.image_size,\n",
    "            patch_size=config.patch_size,\n",
    "            num_channels=config.num_channels,\n",
    "            embed_dim=config.hidden_size,\n",
    "        )\n",
    "        num_patches = self.patch_embeddings.num_patches\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches, config.hidden_size))\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.config = config\n",
    "\n",
    "    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher\n",
    "        resolution images.\n",
    "\n",
    "        Source:\n",
    "        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n",
    "        \"\"\"\n",
    "\n",
    "        npatch = embeddings.shape[1]\n",
    "        if npatch == self.position_embeddings.shape[1] and height == width:\n",
    "            return self.position_embeddings\n",
    "        patch_pos_embed = self.position_embeddings\n",
    "        num_original_pos_embed = patch_pos_embed.shape[1]\n",
    "        dim = embeddings.shape[-1]\n",
    "        feat_height = height // self.config.patch_size\n",
    "        feat_width = width // self.config.patch_size\n",
    "        # we add a small number to avoid floating point error in the interpolation\n",
    "        # see discussion at https://github.com/facebookresearch/dino/issues/8\n",
    "        feat_height, feat_width = feat_height + 0.1, feat_width + 0.1\n",
    "        original_height = original_width = math.sqrt(num_original_pos_embed)\n",
    "        reshaped_patch_pos_embed = patch_pos_embed.reshape(1, int(original_height), int(original_width), dim).permute(\n",
    "            0, 3, 1, 2\n",
    "        )\n",
    "        scale_factor = (feat_height / original_height, feat_width / original_width)\n",
    "        patch_pos_embed = nn.functional.interpolate(\n",
    "            reshaped_patch_pos_embed,\n",
    "            scale_factor=scale_factor,\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return patch_pos_embed\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor, interpolate_pos_encoding: bool = False) -> torch.Tensor:\n",
    "        batch_size, num_channels, height, width = pixel_values.shape\n",
    "        embeddings = self.patch_embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n",
    "\n",
    "        embeddings = self.layernorm(embeddings)\n",
    "\n",
    "        batch_size, seq_len, _ = embeddings.size()\n",
    "\n",
    "        # add positional encoding to each token\n",
    "        if interpolate_pos_encoding:\n",
    "            embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)\n",
    "        else:\n",
    "            embeddings = embeddings + self.position_embeddings\n",
    "\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217563,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "jZ_xSDus-ULA"
   },
   "outputs": [],
   "source": [
    "# Copied from transformers.models.clip.modeling_clip.CLIPTextEmbeddings with CLIP->GroupViT\n",
    "class GroupViTTextEmbeddings(nn.Module):\n",
    "    def __init__(self, config: GroupViTTextConfig):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(config.max_position_embeddings, embed_dim)\n",
    "\n",
    "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
    "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        seq_length = input_ids.shape[-1] if input_ids is not None else inputs_embeds.shape[-2]\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, :seq_length]\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.token_embedding(input_ids)\n",
    "\n",
    "        position_embeddings = self.position_embedding(position_ids)\n",
    "        embeddings = inputs_embeds + position_embeddings\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217563,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "WI0LJqF--anN"
   },
   "outputs": [],
   "source": [
    "class GroupViTStage(nn.Module):\n",
    "    \"\"\"This corresponds to the `GroupingLayer` class in the GroupViT implementation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: GroupViTVisionConfig,\n",
    "        depth: int,\n",
    "        num_prev_group_token: int,\n",
    "        num_group_token: int,\n",
    "        num_output_group: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.num_group_token = num_group_token\n",
    "        if num_group_token > 0:\n",
    "            self.group_token = nn.Parameter(torch.zeros(1, num_group_token, config.hidden_size))\n",
    "        else:\n",
    "            self.group_token = None\n",
    "        self.gradient_checkpointing = False\n",
    "        self.layers = nn.ModuleList([GroupViTEncoderLayer(config) for _ in range(depth)])\n",
    "\n",
    "        if num_group_token > 0:\n",
    "            self.downsample = GroupViTTokenAssign(\n",
    "                config=config,\n",
    "                num_group_token=num_group_token,\n",
    "                num_output_group=num_output_group,\n",
    "            )\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "        if num_prev_group_token > 0 and num_group_token > 0:\n",
    "            self.group_projector = nn.Sequential(\n",
    "                nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps),\n",
    "                GroupViTMixerMLP(config, num_prev_group_token, config.hidden_size // 2, num_group_token),\n",
    "            )\n",
    "        else:\n",
    "            self.group_projector = None\n",
    "\n",
    "    @property\n",
    "    def with_group_token(self):\n",
    "        return self.group_token is not None\n",
    "\n",
    "    def split_x(self, x):\n",
    "        if self.with_group_token:\n",
    "            return x[:, : -self.num_group_token], x[:, -self.num_group_token :]\n",
    "        else:\n",
    "            return x, None\n",
    "\n",
    "    def concat_x(self, x: torch.Tensor, group_token: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        if group_token is None:\n",
    "            return x\n",
    "        return torch.cat([x, group_token], dim=1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        prev_group_token: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.FloatTensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`): attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "                `(config.encoder_attention_heads,)`.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the grouping tensors of Grouping block.\n",
    "        \"\"\"\n",
    "        if self.with_group_token:\n",
    "            group_token = self.group_token.expand(hidden_states.size(0), -1, -1)\n",
    "            if self.group_projector is not None:\n",
    "                group_token = group_token + self.group_projector(prev_group_token)\n",
    "        else:\n",
    "            group_token = None\n",
    "\n",
    "        x = hidden_states\n",
    "\n",
    "        cat_x = self.concat_x(x, group_token)\n",
    "        for layer in self.layers:\n",
    "            layer_out = layer(cat_x, attention_mask=None, causal_attention_mask=None)\n",
    "            cat_x = layer_out[0]\n",
    "\n",
    "        x, group_token = self.split_x(cat_x)\n",
    "\n",
    "        attention = None\n",
    "        if self.downsample is not None:\n",
    "            x, attention = self.downsample(x, group_token)\n",
    "\n",
    "        outputs = (x, group_token)\n",
    "        if output_attentions:\n",
    "            outputs = outputs + (attention,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217563,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "lZNeThbG-bdS"
   },
   "outputs": [],
   "source": [
    "class GroupViTMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: GroupViTVisionConfig,\n",
    "        hidden_size: Optional[int] = None,\n",
    "        intermediate_size: Optional[int] = None,\n",
    "        output_size: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.activation_fn = ACT2FN[config.hidden_act]\n",
    "        hidden_size = hidden_size if hidden_size is not None else config.hidden_size\n",
    "        intermediate_size = intermediate_size if intermediate_size is not None else config.intermediate_size\n",
    "        output_size = output_size if output_size is not None else hidden_size\n",
    "        self.fc1 = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.fc2 = nn.Linear(intermediate_size, output_size)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.fc1(hidden_states)\n",
    "        hidden_states = self.activation_fn(hidden_states)\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217563,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "F5PfLQe2-dnX"
   },
   "outputs": [],
   "source": [
    "class GroupViTMixerMLP(GroupViTMLP):\n",
    "    def forward(self, x):\n",
    "        x = super().forward(x.transpose(1, 2))\n",
    "        return x.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217564,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "zIDEv8MG-fPE"
   },
   "outputs": [],
   "source": [
    "class GroupViTAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        if self.head_dim * self.num_heads != self.embed_dim:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n",
    "                f\" {self.num_heads}).\"\n",
    "            )\n",
    "        self.scale = self.head_dim**-0.5\n",
    "        self.dropout = config.attention_dropout\n",
    "\n",
    "        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
    "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        causal_attention_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
    "\n",
    "        bsz, tgt_len, embed_dim = hidden_states.size()\n",
    "        is_cross_attention = encoder_hidden_states is not None\n",
    "\n",
    "        # get query proj\n",
    "        query_states = self.q_proj(hidden_states) * self.scale\n",
    "        if is_cross_attention:\n",
    "            key_states = self._shape(self.k_proj(encoder_hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(encoder_hidden_states), -1, bsz)\n",
    "        else:\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "\n",
    "        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
    "        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
    "        key_states = key_states.view(*proj_shape)\n",
    "        value_states = value_states.view(*proj_shape)\n",
    "\n",
    "        src_len = key_states.size(1)\n",
    "        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
    "\n",
    "        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n",
    "                f\" {attn_weights.size()}\"\n",
    "            )\n",
    "\n",
    "        # apply the causal_attention_mask first\n",
    "        if causal_attention_mask is not None:\n",
    "            if causal_attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is\"\n",
    "                    f\" {causal_attention_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        if output_attentions:\n",
    "            # this operation is a bit akward, but it's required to\n",
    "            # make sure that attn_weights keeps its gradient.\n",
    "            # In order to do so, attn_weights have to reshaped\n",
    "            # twice and have to be reused in the following\n",
    "            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "        else:\n",
    "            attn_weights_reshaped = None\n",
    "\n",
    "        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "\n",
    "        attn_output = torch.bmm(attn_probs, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "        attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
    "\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights_reshaped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217564,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "MxxVauJA-ioM"
   },
   "outputs": [],
   "source": [
    "# Copied from transformers.models.clip.modeling_clip.CLIPEncoderLayer with CLIP->GroupViT\n",
    "class GroupViTEncoderLayer(nn.Module):\n",
    "    def __init__(self, config: GroupViTConfig):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.self_attn = GroupViTAttention(config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n",
    "        self.mlp = GroupViTMLP(config)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        causal_attention_mask: torch.Tensor,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.FloatTensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`): attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "                `(config.encoder_attention_heads,)`.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "        \"\"\"\n",
    "        residual = hidden_states\n",
    "\n",
    "        hidden_states = self.layer_norm1(hidden_states)\n",
    "        hidden_states, attn_weights = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            causal_attention_mask=causal_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.layer_norm2(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs += (attn_weights,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217564,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "iFAIWiAZ-k9h"
   },
   "outputs": [],
   "source": [
    "class GroupViTPreTrainedModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
    "    models.\n",
    "    \"\"\"\n",
    "\n",
    "    config_class = GroupViTConfig\n",
    "    base_model_prefix = \"groupvit\"\n",
    "    supports_gradient_checkpointing = True\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "\n",
    "        init_range = self.config.initializer_range\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=init_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "        factor = self.config.initializer_factor\n",
    "        if isinstance(module, GroupViTTextEmbeddings):\n",
    "            module.token_embedding.weight.data.normal_(mean=0.0, std=factor * 0.02)\n",
    "            module.position_embedding.weight.data.normal_(mean=0.0, std=factor * 0.02)\n",
    "        elif isinstance(module, GroupViTAttention):\n",
    "            factor = self.config.initializer_factor\n",
    "            in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n",
    "            out_proj_std = (module.embed_dim**-0.5) * factor\n",
    "            nn.init.normal_(module.q_proj.weight, std=in_proj_std)\n",
    "            nn.init.normal_(module.k_proj.weight, std=in_proj_std)\n",
    "            nn.init.normal_(module.v_proj.weight, std=in_proj_std)\n",
    "            nn.init.normal_(module.out_proj.weight, std=out_proj_std)\n",
    "        elif isinstance(module, GroupViTMLP):\n",
    "            factor = self.config.initializer_factor\n",
    "            in_proj_std = (\n",
    "                (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n",
    "            )\n",
    "            fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n",
    "            nn.init.normal_(module.fc1.weight, std=fc_std)\n",
    "            nn.init.normal_(module.fc2.weight, std=in_proj_std)\n",
    "\n",
    "    def _set_gradient_checkpointing(self, module, value=False):\n",
    "        if isinstance(module, (GroupViTTextEncoder, GroupViTVisionEncoder)):\n",
    "            module.gradient_checkpointing = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217564,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "jzru1jUe-wAJ"
   },
   "outputs": [],
   "source": [
    "GROUPVIT_START_DOCSTRING = r\"\"\"\n",
    "    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n",
    "    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n",
    "    behavior.\n",
    "\n",
    "    Parameters:\n",
    "        config ([`GroupViTConfig`]): Model configuration class with all the parameters of the model.\n",
    "            Initializing with a config file does not load the weights associated with the model, only the\n",
    "            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
    "\"\"\"\n",
    "\n",
    "GROUPVIT_TEXT_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n",
    "            it.\n",
    "\n",
    "            Indices can be obtained using [`CLIPTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
    "            [`PreTrainedTokenizer.__call__`] for details.\n",
    "\n",
    "            [What are input IDs?](../glossary#input-ids)\n",
    "        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "\n",
    "            [What are attention masks?](../glossary#attention-mask)\n",
    "        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n",
    "            config.max_position_embeddings - 1]`.\n",
    "\n",
    "            [What are position IDs?](../glossary#position-ids)\n",
    "        output_attentions (`bool`, *optional*):\n",
    "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
    "            tensors for more detail.\n",
    "        output_hidden_states (`bool`, *optional*):\n",
    "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
    "            more detail.\n",
    "        return_dict (`bool`, *optional*):\n",
    "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "\"\"\"\n",
    "\n",
    "GROUPVIT_VISION_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n",
    "            Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using\n",
    "            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details.\n",
    "        output_attentions (`bool`, *optional*):\n",
    "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
    "            tensors for more detail.\n",
    "        output_hidden_states (`bool`, *optional*):\n",
    "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
    "            more detail.\n",
    "        return_dict (`bool`, *optional*):\n",
    "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "\"\"\"\n",
    "\n",
    "GROUPVIT_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n",
    "            it.\n",
    "\n",
    "            Indices can be obtained using [`CLIPTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
    "            [`PreTrainedTokenizer.__call__`] for details.\n",
    "\n",
    "            [What are input IDs?](../glossary#input-ids)\n",
    "        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "\n",
    "            [What are attention masks?](../glossary#attention-mask)\n",
    "        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n",
    "            config.max_position_embeddings - 1]`.\n",
    "\n",
    "            [What are position IDs?](../glossary#position-ids)\n",
    "        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n",
    "            Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See\n",
    "            [`CLIPImageProcessor.__call__`] for details.\n",
    "        return_loss (`bool`, *optional*):\n",
    "            Whether or not to return the contrastive loss.\n",
    "        output_attentions (`bool`, *optional*):\n",
    "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
    "            tensors for more detail.\n",
    "        output_hidden_states (`bool`, *optional*):\n",
    "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
    "            more detail.\n",
    "        return_dict (`bool`, *optional*):\n",
    "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217564,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "diwTOUkc-yuM"
   },
   "outputs": [],
   "source": [
    "class GroupViTVisionEncoder(nn.Module):\n",
    "    def __init__(self, config: GroupViTVisionConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.stages = nn.ModuleList(\n",
    "            [\n",
    "                GroupViTStage(\n",
    "                    config=config,\n",
    "                    depth=config.depths[i],\n",
    "                    num_group_token=config.num_group_tokens[i],\n",
    "                    num_output_group=config.num_output_groups[i],\n",
    "                    num_prev_group_token=config.num_output_groups[i - 1] if i > 0 else 0,\n",
    "                )\n",
    "                for i in range(len(config.depths))\n",
    "            ]\n",
    "        )\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[tuple, BaseModelOutput]:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_groupings = () if output_attentions else None\n",
    "\n",
    "        group_tokens = None\n",
    "\n",
    "        for i, stage in enumerate(self.stages):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_outputs = stage(hidden_states, group_tokens, output_attentions)\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            group_tokens = layer_outputs[1]\n",
    "\n",
    "            if output_attentions and layer_outputs[2] is not None:\n",
    "                all_groupings = all_groupings + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, all_hidden_states, all_groupings] if v is not None)\n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=hidden_states, hidden_states=all_hidden_states, attentions=all_groupings\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217565,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "ErgnsQgL-zvh"
   },
   "outputs": [],
   "source": [
    "class GroupViTTextEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder consisting of `config.num_hidden_layers` self-attention layers. Each layer is a\n",
    "    [`GroupViTEncoderLayer`].\n",
    "\n",
    "    Args:\n",
    "        config: GroupViTTextConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: GroupViTTextConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layers = nn.ModuleList([GroupViTEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        inputs_embeds,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        causal_attention_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutput]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n",
    "                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n",
    "                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n",
    "                than the model's internal embedding lookup matrix.\n",
    "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            causal_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Causal mask for the text model. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            output_hidden_states (`bool`, *optional*):\n",
    "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
    "                for more detail.\n",
    "            return_dict (`bool`, *optional*):\n",
    "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        encoder_states = () if output_hidden_states else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "        for idx, encoder_layer in enumerate(self.layers):\n",
    "            if output_hidden_states:\n",
    "                encoder_states = encoder_states + (hidden_states,)\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(encoder_layer),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    causal_attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = encoder_layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    causal_attention_mask,\n",
    "                    output_attentions=output_attentions,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            encoder_states = encoder_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217566,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "fsgeLvGW_gB0"
   },
   "outputs": [],
   "source": [
    "# Copied from transformers.models.clip.modeling_clip.CLIPTextTransformer with CLIPText->GroupViTText, CLIPEncoder->GroupViTTextEncoder, CLIP_TEXT->GROUPVIT_TEXT\n",
    "class GroupViTTextTransformer(nn.Module):\n",
    "    def __init__(self, config: GroupViTTextConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        embed_dim = config.hidden_size\n",
    "        self.embeddings = GroupViTTextEmbeddings(config)\n",
    "        self.encoder = GroupViTTextEncoder(config)\n",
    "        self.final_layer_norm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(GROUPVIT_TEXT_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=GroupViTTextConfig)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if input_ids is None:\n",
    "            raise ValueError(\"You have to specify input_ids\")\n",
    "\n",
    "        input_shape = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, input_shape[-1])\n",
    "\n",
    "        hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n",
    "\n",
    "        bsz, seq_len = input_shape\n",
    "        # CLIP's text model uses causal mask, prepare it here.\n",
    "        # https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/clip/model.py#L324\n",
    "        causal_attention_mask = self._build_causal_attention_mask(bsz, seq_len, hidden_states.dtype).to(\n",
    "            hidden_states.device\n",
    "        )\n",
    "        # expand attention_mask\n",
    "        if attention_mask is not None:\n",
    "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "            attention_mask = _expand_mask(attention_mask, hidden_states.dtype)\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            inputs_embeds=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            causal_attention_mask=causal_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        last_hidden_state = encoder_outputs[0]\n",
    "        last_hidden_state = self.final_layer_norm(last_hidden_state)\n",
    "\n",
    "        # text_embeds.shape = [batch_size, sequence_length, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        # casting to torch.int for onnx compatibility: argmax doesn't support int64 inputs with opset 14\n",
    "        pooled_output = last_hidden_state[\n",
    "            torch.arange(last_hidden_state.shape[0], device=last_hidden_state.device),\n",
    "            input_ids.to(dtype=torch.int, device=last_hidden_state.device).argmax(dim=-1),\n",
    "        ]\n",
    "\n",
    "        if not return_dict:\n",
    "            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPooling(\n",
    "            last_hidden_state=last_hidden_state,\n",
    "            pooler_output=pooled_output,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def _build_causal_attention_mask(self, bsz, seq_len, dtype):\n",
    "        # lazily create causal attention mask, with full attention between the vision tokens\n",
    "        # pytorch uses additive attention mask; fill with -inf\n",
    "        mask = torch.empty(bsz, seq_len, seq_len, dtype=dtype)\n",
    "        mask.fill_(torch.tensor(torch.finfo(dtype).min))\n",
    "        mask.triu_(1)  # zero out the lower diagonal\n",
    "        mask = mask.unsqueeze(1)  # expand mask\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 1409,
     "status": "ok",
     "timestamp": 1677512218961,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "QAAhPken_ibc"
   },
   "outputs": [],
   "source": [
    "class GroupViTTextModel(GroupViTPreTrainedModel):\n",
    "    config_class = GroupViTTextConfig\n",
    "\n",
    "    def __init__(self, config: GroupViTTextConfig):\n",
    "        super().__init__(config)\n",
    "        self.text_model = GroupViTTextTransformer(config)\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self) -> nn.Module:\n",
    "        return self.text_model.embeddings.token_embedding\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.text_model.embeddings.token_embedding = value\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(GROUPVIT_TEXT_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=GroupViTTextConfig)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import CLIPTokenizer, GroupViTTextModel\n",
    "\n",
    "        >>> tokenizer = CLIPTokenizer.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "        >>> model = GroupViTTextModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "\n",
    "        >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n",
    "\n",
    "        >>> outputs = model(**inputs)\n",
    "        >>> last_hidden_state = outputs.last_hidden_state\n",
    "        >>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n",
    "        ```\"\"\"\n",
    "        return self.text_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1677512218961,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "pNJ4HozD_ls2"
   },
   "outputs": [],
   "source": [
    "class GroupViTVisionTransformer(nn.Module):########################\n",
    "    def __init__(self, config: GroupViTVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        embed_dim = config.hidden_size\n",
    "\n",
    "        self.embeddings = GroupViTVisionEmbeddings(config)\n",
    "        self.encoder = GroupViTVisionEncoder(config)\n",
    "        self.layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(GROUPVIT_VISION_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=GroupViTVisionConfig)\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if pixel_values is None:\n",
    "            raise ValueError(\"You have to specify pixel_values\")\n",
    "\n",
    "        hidden_states = self.embeddings(pixel_values)\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            hidden_states=hidden_states,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            output_attentions=output_attentions,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        last_hidden_state = encoder_outputs[0]\n",
    "\n",
    "        # normalize the last hidden state\n",
    "        last_hidden_state = self.layernorm(last_hidden_state)\n",
    "        pooled_output = last_hidden_state.mean(dim=1)\n",
    "\n",
    "        if not return_dict:\n",
    "            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPooling(\n",
    "            last_hidden_state=last_hidden_state,\n",
    "            pooler_output=pooled_output,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1677512218961,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "Tf5zCFfp_pS0"
   },
   "outputs": [],
   "source": [
    "class GroupViTVisionModel(GroupViTPreTrainedModel):\n",
    "    config_class = GroupViTVisionConfig\n",
    "    main_input_name = \"pixel_values\"\n",
    "\n",
    "    def __init__(self, config: GroupViTVisionConfig, projection_dim=128):\n",
    "        super().__init__(config)\n",
    "        self.vision_model = GroupViTVisionTransformer(config)\n",
    "\n",
    "        self.projection_dim = projection_dim\n",
    "        self.projection_intermediate_dim = 4096\n",
    "        self.vision_embed_dim = config.hidden_size\n",
    "\n",
    "        self.visual_projection = nn.Sequential(\n",
    "            nn.Linear(self.vision_embed_dim, self.projection_intermediate_dim, bias=True),\n",
    "            nn.BatchNorm1d(self.projection_intermediate_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.projection_intermediate_dim, self.projection_dim, bias=True),\n",
    "        )\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self) -> GroupViTPatchEmbeddings:\n",
    "        return self.vision_model.embeddings.patch_embeddings\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(GROUPVIT_VISION_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=GroupViTVisionConfig)\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from PIL import Image\n",
    "        >>> import requests\n",
    "        >>> from transformers import AutoProcessor, GroupViTVisionModel\n",
    "\n",
    "        >>> processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "        >>> model = GroupViTVisionModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "\n",
    "        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "        >>> image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "        >>> inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "        >>> outputs = model(**inputs)\n",
    "        >>> last_hidden_state = outputs.last_hidden_state\n",
    "        >>> pooled_output = outputs.pooler_output  # pooled CLS states\n",
    "        ```\"\"\"\n",
    "\n",
    "        # print(type(pixel_values), type(output_attentions), type(output_hidden_states), type(return_dict))\n",
    "        # print(pixel_values.shape, output_attentions, output_hidden_states, return_dict)\n",
    "        # print('pixel_values=', pixel_values.shape)\n",
    "        output_attentions = True\n",
    "        output_hidden_states = False\n",
    "        return_dict = True\n",
    "        # print(pixel_values.shape, output_attentions, output_hidden_states, return_dict)\n",
    "        vision_outputs = self.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        # print(vision_outputs)\n",
    "\n",
    "        attentions = vision_outputs[2]\n",
    "            \n",
    "        # [batch_size_image, num_group, height, width]\n",
    "        grouping = get_grouping_from_attentions(attentions, pixel_values.shape[2:])\n",
    "        seg_logits = grouping\n",
    "\n",
    "        pooled_output = vision_outputs[1]  # pooled_output\n",
    "        image_features = self.visual_projection(pooled_output)\n",
    "\n",
    "        # print(image_features.shape)\n",
    "        return vision_outputs, seg_logits, image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1677512218962,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "Q3lIeXoH_slB"
   },
   "outputs": [],
   "source": [
    "@add_start_docstrings(GROUPVIT_START_DOCSTRING)\n",
    "class GroupViTModel(GroupViTPreTrainedModel):\n",
    "    config_class = GroupViTConfig\n",
    "\n",
    "    def __init__(self, config: GroupViTConfig):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # if not isinstance(config.text_config, GroupViTTextConfig):\n",
    "        #     raise ValueError(\n",
    "        #         \"config.text_config is expected to be of type GroupViTTextConfig but is of type\"\n",
    "        #         f\" {type(config.text_config)}.\"\n",
    "        #     )\n",
    "\n",
    "        if not isinstance(config.vision_config, GroupViTVisionConfig):\n",
    "            raise ValueError(\n",
    "                \"config.vision_config is expected to be of type GroupViTVisionConfig but is of type\"\n",
    "                f\" {type(config.vision_config)}.\"\n",
    "            )\n",
    "\n",
    "        # text_config = config.text_config\n",
    "        vision_config = config.vision_config\n",
    "\n",
    "        self.projection_dim = config.projection_dim\n",
    "        self.projection_intermediate_dim = config.projection_intermediate_dim\n",
    "        # self.text_embed_dim = text_config.hidden_size\n",
    "        self.vision_embed_dim = vision_config.hidden_size\n",
    "        print('hidden_size', vision_config.hidden_size)\n",
    "\n",
    "        # self.text_model = GroupViTTextTransformer(text_config)\n",
    "        self.vision_model = GroupViTVisionTransformer(vision_config)\n",
    "\n",
    "        self.visual_projection = nn.Sequential(\n",
    "            nn.Linear(self.vision_embed_dim, self.projection_intermediate_dim, bias=True),\n",
    "            nn.BatchNorm1d(self.projection_intermediate_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.projection_intermediate_dim, self.projection_dim, bias=True),\n",
    "        )\n",
    "        # self.text_projection = nn.Sequential(\n",
    "        #     nn.Linear(self.text_embed_dim, self.projection_intermediate_dim, bias=True),\n",
    "        #     nn.BatchNorm1d(self.projection_intermediate_dim),\n",
    "        #     nn.ReLU(inplace=True),\n",
    "        #     nn.Linear(self.projection_intermediate_dim, self.projection_dim, bias=True),\n",
    "        # )\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * self.config.logit_scale_init_value)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    # @add_start_docstrings_to_model_forward(GROUPVIT_TEXT_INPUTS_DOCSTRING)\n",
    "    # def get_text_features(\n",
    "    #     self,\n",
    "    #     input_ids: Optional[torch.Tensor] = None,\n",
    "    #     attention_mask: Optional[torch.Tensor] = None,\n",
    "    #     position_ids: Optional[torch.Tensor] = None,\n",
    "    #     output_attentions: Optional[bool] = None,\n",
    "    #     output_hidden_states: Optional[bool] = None,\n",
    "    #     return_dict: Optional[bool] = None,\n",
    "    # ) -> torch.FloatTensor:\n",
    "    #     r\"\"\"\n",
    "    #     Returns:\n",
    "    #         text_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The text embeddings obtained by\n",
    "    #         applying the projection layer to the pooled output of [`GroupViTTextModel`].\n",
    "\n",
    "    #     Examples:\n",
    "\n",
    "    #     ```python\n",
    "    #     >>> from transformers import CLIPTokenizer, GroupViTModel\n",
    "\n",
    "    #     >>> model = GroupViTModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "    #     >>> tokenizer = CLIPTokenizer.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "\n",
    "    #     >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n",
    "    #     >>> text_features = model.get_text_features(**inputs)\n",
    "    #     ```\"\"\"\n",
    "    #     # Use GROUPVIT model's config for some fields (if specified) instead of those of vision & text components.\n",
    "    #     output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "    #     output_hidden_states = (\n",
    "    #         output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "    #     )\n",
    "    #     return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "    #     text_outputs = self.text_model(\n",
    "    #         input_ids=input_ids,\n",
    "    #         attention_mask=attention_mask,\n",
    "    #         position_ids=position_ids,\n",
    "    #         output_attentions=output_attentions,\n",
    "    #         output_hidden_states=output_hidden_states,\n",
    "    #         return_dict=return_dict,\n",
    "    #     )\n",
    "\n",
    "    #     pooled_output = text_outputs[1]\n",
    "    #     text_features = self.text_projection(pooled_output)\n",
    "\n",
    "    #     return text_features\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(GROUPVIT_VISION_INPUTS_DOCSTRING)\n",
    "    def get_image_features(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> torch.FloatTensor:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "            image_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The image embeddings obtained by\n",
    "            applying the projection layer to the pooled output of [`GroupViTVisionModel`].\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from PIL import Image\n",
    "        >>> import requests\n",
    "        >>> from transformers import AutoProcessor, GroupViTModel\n",
    "\n",
    "        >>> model = GroupViTModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "        >>> processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "\n",
    "        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "        >>> image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "        >>> inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "        >>> image_features = model.get_image_features(**inputs)\n",
    "        ```\"\"\"\n",
    "        # Use GROUPVIT model's config for some fields (if specified) instead of those of vision & text components.\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        print(pixel_values.shape, output_attentions, output_hidden_states, return_dict)\n",
    "\n",
    "        vision_outputs = self.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = vision_outputs[1]  # pooled_output\n",
    "        print('01 ', pooled_output.shape)\n",
    "\n",
    "        image_features = self.visual_projection(pooled_output)\n",
    "        print('02 ', image_features.shape)\n",
    "\n",
    "        return image_features\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(GROUPVIT_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=GroupViTModelOutput, config_class=GroupViTConfig)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        return_loss: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_segmentation: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, GroupViTModelOutput]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from PIL import Image\n",
    "        >>> import requests\n",
    "        >>> from transformers import AutoProcessor, GroupViTModel\n",
    "\n",
    "        >>> model = GroupViTModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "        >>> processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "\n",
    "        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "        >>> image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "        >>> inputs = processor(\n",
    "        ...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True\n",
    "        ... )\n",
    "\n",
    "        >>> outputs = model(**inputs)\n",
    "        >>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "        >>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n",
    "        ```\"\"\"\n",
    "        # Use GROUPVIT model's config for some fields (if specified) instead of those of vision & text components.\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_segmentation = (\n",
    "            output_segmentation if output_segmentation is not None else self.config.output_segmentation\n",
    "        )\n",
    "        if output_segmentation:\n",
    "            output_attentions = True\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # print(type(pixel_values), type(output_attentions), type(output_hidden_states), type(return_dict))\n",
    "        print(pixel_values.shape, output_attentions, output_hidden_states, return_dict)\n",
    "\n",
    "        vision_outputs = self.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        # print(vision_outputs)\n",
    "\n",
    "        # text_outputs = self.text_model(\n",
    "        #     input_ids=input_ids,\n",
    "        #     attention_mask=attention_mask,\n",
    "        #     position_ids=position_ids,\n",
    "        #     output_attentions=output_attentions,\n",
    "        #     output_hidden_states=output_hidden_states,\n",
    "        #     return_dict=return_dict,\n",
    "        # )\n",
    "\n",
    "        image_embeds = vision_outputs[1]\n",
    "        image_embeds = self.visual_projection(image_embeds)\n",
    "\n",
    "        # text_embeds = text_outputs[1]\n",
    "        # text_embeds = self.text_projection(text_embeds)\n",
    "\n",
    "        # normalized features\n",
    "        image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n",
    "        # text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        # logit_scale = self.logit_scale.exp()\n",
    "        # logits_per_text = torch.matmul(text_embeds, image_embeds.t()) * logit_scale\n",
    "        # logits_per_image = logits_per_text.t()\n",
    "\n",
    "        seg_logits = None\n",
    "        if output_segmentation:\n",
    "            # grouped features\n",
    "            # [batch_size_image, num_group, hidden_size]\n",
    "            image_group_embeds = vision_outputs[0]\n",
    "            print('image_group_embeds_01', image_group_embeds.shape, type(image_group_embeds)) # torch.Size([1, 8, 384]) <class 'torch.Tensor'>\n",
    "\n",
    "            # [batch_size_image*num_group, hidden_size]\n",
    "            image_group_embeds = self.visual_projection(image_group_embeds.reshape(-1, image_group_embeds.shape[-1]))\n",
    "            print('image_group_embeds_02', image_group_embeds.shape, type(image_group_embeds)) # torch.Size([8, 256]) <class 'torch.Tensor'>\n",
    "\n",
    "            if output_hidden_states:\n",
    "                attentions = vision_outputs[3]\n",
    "                print('attentions_01', attentions.shape, type(attentions)) # *\n",
    "\n",
    "            else:\n",
    "                attentions = vision_outputs[2]\n",
    "                print('attentions_02', attentions[0].shape, type(attentions[0]), attentions[1].shape, type(attentions[1])) # torch.Size([1, 64, 196]) torch.Size([1, 8, 64]) <class 'torch.Tensor'>\n",
    "                \n",
    "            # [batch_size_image, num_group, height, width]\n",
    "            grouping = get_grouping_from_attentions(attentions, pixel_values.shape[2:])\n",
    "            print(pixel_values.shape)\n",
    "            print(pixel_values.shape[2:])\n",
    "            print('grouping_01', grouping.shape, type(grouping)) # torch.Size([1, 8, 224, 224]) <class 'torch.Tensor'>\n",
    "            seg_logits = grouping\n",
    "\n",
    "            # # normalized features\n",
    "            # image_group_embeds = image_group_embeds / image_group_embeds.norm(dim=-1, keepdim=True)\n",
    "            # print('image_group_embeds_03', image_group_embeds.shape, type(image_group_embeds)) # torch.Size([8, 256]) <class 'torch.Tensor'>\n",
    "\n",
    "            # # [batch_size_image x num_group, batch_size_text]\n",
    "            # logits_per_image_group = torch.matmul(image_group_embeds, text_embeds.t()) * logit_scale\n",
    "            # print('logits_per_image_group_01', logits_per_image_group.shape, type(logits_per_image_group)) # torch.Size([8, 3]) <class 'torch.Tensor'>\n",
    "\n",
    "            # # [batch_size_image, batch_size_text, num_group]\n",
    "            # logits_per_image_group = logits_per_image_group.reshape(\n",
    "            #     image_embeds.shape[0], -1, text_embeds.shape[0]\n",
    "            # ).permute(0, 2, 1)\n",
    "            # print('logits_per_image_group_02', logits_per_image_group.shape, type(logits_per_image_group)) # torch.Size([1, 3, 8]) <class 'torch.Tensor'>\n",
    "\n",
    "\n",
    "            # # [batch_size_image, batch_size_text, height x width]\n",
    "            # flatten_grouping = grouping.reshape(grouping.shape[0], grouping.shape[1], -1)\n",
    "            # print('flatten_grouping_01', flatten_grouping.shape, type(flatten_grouping)) # torch.Size([1, 8, 50176]) <class 'torch.Tensor'>\n",
    "\n",
    "\n",
    "            # # [batch_size_image, batch_size_text, height, width]\n",
    "            # seg_logits = torch.matmul(logits_per_image_group, flatten_grouping) * logit_scale\n",
    "            # print('seg_logits_01', seg_logits.shape, type(seg_logits)) # torch.Size([1, 3, 50176]) <class 'torch.Tensor'>\n",
    "\n",
    "            # seg_logits = seg_logits.reshape(\n",
    "            #     seg_logits.shape[0], seg_logits.shape[1], grouping.shape[2], grouping.shape[3]\n",
    "            # )\n",
    "            # print('seg_logits_02', seg_logits.shape, type(seg_logits)) # torch.Size([1, 3, 224, 224]) <class 'torch.Tensor'>\n",
    "\n",
    "        loss = None\n",
    "        if return_loss:\n",
    "            loss = groupvit_loss(logits_per_text)\n",
    "\n",
    "        if not return_dict:\n",
    "            if seg_logits is not None:\n",
    "                output = (\n",
    "                    logits_per_image,\n",
    "                    logits_per_text,\n",
    "                    seg_logits,\n",
    "                    text_embeds,\n",
    "                    image_embeds,\n",
    "                    text_outputs,\n",
    "                    vision_outputs,\n",
    "                )\n",
    "            else:\n",
    "                output = (logits_per_image, logits_per_text, text_embeds, image_embeds, text_outputs, vision_outputs)\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return GroupViTModelOutput(\n",
    "            loss=loss,\n",
    "            # logits_per_image=logits_per_image,\n",
    "            # logits_per_text=logits_per_text,\n",
    "            segmentation_logits=seg_logits,\n",
    "            # text_embeds=text_embeds,\n",
    "            image_embeds=image_embeds,\n",
    "            # text_model_output=text_outputs,\n",
    "            vision_model_output=vision_outputs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuwerq7N9s5S"
   },
   "source": [
    "### Set Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1677512218962,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "dvuxcmejkKt8",
    "outputId": "4e6311d9-6fd0-43ed-c40a-e5541e35f105"
   },
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description='Train MoCo on CIFAR-10')\n",
    "\n",
    "# parser.add_argument('--finetune_data_dir', default=os.path.join(ROOT_PATH, '../lab_06', 'fine-tune_set', 'siim-acr-pneumothorax'))\n",
    "# parser.add_argument('--pretrain_data_dir', default=os.path.join(ROOT_PATH, '../lab_05', 'unlabel_pre-training_set'))\n",
    "\n",
    "# parser.add_argument('--image_size', default=256, type=int)\n",
    "\n",
    "# parser.add_argument('--lr', '--learning-rate', default=0.06, type=float, metavar='LR', help='initial learning rate', dest='lr')\n",
    "# parser.add_argument('--epochs', default=200, type=int, metavar='N', help='number of total epochs to run')\n",
    "# parser.add_argument('--batch-size', default=4, type=int, metavar='N', help='mini-batch size')\n",
    "\n",
    "# '''\n",
    "# args = parser.parse_args()  # running in command line\n",
    "# '''\n",
    "# args = parser.parse_args('')  # running in ipynb\n",
    "\n",
    "\n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygQeHtsngrC8"
   },
   "source": [
    "### Dataset Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_length_decode(rle, height=1024, width=1024, fill_value=1):\n",
    "    component = np.zeros((height, width), np.float32)\n",
    "    component = component.reshape(-1)\n",
    "    rle = np.array([int(s) for s in rle.strip().split(' ')])\n",
    "    rle = rle.reshape(-1, 2)\n",
    "    start = 0\n",
    "    for index, length in rle:\n",
    "        start = start+index\n",
    "        end = start+length\n",
    "        component[start: end] = fill_value\n",
    "        start = end\n",
    "    component = component.reshape(width, height).T\n",
    "    return component\n",
    "\n",
    "def run_length_encode(component):\n",
    "    component = component.T.flatten()\n",
    "    start = np.where(component[1:] > component[:-1])[0]+1\n",
    "    end = np.where(component[:-1] > component[1:])[0]+1\n",
    "    length = end-start\n",
    "    rle = []\n",
    "    for i in range(len(length)):\n",
    "        if i == 0:\n",
    "            rle.extend([start[0], length[0]])\n",
    "        else:\n",
    "            rle.extend([start[i]-end[i-1], length[i]])\n",
    "    rle = ' '.join([str(r) for r in rle])\n",
    "    return rle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(phase, size, mean, std):\n",
    "    list_transforms = []\n",
    "    if phase == \"train\":\n",
    "        list_transforms.extend(\n",
    "            [\n",
    "#                 HorizontalFlip(),\n",
    "                ShiftScaleRotate(\n",
    "                    shift_limit=0,  # no resizing\n",
    "                    scale_limit=0.1,\n",
    "                    rotate_limit=10, # rotate\n",
    "                    p=0.5,\n",
    "                    border_mode=cv2.BORDER_CONSTANT\n",
    "                ),\n",
    "#                 GaussNoise(),\n",
    "            ]\n",
    "        )\n",
    "    list_transforms.extend(\n",
    "        [\n",
    "            Resize(size, size),\n",
    "            Normalize(mean=mean, std=std, p=1),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    list_trfms = Compose(list_transforms)\n",
    "    return list_trfms\n",
    "\n",
    "def provider(\n",
    "    fold,\n",
    "    total_folds,\n",
    "    data_folder,\n",
    "    df_path,\n",
    "    phase,\n",
    "    size,\n",
    "    mean=None,\n",
    "    std=None,\n",
    "    batch_size=8,\n",
    "    num_workers=4,\n",
    "):\n",
    "    df_all = pd.read_csv(df_path)\n",
    "    df = df_all.drop_duplicates('ImageId')\n",
    "    df_with_mask = df[df[\" EncodedPixels\"] != \" -1\"]\n",
    "    df_with_mask['has_mask'] = 1\n",
    "    df_without_mask = df[df[\" EncodedPixels\"] == \" -1\"]\n",
    "    df_without_mask['has_mask'] = 0\n",
    "    df_without_mask_sampled = df_without_mask.sample(len(df_with_mask), random_state=69) # random state is imp\n",
    "    df = pd.concat([df_with_mask, df_without_mask_sampled])\n",
    "    \n",
    "    #NOTE: equal number of positive and negative cases are chosen.\n",
    "    \n",
    "    kfold = StratifiedKFold(total_folds, shuffle=True, random_state=69)\n",
    "    train_idx, val_idx = list(kfold.split(df[\"ImageId\"], df[\"has_mask\"]))[fold]\n",
    "    train_df, val_df = df.iloc[train_idx], df.iloc[val_idx]\n",
    "    df = train_df if phase == \"train\" else val_df\n",
    "    # NOTE: total_folds=5 -> train/val : 80%/20%\n",
    "    \n",
    "    fnames = df['ImageId'].values\n",
    "    \n",
    "    image_dataset = SIIMDataset(df_all, fnames, data_folder, size, mean, std, phase)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        image_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,\n",
    "        generator=torch.Generator(device='cuda')\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get from path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader = provider(\n",
    "#     fold=0,\n",
    "#     total_folds=5,\n",
    "#     data_folder=data_folder,\n",
    "#     df_path=train_rle_path,\n",
    "#     phase=\"train\",\n",
    "#     size=512,\n",
    "#     mean = (0.485, 0.456, 0.406),\n",
    "#     std = (0.229, 0.224, 0.225),\n",
    "#     batch_size=16,\n",
    "#     num_workers=4,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(dataloader)) # get a batch from the dataloader\n",
    "# images, masks = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot some random images in the `batch`\n",
    "# idx = random.choice(range(16))\n",
    "# plt.imshow(np.asarray(images[idx]).transpose(1, 2, 0))\n",
    "# plt.imshow(masks[idx][0], alpha=0.2, cmap='Reds')\n",
    "# plt.show()\n",
    "# if len(np.unique(masks[idx][0])) == 1: # only zeros\n",
    "#     print('Chosen image has no ground truth mask, rerun the cell')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set dataset & loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SIIMDataset(Dataset):\n",
    "    def __init__(self, df, fnames, data_folder, size, mean, std, phase):\n",
    "        self.df = df\n",
    "        self.root = data_folder\n",
    "        self.size = size\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.phase = phase\n",
    "        self.transforms = get_transforms(phase, size, mean, std)\n",
    "        self.gb = self.df.groupby('ImageId')\n",
    "        self.fnames = fnames\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.fnames[idx]\n",
    "        df = self.gb.get_group(image_id)\n",
    "        annotations = df[' EncodedPixels'].tolist()\n",
    "        image_path = os.path.join(self.root, 'segmix_' + image_id + \".png\")\n",
    "        image = cv2.imread(image_path)\n",
    "        mask = np.zeros([1024, 1024])\n",
    "        if annotations[0] != ' -1':\n",
    "            for rle in annotations:\n",
    "                mask += run_length_decode(rle)\n",
    "        mask = (mask >= 1).astype('float32') # for overlap cases\n",
    "        augmented = self.transforms(image=image, mask=mask)\n",
    "        image = augmented['image']\n",
    "        mask = augmented['mask']\n",
    "        return image, mask.unsqueeze(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsAVAtRoiBbG"
   },
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define base encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(\"resnet34\", encoder_weights=\"imagenet\", activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ModelBase(configuration=configuration)\n",
    "\n",
    "# t = torch.randn((32,3,512,512))\n",
    "# print(t.shape)\n",
    "# print(model(t).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define MoCo wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YXcpXBwi8KV"
   },
   "source": [
    "### Define train/test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(input, target):\n",
    "    input = torch.sigmoid(input)\n",
    "    smooth = 1.0\n",
    "    iflat = input.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    return ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if not (target.size() == input.size()):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n",
    "                             .format(target.size(), input.size()))\n",
    "        max_val = (-input).clamp(min=0)\n",
    "        loss = input - input * target + max_val + \\\n",
    "            ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class MixedLoss(nn.Module):\n",
    "    def __init__(self, alpha, gamma):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.focal = FocalLoss(gamma)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        loss = self.alpha*self.focal(input, target) - torch.log(dice_loss(input, target))\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define train & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, threshold):\n",
    "    X_p = np.copy(X)\n",
    "    preds = (X_p > threshold).astype('uint8')\n",
    "    return preds\n",
    "\n",
    "def metric(probability, truth, threshold=0.5, reduction='none'):\n",
    "    '''Calculates dice of positive and negative images seperately'''\n",
    "    '''probability and truth must be torch tensors'''\n",
    "    batch_size = len(truth)\n",
    "    with torch.no_grad():\n",
    "        probability = probability.view(batch_size, -1)\n",
    "        truth = truth.view(batch_size, -1)\n",
    "        assert(probability.shape == truth.shape)\n",
    "\n",
    "        p = (probability > threshold).float()\n",
    "        t = (truth > 0.5).float()\n",
    "\n",
    "        t_sum = t.sum(-1)\n",
    "        p_sum = p.sum(-1)\n",
    "        neg_index = torch.nonzero(t_sum == 0)\n",
    "        pos_index = torch.nonzero(t_sum >= 1)\n",
    "\n",
    "        dice_neg = (p_sum == 0).float()\n",
    "        dice_pos = 2 * (p*t).sum(-1)/((p+t).sum(-1))\n",
    "\n",
    "        dice_neg = dice_neg[neg_index]\n",
    "        dice_pos = dice_pos[pos_index]\n",
    "        dice = torch.cat([dice_pos, dice_neg])\n",
    "\n",
    "#         dice_neg = np.nan_to_num(dice_neg.mean().item(), 0)\n",
    "#         dice_pos = np.nan_to_num(dice_pos.mean().item(), 0)\n",
    "#         dice = dice.mean().item()\n",
    "\n",
    "        num_neg = len(neg_index)\n",
    "        num_pos = len(pos_index)\n",
    "\n",
    "    return dice, dice_neg, dice_pos, num_neg, num_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Meter:\n",
    "    '''A meter to keep track of iou and dice scores throughout an epoch'''\n",
    "    def __init__(self, phase, epoch):\n",
    "        self.base_threshold = 0.5 # <<<<<<<<<<< here's the threshold\n",
    "        self.base_dice_scores = []\n",
    "        self.dice_neg_scores = []\n",
    "        self.dice_pos_scores = []\n",
    "        self.iou_scores = []\n",
    "\n",
    "    def update(self, targets, outputs):\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        dice, dice_neg, dice_pos, _, _ = metric(probs, targets, self.base_threshold)\n",
    "        self.base_dice_scores.extend(dice)\n",
    "        self.dice_pos_scores.extend(dice_pos)\n",
    "        self.dice_neg_scores.extend(dice_neg)\n",
    "        preds = predict(probs, self.base_threshold)\n",
    "        iou = compute_iou_batch(preds, targets, classes=[1])\n",
    "        self.iou_scores.append(iou)\n",
    "\n",
    "    def get_metrics(self):\n",
    "        dice = np.nanmean(self.base_dice_scores)\n",
    "        dice_neg = np.nanmean(self.dice_neg_scores)\n",
    "        dice_pos = np.nanmean(self.dice_pos_scores)\n",
    "        dices = [dice, dice_neg, dice_pos]\n",
    "        iou = np.nanmean(self.iou_scores)\n",
    "        return dices, iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_log(phase, epoch, epoch_loss, meter, start):\n",
    "    '''logging the metrics at the end of an epoch'''\n",
    "    dices, iou = meter.get_metrics()\n",
    "    dice, dice_neg, dice_pos = dices\n",
    "    print(\"Loss: %0.4f | dice: %0.4f | dice_neg: %0.4f | dice_pos: %0.4f | IoU: %0.4f\" % (epoch_loss, dice, dice_neg, dice_pos, iou))\n",
    "    return dice, iou\n",
    "\n",
    "def compute_ious(pred, label, classes, ignore_index=255, only_present=True):\n",
    "    '''computes iou for one ground truth mask and predicted mask'''\n",
    "    pred[label == ignore_index] = 0\n",
    "    ious = []\n",
    "    for c in classes:\n",
    "        label_c = label == c\n",
    "        if only_present and np.sum(label_c) == 0:\n",
    "            ious.append(np.nan)\n",
    "            continue\n",
    "        pred_c = pred == c\n",
    "        intersection = np.logical_and(pred_c, label_c).sum()\n",
    "        union = np.logical_or(pred_c, label_c).sum()\n",
    "        if union != 0:\n",
    "            ious.append(intersection / union)\n",
    "    return ious if ious else [1]\n",
    "\n",
    "\n",
    "def compute_iou_batch(outputs, labels, classes=None):\n",
    "    '''computes mean iou for a batch of ground truth masks and predicted masks'''\n",
    "    ious = []\n",
    "    preds = np.copy(outputs) # copy is imp\n",
    "    labels = np.array(labels) # tensor to np\n",
    "    for pred, label in zip(preds, labels):\n",
    "        ious.append(np.nanmean(compute_ious(pred, label, classes)))\n",
    "    iou = np.nanmean(ious)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    '''This class takes care of training and validation of our model'''\n",
    "    def __init__(self, model):\n",
    "        self.fold = 1\n",
    "        self.total_folds = 5\n",
    "        self.num_workers = 6\n",
    "        self.batch_size = {\"train\": 8, \"val\": 4}\n",
    "        self.accumulation_steps = 32 // self.batch_size['train']\n",
    "        self.lr = 5e-4\n",
    "        self.num_epochs = 100\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.phases = [\"train\", \"val\"]\n",
    "        self.device = torch.device(\"cuda:0\")\n",
    "        torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "        self.net = model\n",
    "        self.criterion = MixedLoss(10.0, 2.0)\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode=\"min\", patience=3, verbose=True)\n",
    "        self.net = self.net.to(self.device)\n",
    "        cudnn.benchmark = True\n",
    "        self.dataloaders = {\n",
    "            phase: provider(\n",
    "                fold=1,\n",
    "                total_folds=5,\n",
    "                data_folder=data_folder,\n",
    "                df_path=train_rle_path,\n",
    "                phase=phase,\n",
    "                size=1024,\n",
    "                mean=(0.485, 0.456, 0.406),\n",
    "                std=(0.229, 0.224, 0.225),\n",
    "                batch_size=self.batch_size[phase],\n",
    "                num_workers=self.num_workers,\n",
    "            )\n",
    "            for phase in self.phases\n",
    "        }\n",
    "        self.losses = {phase: [] for phase in self.phases}\n",
    "        self.iou_scores = {phase: [] for phase in self.phases}\n",
    "        self.dice_scores = {phase: [] for phase in self.phases}\n",
    "        \n",
    "    def forward(self, images, targets):\n",
    "        images = images.to(self.device)\n",
    "        masks = targets.to(self.device)\n",
    "        outputs = self.net(images)\n",
    "        loss = self.criterion(outputs, masks)\n",
    "        return loss, outputs\n",
    "\n",
    "    def iterate(self, epoch, phase):\n",
    "        meter = Meter(phase, epoch)\n",
    "        start = time.strftime(\"%H:%M:%S\")\n",
    "        print(f\"Starting epoch: {epoch} | phase: {phase} | ⏰: {start}\")\n",
    "        batch_size = self.batch_size[phase]\n",
    "        self.net.train(phase == \"train\")\n",
    "        dataloader = self.dataloaders[phase]\n",
    "        running_loss = 0.0\n",
    "        total_batches = len(dataloader)\n",
    "#         tk0 = tqdm(dataloader, total=total_batches)\n",
    "        self.optimizer.zero_grad()\n",
    "        for itr, batch in enumerate(dataloader):\n",
    "            images, targets = batch\n",
    "            loss, outputs = self.forward(images, targets)\n",
    "            loss = loss / self.accumulation_steps\n",
    "            if phase == \"train\":\n",
    "                loss.backward()\n",
    "                if (itr + 1 ) % self.accumulation_steps == 0:\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "            running_loss += loss.item()\n",
    "            outputs = outputs.detach().cpu()\n",
    "            meter.update(targets, outputs)\n",
    "#             tk0.set_postfix(loss=(running_loss / ((itr + 1))))\n",
    "        epoch_loss = (running_loss * self.accumulation_steps) / total_batches\n",
    "        dice, iou = epoch_log(phase, epoch, epoch_loss, meter, start)\n",
    "        self.losses[phase].append(epoch_loss)\n",
    "        self.dice_scores[phase].append(dice)\n",
    "        self.iou_scores[phase].append(iou)\n",
    "        torch.cuda.empty_cache()\n",
    "        return epoch_loss\n",
    "\n",
    "    def start(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.iterate(epoch, \"train\")\n",
    "            state = {\n",
    "                \"epoch\": epoch,\n",
    "                \"best_loss\": self.best_loss,\n",
    "                \"state_dict\": self.net.state_dict(),\n",
    "                \"optimizer\": self.optimizer.state_dict(),\n",
    "            }\n",
    "            val_loss = self.iterate(epoch, \"val\")\n",
    "            self.scheduler.step(val_loss)\n",
    "            if val_loss < self.best_loss:\n",
    "                print(\"******** New optimal found, saving state ********\")\n",
    "                state[\"best_loss\"] = self.best_loss = val_loss\n",
    "                torch.save(state, \"./model.pth\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86lHkiKox3KO"
   },
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0 | phase: train | ⏰: 07:32:45\n",
      "Loss: 3.9953 | dice: 0.1395 | dice_neg: 0.0231 | dice_pos: 0.2558 | IoU: 0.1639\n",
      "Starting epoch: 0 | phase: val | ⏰: 07:41:55\n",
      "Loss: 3.6519 | dice: 0.1741 | dice_neg: 0.0504 | dice_pos: 0.2979 | IoU: 0.1891\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 1 | phase: train | ⏰: 07:43:06\n",
      "Loss: 2.4477 | dice: 0.2263 | dice_neg: 0.1266 | dice_pos: 0.3259 | IoU: 0.2192\n",
      "Starting epoch: 1 | phase: val | ⏰: 07:52:07\n",
      "Loss: 4.7039 | dice: 0.1064 | dice_neg: 0.0252 | dice_pos: 0.1877 | IoU: 0.1157\n",
      "\n",
      "Starting epoch: 2 | phase: train | ⏰: 07:53:15\n",
      "Loss: 1.7720 | dice: 0.3139 | dice_neg: 0.2859 | dice_pos: 0.3419 | IoU: 0.2394\n",
      "Starting epoch: 2 | phase: val | ⏰: 08:02:01\n",
      "Loss: 2.5617 | dice: 0.2076 | dice_neg: 0.0798 | dice_pos: 0.3354 | IoU: 0.2212\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 3 | phase: train | ⏰: 08:03:13\n",
      "Loss: 1.6273 | dice: 0.3245 | dice_neg: 0.3079 | dice_pos: 0.3411 | IoU: 0.2400\n",
      "Starting epoch: 3 | phase: val | ⏰: 08:11:58\n",
      "Loss: 2.2902 | dice: 0.2763 | dice_neg: 0.2017 | dice_pos: 0.3510 | IoU: 0.2450\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 4 | phase: train | ⏰: 08:13:10\n",
      "Loss: 1.3441 | dice: 0.4236 | dice_neg: 0.4714 | dice_pos: 0.3759 | IoU: 0.2755\n",
      "Starting epoch: 4 | phase: val | ⏰: 08:21:54\n",
      "Loss: 2.0807 | dice: 0.4896 | dice_neg: 0.5966 | dice_pos: 0.3825 | IoU: 0.2799\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 5 | phase: train | ⏰: 08:23:05\n",
      "Loss: 1.3301 | dice: 0.4001 | dice_neg: 0.4251 | dice_pos: 0.3752 | IoU: 0.2734\n",
      "Starting epoch: 5 | phase: val | ⏰: 08:31:52\n",
      "Loss: 1.9966 | dice: 0.5620 | dice_neg: 0.7710 | dice_pos: 0.3529 | IoU: 0.2528\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 6 | phase: train | ⏰: 08:33:03\n",
      "Loss: 1.3212 | dice: 0.4125 | dice_neg: 0.4493 | dice_pos: 0.3758 | IoU: 0.2740\n",
      "Starting epoch: 6 | phase: val | ⏰: 08:41:50\n",
      "Loss: 2.0726 | dice: 0.4964 | dice_neg: 0.6071 | dice_pos: 0.3856 | IoU: 0.2790\n",
      "\n",
      "Starting epoch: 7 | phase: train | ⏰: 08:42:59\n",
      "Loss: 1.3107 | dice: 0.4487 | dice_neg: 0.5108 | dice_pos: 0.3866 | IoU: 0.2840\n",
      "Starting epoch: 7 | phase: val | ⏰: 08:51:38\n",
      "Loss: 2.0556 | dice: 0.5902 | dice_neg: 0.8529 | dice_pos: 0.3275 | IoU: 0.2473\n",
      "\n",
      "Starting epoch: 8 | phase: train | ⏰: 08:52:48\n",
      "Loss: 1.2278 | dice: 0.4636 | dice_neg: 0.5507 | dice_pos: 0.3765 | IoU: 0.2809\n",
      "Starting epoch: 8 | phase: val | ⏰: 09:01:17\n",
      "Loss: 2.0746 | dice: 0.5110 | dice_neg: 0.6092 | dice_pos: 0.4128 | IoU: 0.3037\n",
      "\n",
      "Starting epoch: 9 | phase: train | ⏰: 09:02:23\n",
      "Loss: 1.2296 | dice: 0.4483 | dice_neg: 0.5081 | dice_pos: 0.3885 | IoU: 0.2888\n",
      "Starting epoch: 9 | phase: val | ⏰: 09:11:09\n",
      "Loss: 2.1200 | dice: 0.5636 | dice_neg: 0.8088 | dice_pos: 0.3184 | IoU: 0.2292\n",
      "Epoch    10: reducing learning rate of group 0 to 5.0000e-05.\n",
      "\n",
      "Starting epoch: 10 | phase: train | ⏰: 09:12:18\n",
      "Loss: 1.1118 | dice: 0.5214 | dice_neg: 0.6185 | dice_pos: 0.4243 | IoU: 0.3193\n",
      "Starting epoch: 10 | phase: val | ⏰: 09:21:01\n",
      "Loss: 1.8775 | dice: 0.6111 | dice_neg: 0.8550 | dice_pos: 0.3673 | IoU: 0.2670\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 11 | phase: train | ⏰: 09:22:12\n",
      "Loss: 0.9985 | dice: 0.5771 | dice_neg: 0.7173 | dice_pos: 0.4369 | IoU: 0.3336\n",
      "Starting epoch: 11 | phase: val | ⏰: 09:30:56\n",
      "Loss: 1.9347 | dice: 0.5984 | dice_neg: 0.7899 | dice_pos: 0.4068 | IoU: 0.3012\n",
      "\n",
      "Starting epoch: 12 | phase: train | ⏰: 09:32:04\n",
      "Loss: 0.9714 | dice: 0.6069 | dice_neg: 0.7683 | dice_pos: 0.4455 | IoU: 0.3431\n",
      "Starting epoch: 12 | phase: val | ⏰: 09:40:33\n",
      "Loss: 1.7757 | dice: 0.6191 | dice_neg: 0.8319 | dice_pos: 0.4062 | IoU: 0.3023\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 13 | phase: train | ⏰: 09:41:43\n",
      "Loss: 0.9341 | dice: 0.6191 | dice_neg: 0.7888 | dice_pos: 0.4494 | IoU: 0.3464\n",
      "Starting epoch: 13 | phase: val | ⏰: 09:50:04\n",
      "Loss: 1.7338 | dice: 0.6116 | dice_neg: 0.7962 | dice_pos: 0.4269 | IoU: 0.3167\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 14 | phase: train | ⏰: 09:51:17\n",
      "Loss: 0.9311 | dice: 0.6107 | dice_neg: 0.7578 | dice_pos: 0.4636 | IoU: 0.3594\n",
      "Starting epoch: 14 | phase: val | ⏰: 09:59:37\n",
      "Loss: 1.8319 | dice: 0.6190 | dice_neg: 0.8151 | dice_pos: 0.4230 | IoU: 0.3276\n",
      "\n",
      "Starting epoch: 15 | phase: train | ⏰: 10:00:48\n",
      "Loss: 0.8542 | dice: 0.6245 | dice_neg: 0.7725 | dice_pos: 0.4766 | IoU: 0.3682\n",
      "Starting epoch: 15 | phase: val | ⏰: 10:09:32\n",
      "Loss: 1.7684 | dice: 0.5742 | dice_neg: 0.6933 | dice_pos: 0.4550 | IoU: 0.3328\n",
      "\n",
      "Starting epoch: 16 | phase: train | ⏰: 10:10:40\n",
      "Loss: 0.8789 | dice: 0.6373 | dice_neg: 0.7924 | dice_pos: 0.4821 | IoU: 0.3763\n",
      "Starting epoch: 16 | phase: val | ⏰: 10:19:27\n",
      "Loss: 1.5546 | dice: 0.6073 | dice_neg: 0.7647 | dice_pos: 0.4498 | IoU: 0.3485\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 17 | phase: train | ⏰: 10:20:38\n",
      "Loss: 0.8968 | dice: 0.6407 | dice_neg: 0.7888 | dice_pos: 0.4927 | IoU: 0.3782\n",
      "Starting epoch: 17 | phase: val | ⏰: 10:29:00\n",
      "Loss: 1.7926 | dice: 0.6307 | dice_neg: 0.8445 | dice_pos: 0.4169 | IoU: 0.3231\n",
      "\n",
      "Starting epoch: 18 | phase: train | ⏰: 10:30:10\n",
      "Loss: 0.8810 | dice: 0.6237 | dice_neg: 0.7567 | dice_pos: 0.4907 | IoU: 0.3836\n",
      "Starting epoch: 18 | phase: val | ⏰: 10:38:53\n",
      "Loss: 1.7707 | dice: 0.6284 | dice_neg: 0.8340 | dice_pos: 0.4227 | IoU: 0.3172\n",
      "\n",
      "Starting epoch: 19 | phase: train | ⏰: 10:40:01\n",
      "Loss: 0.8932 | dice: 0.6417 | dice_neg: 0.7856 | dice_pos: 0.4978 | IoU: 0.3839\n",
      "Starting epoch: 19 | phase: val | ⏰: 10:48:36\n",
      "Loss: 1.8373 | dice: 0.6345 | dice_neg: 0.8361 | dice_pos: 0.4329 | IoU: 0.3208\n",
      "\n",
      "Starting epoch: 20 | phase: train | ⏰: 10:49:45\n",
      "Loss: 0.8363 | dice: 0.6413 | dice_neg: 0.7924 | dice_pos: 0.4902 | IoU: 0.3837\n",
      "Starting epoch: 20 | phase: val | ⏰: 10:58:24\n",
      "Loss: 1.5534 | dice: 0.6266 | dice_neg: 0.8193 | dice_pos: 0.4338 | IoU: 0.3458\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 21 | phase: train | ⏰: 10:59:32\n",
      "Loss: 0.8104 | dice: 0.6475 | dice_neg: 0.7935 | dice_pos: 0.5016 | IoU: 0.3894\n",
      "Starting epoch: 21 | phase: val | ⏰: 11:08:19\n",
      "Loss: 1.6814 | dice: 0.6096 | dice_neg: 0.7920 | dice_pos: 0.4271 | IoU: 0.3127\n",
      "\n",
      "Starting epoch: 22 | phase: train | ⏰: 11:09:27\n",
      "Loss: 0.8320 | dice: 0.6618 | dice_neg: 0.8224 | dice_pos: 0.5011 | IoU: 0.3949\n",
      "Starting epoch: 22 | phase: val | ⏰: 11:18:07\n",
      "Loss: 1.7661 | dice: 0.6232 | dice_neg: 0.7878 | dice_pos: 0.4586 | IoU: 0.3403\n",
      "\n",
      "Starting epoch: 23 | phase: train | ⏰: 11:19:16\n",
      "Loss: 0.7641 | dice: 0.6794 | dice_neg: 0.8403 | dice_pos: 0.5186 | IoU: 0.4059\n",
      "Starting epoch: 23 | phase: val | ⏰: 11:28:01\n",
      "Loss: 1.7832 | dice: 0.6455 | dice_neg: 0.8824 | dice_pos: 0.4086 | IoU: 0.3181\n",
      "\n",
      "Starting epoch: 24 | phase: train | ⏰: 11:29:10\n",
      "Loss: 0.8094 | dice: 0.6633 | dice_neg: 0.8129 | dice_pos: 0.5137 | IoU: 0.3979\n",
      "Starting epoch: 24 | phase: val | ⏰: 11:37:54\n",
      "Loss: 1.5428 | dice: 0.6310 | dice_neg: 0.8214 | dice_pos: 0.4406 | IoU: 0.3391\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 25 | phase: train | ⏰: 11:39:04\n",
      "Loss: 0.7909 | dice: 0.6716 | dice_neg: 0.8224 | dice_pos: 0.5208 | IoU: 0.4059\n",
      "Starting epoch: 25 | phase: val | ⏰: 11:47:49\n",
      "Loss: 1.8331 | dice: 0.6331 | dice_neg: 0.8908 | dice_pos: 0.3755 | IoU: 0.2867\n",
      "\n",
      "Starting epoch: 26 | phase: train | ⏰: 11:48:57\n",
      "Loss: 0.7673 | dice: 0.6797 | dice_neg: 0.8382 | dice_pos: 0.5213 | IoU: 0.4081\n",
      "Starting epoch: 26 | phase: val | ⏰: 11:57:37\n",
      "Loss: 1.6822 | dice: 0.6300 | dice_neg: 0.8382 | dice_pos: 0.4218 | IoU: 0.3263\n",
      "\n",
      "Starting epoch: 27 | phase: train | ⏰: 11:58:48\n",
      "Loss: 0.7239 | dice: 0.6846 | dice_neg: 0.8424 | dice_pos: 0.5269 | IoU: 0.4142\n",
      "Starting epoch: 27 | phase: val | ⏰: 12:07:26\n",
      "Loss: 1.6267 | dice: 0.6494 | dice_neg: 0.8761 | dice_pos: 0.4227 | IoU: 0.3348\n",
      "\n",
      "Starting epoch: 28 | phase: train | ⏰: 12:08:37\n",
      "Loss: 0.7199 | dice: 0.6997 | dice_neg: 0.8749 | dice_pos: 0.5245 | IoU: 0.4178\n",
      "Starting epoch: 28 | phase: val | ⏰: 12:17:22\n",
      "Loss: 1.7046 | dice: 0.6462 | dice_neg: 0.8718 | dice_pos: 0.4205 | IoU: 0.3203\n",
      "Epoch    29: reducing learning rate of group 0 to 5.0000e-06.\n",
      "\n",
      "Starting epoch: 29 | phase: train | ⏰: 12:18:32\n",
      "Loss: 0.7291 | dice: 0.7100 | dice_neg: 0.8870 | dice_pos: 0.5330 | IoU: 0.4200\n",
      "Starting epoch: 29 | phase: val | ⏰: 12:27:13\n",
      "Loss: 1.8030 | dice: 0.6501 | dice_neg: 0.8824 | dice_pos: 0.4178 | IoU: 0.3200\n",
      "\n",
      "Starting epoch: 30 | phase: train | ⏰: 12:28:23\n",
      "Loss: 0.7251 | dice: 0.7134 | dice_neg: 0.8865 | dice_pos: 0.5403 | IoU: 0.4293\n",
      "Starting epoch: 30 | phase: val | ⏰: 12:37:06\n",
      "Loss: 1.5613 | dice: 0.6446 | dice_neg: 0.8550 | dice_pos: 0.4341 | IoU: 0.3314\n",
      "\n",
      "Starting epoch: 31 | phase: train | ⏰: 12:38:16\n",
      "Loss: 0.7400 | dice: 0.7162 | dice_neg: 0.8923 | dice_pos: 0.5401 | IoU: 0.4264\n",
      "Starting epoch: 31 | phase: val | ⏰: 12:46:48\n",
      "Loss: 1.6942 | dice: 0.6507 | dice_neg: 0.8824 | dice_pos: 0.4191 | IoU: 0.3185\n",
      "\n",
      "Starting epoch: 32 | phase: train | ⏰: 12:47:59\n",
      "Loss: 0.6962 | dice: 0.7183 | dice_neg: 0.8897 | dice_pos: 0.5467 | IoU: 0.4336\n",
      "Starting epoch: 32 | phase: val | ⏰: 12:56:46\n",
      "Loss: 1.6820 | dice: 0.6497 | dice_neg: 0.8676 | dice_pos: 0.4317 | IoU: 0.3298\n",
      "Epoch    33: reducing learning rate of group 0 to 5.0000e-07.\n",
      "\n",
      "Starting epoch: 33 | phase: train | ⏰: 12:57:56\n",
      "Loss: 0.6539 | dice: 0.7156 | dice_neg: 0.8875 | dice_pos: 0.5436 | IoU: 0.4303\n",
      "Starting epoch: 33 | phase: val | ⏰: 13:06:25\n",
      "Loss: 1.6933 | dice: 0.6467 | dice_neg: 0.8655 | dice_pos: 0.4279 | IoU: 0.3277\n",
      "\n",
      "Starting epoch: 34 | phase: train | ⏰: 13:07:34\n",
      "Loss: 0.6638 | dice: 0.7168 | dice_neg: 0.8891 | dice_pos: 0.5446 | IoU: 0.4273\n",
      "Starting epoch: 34 | phase: val | ⏰: 13:15:50\n",
      "Loss: 1.4579 | dice: 0.6500 | dice_neg: 0.8634 | dice_pos: 0.4366 | IoU: 0.3408\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 35 | phase: train | ⏰: 13:17:03\n",
      "Loss: 0.6333 | dice: 0.7182 | dice_neg: 0.8923 | dice_pos: 0.5441 | IoU: 0.4319\n",
      "Starting epoch: 35 | phase: val | ⏰: 13:25:23\n",
      "Loss: 1.8690 | dice: 0.6481 | dice_neg: 0.8718 | dice_pos: 0.4243 | IoU: 0.3217\n",
      "\n",
      "Starting epoch: 36 | phase: train | ⏰: 13:26:33\n",
      "Loss: 0.6830 | dice: 0.7165 | dice_neg: 0.8881 | dice_pos: 0.5450 | IoU: 0.4309\n",
      "Starting epoch: 36 | phase: val | ⏰: 13:35:10\n",
      "Loss: 1.8603 | dice: 0.6491 | dice_neg: 0.8718 | dice_pos: 0.4263 | IoU: 0.3204\n",
      "\n",
      "Starting epoch: 37 | phase: train | ⏰: 13:36:18\n",
      "Loss: 0.6809 | dice: 0.7178 | dice_neg: 0.8928 | dice_pos: 0.5427 | IoU: 0.4310\n",
      "Starting epoch: 37 | phase: val | ⏰: 13:44:50\n",
      "Loss: 1.6683 | dice: 0.6517 | dice_neg: 0.8718 | dice_pos: 0.4315 | IoU: 0.3323\n",
      "\n",
      "Starting epoch: 38 | phase: train | ⏰: 13:45:59\n",
      "Loss: 0.7189 | dice: 0.7193 | dice_neg: 0.8933 | dice_pos: 0.5453 | IoU: 0.4254\n",
      "Starting epoch: 38 | phase: val | ⏰: 13:54:43\n",
      "Loss: 1.5770 | dice: 0.6496 | dice_neg: 0.8761 | dice_pos: 0.4232 | IoU: 0.3243\n",
      "Epoch    39: reducing learning rate of group 0 to 5.0000e-08.\n",
      "\n",
      "Starting epoch: 39 | phase: train | ⏰: 13:55:52\n",
      "Loss: 0.6693 | dice: 0.7140 | dice_neg: 0.8823 | dice_pos: 0.5457 | IoU: 0.4318\n",
      "Starting epoch: 39 | phase: val | ⏰: 14:04:38\n",
      "Loss: 1.6173 | dice: 0.6475 | dice_neg: 0.8529 | dice_pos: 0.4421 | IoU: 0.3388\n",
      "\n",
      "Starting epoch: 40 | phase: train | ⏰: 14:05:50\n",
      "Loss: 0.7205 | dice: 0.7173 | dice_neg: 0.8896 | dice_pos: 0.5449 | IoU: 0.4314\n",
      "Starting epoch: 40 | phase: val | ⏰: 14:14:39\n",
      "Loss: 1.6711 | dice: 0.6473 | dice_neg: 0.8529 | dice_pos: 0.4416 | IoU: 0.3397\n",
      "\n",
      "Starting epoch: 41 | phase: train | ⏰: 14:15:49\n",
      "Loss: 0.6959 | dice: 0.7147 | dice_neg: 0.8871 | dice_pos: 0.5420 | IoU: 0.4253\n",
      "Starting epoch: 41 | phase: val | ⏰: 14:24:29\n",
      "Loss: 1.7506 | dice: 0.6486 | dice_neg: 0.8718 | dice_pos: 0.4254 | IoU: 0.3215\n",
      "\n",
      "Starting epoch: 42 | phase: train | ⏰: 14:25:40\n",
      "Loss: 0.6940 | dice: 0.7147 | dice_neg: 0.8807 | dice_pos: 0.5486 | IoU: 0.4306\n",
      "Starting epoch: 42 | phase: val | ⏰: 14:34:26\n",
      "Loss: 1.5261 | dice: 0.6508 | dice_neg: 0.8739 | dice_pos: 0.4277 | IoU: 0.3328\n",
      "Epoch    43: reducing learning rate of group 0 to 5.0000e-09.\n",
      "\n",
      "Starting epoch: 43 | phase: train | ⏰: 14:35:35\n",
      "Loss: 0.6500 | dice: 0.7162 | dice_neg: 0.8818 | dice_pos: 0.5506 | IoU: 0.4320\n",
      "Starting epoch: 43 | phase: val | ⏰: 14:44:19\n",
      "Loss: 1.5441 | dice: 0.6523 | dice_neg: 0.8782 | dice_pos: 0.4265 | IoU: 0.3337\n",
      "\n",
      "Starting epoch: 44 | phase: train | ⏰: 14:45:31\n",
      "Loss: 0.7105 | dice: 0.7125 | dice_neg: 0.8812 | dice_pos: 0.5437 | IoU: 0.4270\n",
      "Starting epoch: 44 | phase: val | ⏰: 14:53:57\n",
      "Loss: 1.8174 | dice: 0.6507 | dice_neg: 0.8803 | dice_pos: 0.4212 | IoU: 0.3184\n",
      "\n",
      "Starting epoch: 45 | phase: train | ⏰: 14:55:08\n",
      "Loss: 0.6879 | dice: 0.7162 | dice_neg: 0.8860 | dice_pos: 0.5465 | IoU: 0.4295\n",
      "Starting epoch: 45 | phase: val | ⏰: 15:03:46\n",
      "Loss: 1.6518 | dice: 0.6458 | dice_neg: 0.8487 | dice_pos: 0.4428 | IoU: 0.3379\n",
      "\n",
      "Starting epoch: 46 | phase: train | ⏰: 15:04:53\n",
      "Loss: 0.7247 | dice: 0.7164 | dice_neg: 0.8886 | dice_pos: 0.5442 | IoU: 0.4251\n",
      "Starting epoch: 46 | phase: val | ⏰: 15:13:38\n",
      "Loss: 1.9014 | dice: 0.6484 | dice_neg: 0.8634 | dice_pos: 0.4333 | IoU: 0.3259\n",
      "\n",
      "Starting epoch: 47 | phase: train | ⏰: 15:14:46\n",
      "Loss: 0.6684 | dice: 0.7156 | dice_neg: 0.8870 | dice_pos: 0.5442 | IoU: 0.4297\n",
      "Starting epoch: 47 | phase: val | ⏰: 15:23:04\n",
      "Loss: 1.7824 | dice: 0.6480 | dice_neg: 0.8529 | dice_pos: 0.4430 | IoU: 0.3264\n",
      "\n",
      "Starting epoch: 48 | phase: train | ⏰: 15:24:14\n",
      "Loss: 0.7264 | dice: 0.7166 | dice_neg: 0.8875 | dice_pos: 0.5457 | IoU: 0.4295\n",
      "Starting epoch: 48 | phase: val | ⏰: 15:32:40\n",
      "Loss: 1.6822 | dice: 0.6508 | dice_neg: 0.8739 | dice_pos: 0.4276 | IoU: 0.3268\n",
      "\n",
      "Starting epoch: 49 | phase: train | ⏰: 15:33:49\n",
      "Loss: 0.7028 | dice: 0.7186 | dice_neg: 0.8907 | dice_pos: 0.5465 | IoU: 0.4296\n",
      "Starting epoch: 49 | phase: val | ⏰: 15:42:32\n",
      "Loss: 1.8940 | dice: 0.6528 | dice_neg: 0.8908 | dice_pos: 0.4149 | IoU: 0.3153\n",
      "\n",
      "Starting epoch: 50 | phase: train | ⏰: 15:43:39\n",
      "Loss: 0.6855 | dice: 0.7117 | dice_neg: 0.8781 | dice_pos: 0.5453 | IoU: 0.4316\n",
      "Starting epoch: 50 | phase: val | ⏰: 15:52:22\n",
      "Loss: 1.5851 | dice: 0.6520 | dice_neg: 0.8739 | dice_pos: 0.4300 | IoU: 0.3286\n",
      "\n",
      "Starting epoch: 51 | phase: train | ⏰: 15:53:32\n",
      "Loss: 0.6912 | dice: 0.7162 | dice_neg: 0.8865 | dice_pos: 0.5460 | IoU: 0.4308\n",
      "Starting epoch: 51 | phase: val | ⏰: 16:01:59\n",
      "Loss: 1.7343 | dice: 0.6477 | dice_neg: 0.8550 | dice_pos: 0.4404 | IoU: 0.3417\n",
      "\n",
      "Starting epoch: 52 | phase: train | ⏰: 16:03:07\n",
      "Loss: 0.7119 | dice: 0.7216 | dice_neg: 0.8981 | dice_pos: 0.5452 | IoU: 0.4304\n",
      "Starting epoch: 52 | phase: val | ⏰: 16:11:21\n",
      "Loss: 1.7140 | dice: 0.6486 | dice_neg: 0.8718 | dice_pos: 0.4253 | IoU: 0.3371\n",
      "\n",
      "Starting epoch: 53 | phase: train | ⏰: 16:12:31\n",
      "Loss: 0.6755 | dice: 0.7191 | dice_neg: 0.8928 | dice_pos: 0.5454 | IoU: 0.4316\n",
      "Starting epoch: 53 | phase: val | ⏰: 16:21:09\n",
      "Loss: 1.7519 | dice: 0.6505 | dice_neg: 0.8739 | dice_pos: 0.4271 | IoU: 0.3198\n",
      "\n",
      "Starting epoch: 54 | phase: train | ⏰: 16:22:17\n",
      "Loss: 0.7252 | dice: 0.7129 | dice_neg: 0.8818 | dice_pos: 0.5441 | IoU: 0.4273\n",
      "Starting epoch: 54 | phase: val | ⏰: 16:30:34\n",
      "Loss: 1.5836 | dice: 0.6476 | dice_neg: 0.8550 | dice_pos: 0.4401 | IoU: 0.3360\n",
      "\n",
      "Starting epoch: 55 | phase: train | ⏰: 16:31:43\n",
      "Loss: 0.7328 | dice: 0.7175 | dice_neg: 0.8896 | dice_pos: 0.5454 | IoU: 0.4294\n",
      "Starting epoch: 55 | phase: val | ⏰: 16:40:00\n",
      "Loss: 1.5365 | dice: 0.6470 | dice_neg: 0.8529 | dice_pos: 0.4410 | IoU: 0.3390\n",
      "\n",
      "Starting epoch: 56 | phase: train | ⏰: 16:41:08\n",
      "Loss: 0.6859 | dice: 0.7176 | dice_neg: 0.8849 | dice_pos: 0.5502 | IoU: 0.4332\n",
      "Starting epoch: 56 | phase: val | ⏰: 16:49:27\n",
      "Loss: 1.6759 | dice: 0.6493 | dice_neg: 0.8655 | dice_pos: 0.4330 | IoU: 0.3307\n",
      "\n",
      "Starting epoch: 57 | phase: train | ⏰: 16:50:36\n",
      "Loss: 0.6818 | dice: 0.7137 | dice_neg: 0.8849 | dice_pos: 0.5425 | IoU: 0.4302\n",
      "Starting epoch: 57 | phase: val | ⏰: 16:59:21\n",
      "Loss: 1.7401 | dice: 0.6429 | dice_neg: 0.8382 | dice_pos: 0.4477 | IoU: 0.3390\n",
      "\n",
      "Starting epoch: 58 | phase: train | ⏰: 17:00:29\n",
      "Loss: 0.6763 | dice: 0.7176 | dice_neg: 0.8886 | dice_pos: 0.5466 | IoU: 0.4355\n",
      "Starting epoch: 58 | phase: val | ⏰: 17:09:12\n",
      "Loss: 1.6253 | dice: 0.6468 | dice_neg: 0.8550 | dice_pos: 0.4385 | IoU: 0.3392\n",
      "\n",
      "Starting epoch: 59 | phase: train | ⏰: 17:10:22\n",
      "Loss: 0.6986 | dice: 0.7208 | dice_neg: 0.8902 | dice_pos: 0.5514 | IoU: 0.4367\n",
      "Starting epoch: 59 | phase: val | ⏰: 17:19:04\n",
      "Loss: 1.7014 | dice: 0.6519 | dice_neg: 0.8929 | dice_pos: 0.4108 | IoU: 0.3203\n",
      "\n",
      "Starting epoch: 60 | phase: train | ⏰: 17:20:12\n",
      "Loss: 0.6942 | dice: 0.7117 | dice_neg: 0.8807 | dice_pos: 0.5426 | IoU: 0.4287\n",
      "Starting epoch: 60 | phase: val | ⏰: 17:28:55\n",
      "Loss: 1.7427 | dice: 0.6491 | dice_neg: 0.8697 | dice_pos: 0.4284 | IoU: 0.3239\n",
      "\n",
      "Starting epoch: 61 | phase: train | ⏰: 17:30:02\n",
      "Loss: 0.6727 | dice: 0.7155 | dice_neg: 0.8844 | dice_pos: 0.5465 | IoU: 0.4354\n",
      "Starting epoch: 61 | phase: val | ⏰: 17:38:41\n",
      "Loss: 1.6714 | dice: 0.6489 | dice_neg: 0.8718 | dice_pos: 0.4259 | IoU: 0.3213\n",
      "\n",
      "Starting epoch: 62 | phase: train | ⏰: 17:39:50\n",
      "Loss: 0.7047 | dice: 0.7152 | dice_neg: 0.8865 | dice_pos: 0.5440 | IoU: 0.4228\n",
      "Starting epoch: 62 | phase: val | ⏰: 17:48:37\n",
      "Loss: 1.5319 | dice: 0.6489 | dice_neg: 0.8676 | dice_pos: 0.4301 | IoU: 0.3389\n",
      "\n",
      "Starting epoch: 63 | phase: train | ⏰: 17:49:46\n",
      "Loss: 0.6981 | dice: 0.7178 | dice_neg: 0.8939 | dice_pos: 0.5418 | IoU: 0.4275\n",
      "Starting epoch: 63 | phase: val | ⏰: 17:58:27\n",
      "Loss: 1.5073 | dice: 0.6496 | dice_neg: 0.8592 | dice_pos: 0.4399 | IoU: 0.3393\n",
      "\n",
      "Starting epoch: 64 | phase: train | ⏰: 17:59:36\n",
      "Loss: 0.6659 | dice: 0.7194 | dice_neg: 0.8944 | dice_pos: 0.5444 | IoU: 0.4307\n",
      "Starting epoch: 64 | phase: val | ⏰: 18:08:25\n",
      "Loss: 1.6030 | dice: 0.6491 | dice_neg: 0.8613 | dice_pos: 0.4369 | IoU: 0.3323\n",
      "\n",
      "Starting epoch: 65 | phase: train | ⏰: 18:09:34\n",
      "Loss: 0.6842 | dice: 0.7162 | dice_neg: 0.8839 | dice_pos: 0.5483 | IoU: 0.4359\n",
      "Starting epoch: 65 | phase: val | ⏰: 18:18:19\n",
      "Loss: 1.6377 | dice: 0.6509 | dice_neg: 0.8697 | dice_pos: 0.4320 | IoU: 0.3287\n",
      "\n",
      "Starting epoch: 66 | phase: train | ⏰: 18:19:31\n",
      "Loss: 0.7361 | dice: 0.7137 | dice_neg: 0.8881 | dice_pos: 0.5393 | IoU: 0.4251\n",
      "Starting epoch: 66 | phase: val | ⏰: 18:27:56\n",
      "Loss: 1.7385 | dice: 0.6525 | dice_neg: 0.8887 | dice_pos: 0.4163 | IoU: 0.3241\n",
      "\n",
      "Starting epoch: 67 | phase: train | ⏰: 18:29:07\n",
      "Loss: 0.7105 | dice: 0.7122 | dice_neg: 0.8812 | dice_pos: 0.5431 | IoU: 0.4289\n",
      "Starting epoch: 67 | phase: val | ⏰: 18:37:53\n",
      "Loss: 1.6791 | dice: 0.6447 | dice_neg: 0.8445 | dice_pos: 0.4448 | IoU: 0.3400\n",
      "\n",
      "Starting epoch: 68 | phase: train | ⏰: 18:39:01\n",
      "Loss: 0.7011 | dice: 0.7149 | dice_neg: 0.8833 | dice_pos: 0.5465 | IoU: 0.4306\n",
      "Starting epoch: 68 | phase: val | ⏰: 18:47:23\n",
      "Loss: 1.5631 | dice: 0.6493 | dice_neg: 0.8655 | dice_pos: 0.4330 | IoU: 0.3323\n",
      "\n",
      "Starting epoch: 69 | phase: train | ⏰: 18:48:33\n",
      "Loss: 0.6788 | dice: 0.7193 | dice_neg: 0.8875 | dice_pos: 0.5510 | IoU: 0.4354\n",
      "Starting epoch: 69 | phase: val | ⏰: 18:56:50\n",
      "Loss: 1.7499 | dice: 0.6456 | dice_neg: 0.8466 | dice_pos: 0.4446 | IoU: 0.3307\n",
      "\n",
      "Starting epoch: 70 | phase: train | ⏰: 18:57:58\n",
      "Loss: 0.6897 | dice: 0.7162 | dice_neg: 0.8839 | dice_pos: 0.5486 | IoU: 0.4378\n",
      "Starting epoch: 70 | phase: val | ⏰: 19:06:44\n",
      "Loss: 1.5731 | dice: 0.6513 | dice_neg: 0.8739 | dice_pos: 0.4286 | IoU: 0.3293\n",
      "\n",
      "Starting epoch: 71 | phase: train | ⏰: 19:07:54\n",
      "Loss: 0.7153 | dice: 0.7203 | dice_neg: 0.8960 | dice_pos: 0.5447 | IoU: 0.4308\n",
      "Starting epoch: 71 | phase: val | ⏰: 19:16:43\n",
      "Loss: 1.7644 | dice: 0.6495 | dice_neg: 0.8613 | dice_pos: 0.4377 | IoU: 0.3384\n",
      "\n",
      "Starting epoch: 72 | phase: train | ⏰: 19:17:50\n",
      "Loss: 0.6561 | dice: 0.7168 | dice_neg: 0.8917 | dice_pos: 0.5419 | IoU: 0.4324\n",
      "Starting epoch: 72 | phase: val | ⏰: 19:26:31\n",
      "Loss: 1.8096 | dice: 0.6473 | dice_neg: 0.8613 | dice_pos: 0.4333 | IoU: 0.3319\n",
      "\n",
      "Starting epoch: 73 | phase: train | ⏰: 19:27:38\n",
      "Loss: 0.7152 | dice: 0.7165 | dice_neg: 0.8891 | dice_pos: 0.5438 | IoU: 0.4273\n",
      "Starting epoch: 73 | phase: val | ⏰: 19:36:19\n",
      "Loss: 1.7254 | dice: 0.6511 | dice_neg: 0.8739 | dice_pos: 0.4283 | IoU: 0.3252\n",
      "\n",
      "Starting epoch: 74 | phase: train | ⏰: 19:37:29\n",
      "Loss: 0.6610 | dice: 0.7239 | dice_neg: 0.8975 | dice_pos: 0.5503 | IoU: 0.4372\n",
      "Starting epoch: 74 | phase: val | ⏰: 19:46:13\n",
      "Loss: 1.6653 | dice: 0.6480 | dice_neg: 0.8613 | dice_pos: 0.4347 | IoU: 0.3221\n",
      "\n",
      "Starting epoch: 75 | phase: train | ⏰: 19:47:23\n",
      "Loss: 0.6740 | dice: 0.7186 | dice_neg: 0.8902 | dice_pos: 0.5471 | IoU: 0.4369\n",
      "Starting epoch: 75 | phase: val | ⏰: 19:56:07\n",
      "Loss: 1.7207 | dice: 0.6479 | dice_neg: 0.8508 | dice_pos: 0.4450 | IoU: 0.3370\n",
      "\n",
      "Starting epoch: 76 | phase: train | ⏰: 19:57:16\n",
      "Loss: 0.6961 | dice: 0.7195 | dice_neg: 0.8923 | dice_pos: 0.5467 | IoU: 0.4339\n",
      "Starting epoch: 76 | phase: val | ⏰: 20:06:05\n",
      "Loss: 1.7088 | dice: 0.6458 | dice_neg: 0.8508 | dice_pos: 0.4407 | IoU: 0.3333\n",
      "\n",
      "Starting epoch: 77 | phase: train | ⏰: 20:07:13\n",
      "Loss: 0.6794 | dice: 0.7150 | dice_neg: 0.8844 | dice_pos: 0.5456 | IoU: 0.4312\n",
      "Starting epoch: 77 | phase: val | ⏰: 20:16:01\n",
      "Loss: 1.7782 | dice: 0.6496 | dice_neg: 0.8782 | dice_pos: 0.4210 | IoU: 0.3234\n",
      "\n",
      "Starting epoch: 78 | phase: train | ⏰: 20:17:11\n",
      "Loss: 0.6700 | dice: 0.7210 | dice_neg: 0.8970 | dice_pos: 0.5449 | IoU: 0.4345\n",
      "Starting epoch: 78 | phase: val | ⏰: 20:25:58\n",
      "Loss: 1.7584 | dice: 0.6471 | dice_neg: 0.8529 | dice_pos: 0.4412 | IoU: 0.3288\n",
      "\n",
      "Starting epoch: 79 | phase: train | ⏰: 20:27:07\n",
      "Loss: 0.6928 | dice: 0.7165 | dice_neg: 0.8902 | dice_pos: 0.5428 | IoU: 0.4340\n",
      "Starting epoch: 79 | phase: val | ⏰: 20:35:51\n",
      "Loss: 1.6866 | dice: 0.6491 | dice_neg: 0.8697 | dice_pos: 0.4284 | IoU: 0.3213\n",
      "\n",
      "Starting epoch: 80 | phase: train | ⏰: 20:37:00\n",
      "Loss: 0.6767 | dice: 0.7154 | dice_neg: 0.8844 | dice_pos: 0.5463 | IoU: 0.4329\n",
      "Starting epoch: 80 | phase: val | ⏰: 20:45:49\n",
      "Loss: 1.6498 | dice: 0.6456 | dice_neg: 0.8466 | dice_pos: 0.4447 | IoU: 0.3458\n",
      "\n",
      "Starting epoch: 81 | phase: train | ⏰: 20:46:59\n",
      "Loss: 0.6469 | dice: 0.7185 | dice_neg: 0.8917 | dice_pos: 0.5452 | IoU: 0.4319\n",
      "Starting epoch: 81 | phase: val | ⏰: 20:55:44\n",
      "Loss: 1.5641 | dice: 0.6507 | dice_neg: 0.8761 | dice_pos: 0.4253 | IoU: 0.3346\n",
      "\n",
      "Starting epoch: 82 | phase: train | ⏰: 20:56:53\n",
      "Loss: 0.6900 | dice: 0.7174 | dice_neg: 0.8870 | dice_pos: 0.5479 | IoU: 0.4316\n",
      "Starting epoch: 82 | phase: val | ⏰: 21:05:38\n",
      "Loss: 1.6016 | dice: 0.6502 | dice_neg: 0.8592 | dice_pos: 0.4411 | IoU: 0.3496\n",
      "\n",
      "Starting epoch: 83 | phase: train | ⏰: 21:06:43\n",
      "Loss: 0.6814 | dice: 0.7178 | dice_neg: 0.8917 | dice_pos: 0.5439 | IoU: 0.4290\n",
      "Starting epoch: 83 | phase: val | ⏰: 21:15:32\n",
      "Loss: 1.7681 | dice: 0.6517 | dice_neg: 0.8803 | dice_pos: 0.4231 | IoU: 0.3257\n",
      "\n",
      "Starting epoch: 84 | phase: train | ⏰: 21:16:40\n",
      "Loss: 0.6453 | dice: 0.7177 | dice_neg: 0.8896 | dice_pos: 0.5457 | IoU: 0.4345\n",
      "Starting epoch: 84 | phase: val | ⏰: 21:25:27\n",
      "Loss: 1.5439 | dice: 0.6494 | dice_neg: 0.8592 | dice_pos: 0.4396 | IoU: 0.3409\n",
      "\n",
      "Starting epoch: 85 | phase: train | ⏰: 21:26:35\n",
      "Loss: 0.6594 | dice: 0.7165 | dice_neg: 0.8891 | dice_pos: 0.5440 | IoU: 0.4334\n",
      "Starting epoch: 85 | phase: val | ⏰: 21:35:21\n",
      "Loss: 1.7437 | dice: 0.6486 | dice_neg: 0.8613 | dice_pos: 0.4359 | IoU: 0.3348\n",
      "\n",
      "Starting epoch: 86 | phase: train | ⏰: 21:36:28\n",
      "Loss: 0.6963 | dice: 0.7159 | dice_neg: 0.8881 | dice_pos: 0.5436 | IoU: 0.4320\n",
      "Starting epoch: 86 | phase: val | ⏰: 21:45:02\n",
      "Loss: 1.6566 | dice: 0.6505 | dice_neg: 0.8718 | dice_pos: 0.4292 | IoU: 0.3316\n",
      "\n",
      "Starting epoch: 87 | phase: train | ⏰: 21:46:14\n",
      "Loss: 0.6836 | dice: 0.7182 | dice_neg: 0.8917 | dice_pos: 0.5447 | IoU: 0.4299\n",
      "Starting epoch: 87 | phase: val | ⏰: 21:55:00\n",
      "Loss: 1.7821 | dice: 0.6513 | dice_neg: 0.8697 | dice_pos: 0.4329 | IoU: 0.3346\n",
      "\n",
      "Starting epoch: 88 | phase: train | ⏰: 21:56:11\n",
      "Loss: 0.7004 | dice: 0.7204 | dice_neg: 0.8949 | dice_pos: 0.5459 | IoU: 0.4306\n",
      "Starting epoch: 88 | phase: val | ⏰: 22:04:51\n",
      "Loss: 1.5245 | dice: 0.6464 | dice_neg: 0.8508 | dice_pos: 0.4420 | IoU: 0.3396\n",
      "\n",
      "Starting epoch: 89 | phase: train | ⏰: 22:06:00\n",
      "Loss: 0.6823 | dice: 0.7175 | dice_neg: 0.8912 | dice_pos: 0.5437 | IoU: 0.4313\n",
      "Starting epoch: 89 | phase: val | ⏰: 22:14:46\n",
      "Loss: 1.7913 | dice: 0.6472 | dice_neg: 0.8508 | dice_pos: 0.4435 | IoU: 0.3443\n",
      "\n",
      "Starting epoch: 90 | phase: train | ⏰: 22:15:55\n",
      "Loss: 0.6917 | dice: 0.7166 | dice_neg: 0.8891 | dice_pos: 0.5441 | IoU: 0.4294\n",
      "Starting epoch: 90 | phase: val | ⏰: 22:24:42\n",
      "Loss: 1.6675 | dice: 0.6499 | dice_neg: 0.8697 | dice_pos: 0.4301 | IoU: 0.3305\n",
      "\n",
      "Starting epoch: 91 | phase: train | ⏰: 22:25:50\n",
      "Loss: 0.6508 | dice: 0.7151 | dice_neg: 0.8860 | dice_pos: 0.5443 | IoU: 0.4296\n",
      "Starting epoch: 91 | phase: val | ⏰: 22:34:39\n",
      "Loss: 1.7920 | dice: 0.6493 | dice_neg: 0.8739 | dice_pos: 0.4247 | IoU: 0.3183\n",
      "\n",
      "Starting epoch: 92 | phase: train | ⏰: 22:35:48\n",
      "Loss: 0.6856 | dice: 0.7106 | dice_neg: 0.8818 | dice_pos: 0.5394 | IoU: 0.4272\n",
      "Starting epoch: 92 | phase: val | ⏰: 22:44:38\n",
      "Loss: 1.7668 | dice: 0.6507 | dice_neg: 0.8697 | dice_pos: 0.4317 | IoU: 0.3309\n",
      "\n",
      "Starting epoch: 93 | phase: train | ⏰: 22:45:46\n",
      "Loss: 0.7186 | dice: 0.7164 | dice_neg: 0.8865 | dice_pos: 0.5463 | IoU: 0.4282\n",
      "Starting epoch: 93 | phase: val | ⏰: 22:54:36\n",
      "Loss: 1.5932 | dice: 0.6506 | dice_neg: 0.8676 | dice_pos: 0.4336 | IoU: 0.3435\n",
      "\n",
      "Starting epoch: 94 | phase: train | ⏰: 22:55:40\n",
      "Loss: 0.6623 | dice: 0.7185 | dice_neg: 0.8917 | dice_pos: 0.5452 | IoU: 0.4327\n",
      "Starting epoch: 94 | phase: val | ⏰: 23:04:29\n",
      "Loss: 1.6727 | dice: 0.6451 | dice_neg: 0.8508 | dice_pos: 0.4393 | IoU: 0.3340\n",
      "\n",
      "Starting epoch: 95 | phase: train | ⏰: 23:05:38\n",
      "Loss: 0.6811 | dice: 0.7172 | dice_neg: 0.8891 | dice_pos: 0.5453 | IoU: 0.4309\n",
      "Starting epoch: 95 | phase: val | ⏰: 23:14:26\n",
      "Loss: 1.6109 | dice: 0.6468 | dice_neg: 0.8487 | dice_pos: 0.4449 | IoU: 0.3278\n",
      "\n",
      "Starting epoch: 96 | phase: train | ⏰: 23:15:37\n",
      "Loss: 0.7117 | dice: 0.7150 | dice_neg: 0.8791 | dice_pos: 0.5509 | IoU: 0.4320\n",
      "Starting epoch: 96 | phase: val | ⏰: 23:24:23\n",
      "Loss: 1.7273 | dice: 0.6476 | dice_neg: 0.8529 | dice_pos: 0.4422 | IoU: 0.3395\n",
      "\n",
      "Starting epoch: 97 | phase: train | ⏰: 23:25:33\n",
      "Loss: 0.6544 | dice: 0.7156 | dice_neg: 0.8833 | dice_pos: 0.5478 | IoU: 0.4318\n",
      "Starting epoch: 97 | phase: val | ⏰: 23:34:20\n",
      "Loss: 1.6368 | dice: 0.6431 | dice_neg: 0.8466 | dice_pos: 0.4395 | IoU: 0.3289\n",
      "\n",
      "Starting epoch: 98 | phase: train | ⏰: 23:35:27\n",
      "Loss: 0.6845 | dice: 0.7123 | dice_neg: 0.8786 | dice_pos: 0.5460 | IoU: 0.4307\n",
      "Starting epoch: 98 | phase: val | ⏰: 23:44:13\n",
      "Loss: 1.9288 | dice: 0.6434 | dice_neg: 0.8382 | dice_pos: 0.4486 | IoU: 0.3336\n",
      "\n",
      "Starting epoch: 99 | phase: train | ⏰: 23:45:23\n",
      "Loss: 0.6432 | dice: 0.7241 | dice_neg: 0.8996 | dice_pos: 0.5485 | IoU: 0.4341\n",
      "Starting epoch: 99 | phase: val | ⏰: 23:54:05\n",
      "Loss: 1.6740 | dice: 0.6491 | dice_neg: 0.8655 | dice_pos: 0.4327 | IoU: 0.3324\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_trainer = Trainer(model)\n",
    "model_trainer.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PLOT TRAINING\n",
    "losses = model_trainer.losses\n",
    "dice_scores = model_trainer.dice_scores # overall dice\n",
    "iou_scores = model_trainer.iou_scores\n",
    "\n",
    "def plot(scores, name):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(range(len(scores[\"train\"])), scores[\"train\"], label=f'train {name}')\n",
    "    plt.plot(range(len(scores[\"train\"])), scores[\"val\"], label=f'val {name}')\n",
    "    plt.title(f'{name} plot'); plt.xlabel('Epoch'); plt.ylabel(f'{name}');\n",
    "    plt.legend(); \n",
    "    plt.show()\n",
    "\n",
    "plot(losses, \"BCE loss\")\n",
    "plot(dice_scores, \"Dice score\")\n",
    "plot(iou_scores, \"IoU score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, root, df, size, mean, std, tta=4):\n",
    "        self.root = root\n",
    "        self.size = size\n",
    "        self.fnames = list(df[\"ImageId\"])\n",
    "        self.num_samples = len(self.fnames)\n",
    "        self.transform = Compose(\n",
    "            [\n",
    "                Normalize(mean=mean, std=std, p=1),\n",
    "                Resize(size, size),\n",
    "                ToTensorV2(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.fnames[idx]\n",
    "        path = os.path.join(self.root, fname + \".png\")\n",
    "        image = cv2.imread(path)\n",
    "        images = self.transform(image=image)[\"image\"]\n",
    "        return images\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "def post_process(probability, threshold, min_size):\n",
    "    mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]\n",
    "    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n",
    "    predictions = np.zeros((1024, 1024), np.float32)\n",
    "    num = 0\n",
    "    for c in range(1, num_component):\n",
    "        p = (component == c)\n",
    "        if p.sum() > min_size:\n",
    "            predictions[p] = 1\n",
    "            num += 1\n",
    "    return predictions, num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_submission_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-3c9681cfedf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmin_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_submission_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m testset = DataLoader(\n\u001b[1;32m     11\u001b[0m     \u001b[0mTestDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_submission_path' is not defined"
     ]
    }
   ],
   "source": [
    "size = 512\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "num_workers = 8\n",
    "batch_size = 16\n",
    "best_threshold = 0.5\n",
    "min_size = 3500\n",
    "device = torch.device(\"cuda:0\")\n",
    "df = pd.read_csv(sample_submission_path)\n",
    "testset = DataLoader(\n",
    "    TestDataset(test_data_folder, df, size, mean, std),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "model = model_trainer.net # get the model from model_trainer object\n",
    "model.eval()\n",
    "state = torch.load('./model.pth', map_location=lambda storage, loc: storage)\n",
    "model.load_state_dict(state[\"state_dict\"])\n",
    "encoded_pixels = []\n",
    "for i, batch in enumerate(tqdm(testset)):\n",
    "    preds = torch.sigmoid(model(batch.to(device)))\n",
    "    preds = preds.detach().cpu().numpy()[:, 0, :, :] # (batch_size, 1, size, size) -> (batch_size, size, size)\n",
    "    for probability in preds:\n",
    "        if probability.shape != (1024, 1024):\n",
    "            probability = cv2.resize(probability, dsize=(1024, 1024), interpolation=cv2.INTER_LINEAR)\n",
    "        predict, num_predict = post_process(probability, best_threshold, min_size)\n",
    "        if num_predict == 0:\n",
    "            encoded_pixels.append('-1')\n",
    "        else:\n",
    "            r = run_length_encode(predict)\n",
    "            encoded_pixels.append(r)\n",
    "df['EncodedPixels'] = encoded_pixels\n",
    "df.to_csv('submission.csv', columns=['ImageId', 'EncodedPixels'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/facebookresearch/moco/blob/colab-notebook/colab/moco_cifar10_demo.ipynb",
     "timestamp": 1677256490794
    }
   ]
  },
  "environment": {
   "name": "pytorch-gpu.1-4.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06768cfdf690415e9affdf2a74e59a30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "17032952ca804b558931b93c0fde1540": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cc091016bb8d423f80dddca00096ba0b",
       "IPY_MODEL_6bbbd0d7f0f749939610ccc1aa47bfec",
       "IPY_MODEL_eb9b54187a9b4404b93ce96cb5297a1e"
      ],
      "layout": "IPY_MODEL_48854cfeccd84183a1819d703353a4fa"
     }
    },
    "1d94bafa0c204861869e3c8656f88338": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20faf3fda02c4257b5ac68099de45581": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "23596ead25e842da821eee9281ddd44b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce5a59adc10e4ad3bbd5ee949704a493",
      "placeholder": "​",
      "style": "IPY_MODEL_2e92aabde375495f880ae9752b55e057",
      "value": " 4.64k/4.64k [00:00&lt;00:00, 115kB/s]"
     }
    },
    "2e92aabde375495f880ae9752b55e057": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "31ea43544fcd44a19f86c920afaa12ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3deab35d86db420c963d21fde4f4f770": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1d94bafa0c204861869e3c8656f88338",
      "placeholder": "​",
      "style": "IPY_MODEL_06768cfdf690415e9affdf2a74e59a30",
      "value": "Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"
     }
    },
    "3e187d79da6f477180cac4b95d949388": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "48854cfeccd84183a1819d703353a4fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5528a5ca02bc4cde9fa1bad142519e35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_79cc1507b29d4125ae5e3c704b2b8b75",
      "placeholder": "​",
      "style": "IPY_MODEL_31ea43544fcd44a19f86c920afaa12ab",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "55606bf393b940278f76baf65170eeab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5bffaeb6f389499c958f4a12dca192e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5d806cd2fe7e4424b7fb90371815f4d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3deab35d86db420c963d21fde4f4f770",
       "IPY_MODEL_e3214b7f823d4a12bed90f36b6cd3bbe",
       "IPY_MODEL_6bec28962d4740aabe8e9896e00134b7"
      ],
      "layout": "IPY_MODEL_5da754b21ef34dc9a7eff14bdc3a34bc"
     }
    },
    "5da754b21ef34dc9a7eff14bdc3a34bc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6bbbd0d7f0f749939610ccc1aa47bfec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4d8fcd96c2b44c1a21e6a0cf46d736d",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5bffaeb6f389499c958f4a12dca192e4",
      "value": 170498071
     }
    },
    "6bec28962d4740aabe8e9896e00134b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d083420df95d42ecb16cedd5037dc2a3",
      "placeholder": "​",
      "style": "IPY_MODEL_55606bf393b940278f76baf65170eeab",
      "value": " 223M/223M [00:02&lt;00:00, 86.7MB/s]"
     }
    },
    "73487b45ea444f8e81410a9627c85b40": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79cc1507b29d4125ae5e3c704b2b8b75": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7cdb20bd656249fe85674962d2a4ba7e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9cf2f5d1a7894a73861ce8bd69675ed1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a4d8fcd96c2b44c1a21e6a0cf46d736d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aeabbf27ff5e4ef5a0c3de652c86d632": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5528a5ca02bc4cde9fa1bad142519e35",
       "IPY_MODEL_c3b291f042fe422aa3a2b22f1208d0ca",
       "IPY_MODEL_23596ead25e842da821eee9281ddd44b"
      ],
      "layout": "IPY_MODEL_d74ac37d81974a4780581b554f3e7d23"
     }
    },
    "c3b291f042fe422aa3a2b22f1208d0ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73487b45ea444f8e81410a9627c85b40",
      "max": 4642,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3e187d79da6f477180cac4b95d949388",
      "value": 4642
     }
    },
    "cc091016bb8d423f80dddca00096ba0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7cdb20bd656249fe85674962d2a4ba7e",
      "placeholder": "​",
      "style": "IPY_MODEL_20faf3fda02c4257b5ac68099de45581",
      "value": "100%"
     }
    },
    "ce5a59adc10e4ad3bbd5ee949704a493": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf90228e48af46809d35402d94eb568e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d083420df95d42ecb16cedd5037dc2a3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d74ac37d81974a4780581b554f3e7d23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9070ee51e9c4ebe868777345a25a62d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3214b7f823d4a12bed90f36b6cd3bbe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f48ad91e88b340258d9fbe66bc58696b",
      "max": 223137427,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9cf2f5d1a7894a73861ce8bd69675ed1",
      "value": 223137427
     }
    },
    "eb9b54187a9b4404b93ce96cb5297a1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d9070ee51e9c4ebe868777345a25a62d",
      "placeholder": "​",
      "style": "IPY_MODEL_cf90228e48af46809d35402d94eb568e",
      "value": " 170498071/170498071 [00:14&lt;00:00, 14802420.28it/s]"
     }
    },
    "f48ad91e88b340258d9fbe66bc58696b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
