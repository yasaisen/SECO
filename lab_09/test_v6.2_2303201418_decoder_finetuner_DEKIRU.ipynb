{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MXcbCW8f7Eh"
   },
   "source": [
    "### Some Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# !pip install segmentation_models_pytorch\n",
    "# !pip uninstall opencv-python\n",
    "# !pip install opencv-contrib-python\n",
    "# !pip install numpy==1.21.6\n",
    "# !apt-get update\n",
    "# !apt-get install ffmpeg\n",
    "# !apt-get install libsm6\n",
    "# !apt-get install libxext6\n",
    "# !apt-get install unzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar 20 14:02:08 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.86.01    Driver Version: 515.86.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   45C    P5    21W /  N/A |     30MiB /  6144MiB |      3%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2423      G   /usr/lib/xorg/Xorg                 29MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### for_multi_GPU\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import torch\n",
    "\n",
    "# print(torch.cuda.device_count())\n",
    "# print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5122,
     "status": "ok",
     "timestamp": 1677512171549,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "CvAFrPlS4bLU",
    "outputId": "2900b98f-10a0-48ae-ae6a-0aff7f787c20"
   },
   "outputs": [],
   "source": [
    "# torch.multiprocessing.set_start_method('spawn')\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import pdb\n",
    "import time\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from PIL import Image\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader, Dataset, sampler\n",
    "from matplotlib import pyplot as plt\n",
    "from albumentations import (HorizontalFlip, ShiftScaleRotate, Normalize, Resize, Compose, GaussNoise)\n",
    "# from albumentations.torch import ToTensorV2\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_rtoBGhgSae"
   },
   "source": [
    "### Set Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30443,
     "status": "ok",
     "timestamp": 1677512201987,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "UKCi74HiH1A9",
    "outputId": "779d592f-9214-477e-c922-233d7845cb7c"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "ROOT_PATH = '/home/yasaisen/Desktop/11_research/11_research_main/lab_08'\n",
    "# ROOT_PATH = '/home'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_submission_path = '../input/siim-acr-pneumothorax-segmentation/sample_submission.csv'\n",
    "# sample_submission_path = '/content/drive/My Drive/11_research_main/lab_01/CXR_Dataset/stage_2_sample_submission.csv'\n",
    "\n",
    "train_rle_path = os.path.join(ROOT_PATH, 'SIIM_ACR_dataset', 'train-rle.csv')\n",
    "data_folder = os.path.join(ROOT_PATH, 'SIIM_ACR_dataset', 'train_png')\n",
    "test_data_folder = os.path.join(ROOT_PATH, 'SIIM_ACR_dataset', 'test_png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1677512201988,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "Uh-pffjlIDDw"
   },
   "outputs": [],
   "source": [
    "def checkpath(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2263,
     "status": "ok",
     "timestamp": 1677512204231,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "SRDsG25wIKf-"
   },
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "Version = '230312_v0.1.0'\n",
    "\n",
    "root_folder = os.path.abspath(os.path.join(ROOT_PATH, Version))\n",
    "\n",
    "# model_DIR = os.path.abspath(os.path.join(root_folder, 'model'))\n",
    "# checkpath(root_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupVit Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2463,
     "status": "ok",
     "timestamp": 1677512217559,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "1tdXYOBI7W7Q"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 14:02:11.071245: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-20 14:02:11.165449: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-20 14:02:11.564891: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/yasaisen/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2023-03-20 14:02:11.564961: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/yasaisen/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2023-03-20 14:02:11.564964: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import collections.abc\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "\n",
    "# from ...activations import ACT2FN\n",
    "from transformers.activations import ACT2FN\n",
    "\n",
    "# from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n",
    "from transformers.modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n",
    "# from ...modeling_utils import PreTrainedModel\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "# from ...utils import (\n",
    "#     ModelOutput,\n",
    "#     add_start_docstrings,\n",
    "#     add_start_docstrings_to_model_forward,\n",
    "#     logging,\n",
    "#     replace_return_docstrings,\n",
    "# )\n",
    "from transformers.utils import (\n",
    "    ModelOutput,\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "# from .configuration_groupvit import GroupViTConfig, GroupViTTextConfig, GroupViTVisionConfig\n",
    "from transformers.models.groupvit.configuration_groupvit import GroupViTConfig, GroupViTTextConfig #, GroupViTVisionConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from typing import TYPE_CHECKING, Any, Mapping, Optional, Union\n",
    "\n",
    "# from ...configuration_utils import PretrainedConfig\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "# from ...onnx import OnnxConfig\n",
    "from transformers.onnx import OnnxConfig\n",
    "# from ...utils import logging\n",
    "from transformers.utils import logging\n",
    "\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    # from ...processing_utils import ProcessorMixin\n",
    "    from transformers.processing_utils import ProcessorMixin\n",
    "    # from ...utils import TensorType\n",
    "    from transformers.utils import TensorType\n",
    "\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "GROUPVIT_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n",
    "    \"nvidia/groupvit-gcc-yfcc\": \"https://huggingface.co/nvidia/groupvit-gcc-yfcc/resolve/main/config.json\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupViTVisionConfig(PretrainedConfig):\n",
    "    r\"\"\"\n",
    "    This is the configuration class to store the configuration of a [`GroupViTVisionModel`]. It is used to instantiate\n",
    "    an GroupViT model according to the specified arguments, defining the model architecture. Instantiating a\n",
    "    configuration with the defaults will yield a similar configuration to that of the GroupViT\n",
    "    [nvidia/groupvit-gcc-yfcc](https://huggingface.co/nvidia/groupvit-gcc-yfcc) architecture.\n",
    "\n",
    "    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n",
    "    documentation from [`PretrainedConfig`] for more information.\n",
    "\n",
    "    Args:\n",
    "        hidden_size (`int`, *optional*, defaults to 384):\n",
    "            Dimensionality of the encoder layers and the pooler layer.\n",
    "        intermediate_size (`int`, *optional*, defaults to 1536):\n",
    "            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n",
    "        depths (`List[int]`, *optional*, defaults to [6, 3, 3]):\n",
    "            The number of layers in each encoder block.\n",
    "        num_group_tokens (`List[int]`, *optional*, defaults to [64, 8, 0]):\n",
    "            The number of group tokens for each stage.\n",
    "        num_output_groups (`List[int]`, *optional*, defaults to [64, 8, 8]):\n",
    "            The number of output groups for each stage, 0 means no group.\n",
    "        num_attention_heads (`int`, *optional*, defaults to 6):\n",
    "            Number of attention heads for each attention layer in the Transformer encoder.\n",
    "        image_size (`int`, *optional*, defaults to 224):\n",
    "            The size (resolution) of each image.\n",
    "        patch_size (`int`, *optional*, defaults to 16):\n",
    "            The size (resolution) of each patch.\n",
    "        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n",
    "            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n",
    "            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` ``\"quick_gelu\"` are supported.\n",
    "        layer_norm_eps (`float`, *optional*, defaults to 1e-5):\n",
    "            The epsilon used by the layer normalization layers.\n",
    "        dropout (`float`, *optional*, defaults to 0.0):\n",
    "            The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.\n",
    "        attention_dropout (`float`, *optional*, defaults to 0.0):\n",
    "            The dropout ratio for the attention probabilities.\n",
    "        initializer_range (`float`, *optional*, defaults to 0.02):\n",
    "            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
    "        initializer_factor (`float`, *optional*, defaults to 1.0):\n",
    "            A factor for initializing all weight matrices (should be kept to 1, used internally for initialization\n",
    "            testing).\n",
    "\n",
    "    Example:\n",
    "\n",
    "    ```python\n",
    "    >>> from transformers import GroupViTVisionConfig, GroupViTVisionModel\n",
    "\n",
    "    >>> # Initializing a GroupViTVisionModel with nvidia/groupvit-gcc-yfcc style configuration\n",
    "    >>> configuration = GroupViTVisionConfig()\n",
    "\n",
    "    >>> model = GroupViTVisionModel(configuration)\n",
    "\n",
    "    >>> # Accessing the model configuration\n",
    "    >>> configuration = model.config\n",
    "    ```\"\"\"\n",
    "\n",
    "    model_type = \"groupvit_vision_model\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=384,\n",
    "        intermediate_size=1536,\n",
    "        depths=[6, 3, 3],\n",
    "        num_hidden_layers=12,\n",
    "        num_group_tokens=[64, 8, 0],\n",
    "        num_output_groups=[64, 8, 8],\n",
    "        num_attention_heads=6,\n",
    "        image_size=1024,\n",
    "        patch_size=16,\n",
    "        num_channels=3,\n",
    "        hidden_act=\"gelu\",\n",
    "        layer_norm_eps=1e-5,\n",
    "        dropout=0.0,\n",
    "        attention_dropout=0.0,\n",
    "        initializer_range=0.02,\n",
    "        initializer_factor=1.0,\n",
    "        assign_eps=1.0,\n",
    "        assign_mlp_ratio=[0.5, 4],\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.depths = depths\n",
    "        if num_hidden_layers != sum(depths):\n",
    "            logger.warning(\n",
    "                f\"Manually setting num_hidden_layers to {num_hidden_layers}, but we expect num_hidden_layers =\"\n",
    "                f\" sum(depth) = {sum(depths)}\"\n",
    "            )\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_group_tokens = num_group_tokens\n",
    "        self.num_output_groups = num_output_groups\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_channels = num_channels\n",
    "        self.hidden_act = hidden_act\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.dropout = dropout\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.initializer_range = initializer_range\n",
    "        self.initializer_factor = initializer_factor\n",
    "        self.assign_eps = assign_eps\n",
    "        self.assign_mlp_ratio = assign_mlp_ratio\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n",
    "        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
    "\n",
    "        # get the vision config dict if we are loading from GroupViTConfig\n",
    "        if config_dict.get(\"model_type\") == \"groupvit\":\n",
    "            config_dict = config_dict[\"vision_config\"]\n",
    "\n",
    "        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n",
    "            logger.warning(\n",
    "                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n",
    "                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n",
    "            )\n",
    "\n",
    "        return cls.from_dict(config_dict, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217559,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "M97ik37v9rbD"
   },
   "outputs": [],
   "source": [
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "_CHECKPOINT_FOR_DOC = \"nvidia/groupvit-gcc-yfcc\"\n",
    "\n",
    "GROUPVIT_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
    "    \"nvidia/groupvit-gcc-yfcc\",\n",
    "    # See all GroupViT models at https://huggingface.co/models?filter=groupvit\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217560,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "TfjSCoDn9utr"
   },
   "outputs": [],
   "source": [
    "# Copied from transformers.models.bart.modeling_bart._expand_mask\n",
    "def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n",
    "    \"\"\"\n",
    "    bsz, src_len = mask.size()\n",
    "    tgt_len = tgt_len if tgt_len is not None else src_len\n",
    "\n",
    "    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
    "\n",
    "    inverted_mask = 1.0 - expanded_mask\n",
    "\n",
    "    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217560,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "6PRDe_fu9xoT"
   },
   "outputs": [],
   "source": [
    "# contrastive loss function, adapted from\n",
    "# https://sachinruk.github.io/blog/pytorch/pytorch%20lightning/loss%20function/gpu/2021/03/07/CLIP.html\n",
    "def contrastive_loss(logits: torch.Tensor) -> torch.Tensor:\n",
    "    return nn.functional.cross_entropy(logits, torch.arange(len(logits), device=logits.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217560,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "NTvFTrtG9z1z"
   },
   "outputs": [],
   "source": [
    "# Copied from transformers.models.clip.modeling_clip.clip_loss with clip->groupvit\n",
    "def groupvit_loss(similarity: torch.Tensor) -> torch.Tensor:\n",
    "    caption_loss = contrastive_loss(similarity)\n",
    "    image_loss = contrastive_loss(similarity.t())\n",
    "    return (caption_loss + image_loss) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217560,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "iOvAJP0L91ns"
   },
   "outputs": [],
   "source": [
    "def hard_softmax(logits: torch.Tensor, dim: int):\n",
    "    y_soft = logits.softmax(dim)\n",
    "    # Straight through.\n",
    "    index = y_soft.max(dim, keepdim=True)[1]\n",
    "    y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(dim, index, 1.0)\n",
    "    ret = y_hard - y_soft.detach() + y_soft\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217560,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "6-b2H_fE93b1"
   },
   "outputs": [],
   "source": [
    "def gumbel_softmax(logits: torch.Tensor, tau: float = 1, hard: bool = False, dim: int = -1) -> torch.Tensor:\n",
    "    # more stable https://github.com/pytorch/pytorch/issues/41663\n",
    "    gumbel_dist = torch.distributions.gumbel.Gumbel(\n",
    "        torch.tensor(0.0, device=logits.device, dtype=logits.dtype),\n",
    "        torch.tensor(1.0, device=logits.device, dtype=logits.dtype),\n",
    "    )\n",
    "    gumbels = gumbel_dist.sample(logits.shape)\n",
    "\n",
    "    gumbels = (logits + gumbels) / tau  # ~Gumbel(logits,tau)\n",
    "    y_soft = gumbels.softmax(dim)\n",
    "\n",
    "    if hard:\n",
    "        # Straight through.\n",
    "        index = y_soft.max(dim, keepdim=True)[1]\n",
    "        y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(dim, index, 1.0)\n",
    "        ret = y_hard - y_soft.detach() + y_soft\n",
    "    else:\n",
    "        # Reparametrization trick.\n",
    "        ret = y_soft\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217561,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "sh78Xa0q9564"
   },
   "outputs": [],
   "source": [
    "def resize_attention_map(attentions, height, width, align_corners=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        attentions (`torch.Tensor`): attention map of shape [batch_size, groups, feat_height*feat_width]\n",
    "        height (`int`): height of the output attention map\n",
    "        width (`int`): width of the output attention map\n",
    "        align_corners (`bool`, *optional*): the `align_corner` argument for `nn.functional.interpolate`.\n",
    "\n",
    "    Returns:\n",
    "        `torch.Tensor`: resized attention map of shape [batch_size, groups, height, width]\n",
    "    \"\"\"\n",
    "\n",
    "    scale = (height * width // attentions.shape[2]) ** 0.5\n",
    "    if height > width:\n",
    "        feat_width = int(np.round(width / scale))\n",
    "        feat_height = attentions.shape[2] // feat_width\n",
    "    else:\n",
    "        feat_height = int(np.round(height / scale))\n",
    "        feat_width = attentions.shape[2] // feat_height\n",
    "\n",
    "    batch_size = attentions.shape[0]\n",
    "    groups = attentions.shape[1]  # number of group token\n",
    "    # [batch_size, groups, height*width, groups] -> [batch_size, groups, height, width]\n",
    "    attentions = attentions.reshape(batch_size, groups, feat_height, feat_width)\n",
    "    attentions = nn.functional.interpolate(\n",
    "        attentions, size=(height, width), mode=\"bilinear\", align_corners=align_corners\n",
    "    )\n",
    "    return attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217561,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "KyZtU92l98ix"
   },
   "outputs": [],
   "source": [
    "def get_grouping_from_attentions(attentions, hw_shape):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        attentions (`tuple(torch.FloatTensor)`: tuple of attention maps returned by `GroupViTVisionTransformer`\n",
    "        hw_shape (`tuple(int)`): height and width of the output attention map\n",
    "    Returns:\n",
    "        `torch.Tensor`: the attention map of shape [batch_size, groups, height, width]\n",
    "    \"\"\"\n",
    "\n",
    "    attn_maps = []\n",
    "    with torch.no_grad():\n",
    "        prev_attn_masks = None\n",
    "        for attn_masks in attentions:\n",
    "            # [batch_size, num_groups, height x width] -> [batch_size, height x width, num_groups]\n",
    "            attn_masks = attn_masks.permute(0, 2, 1).contiguous()\n",
    "            if prev_attn_masks is None:\n",
    "                prev_attn_masks = attn_masks\n",
    "            else:\n",
    "                prev_attn_masks = prev_attn_masks @ attn_masks\n",
    "            # [batch_size, heightxwidth, num_groups] -> [batch_size, num_groups, heightxwidth] -> [batch_size, num_groups, height, width]\n",
    "            cur_attn_map = resize_attention_map(prev_attn_masks.permute(0, 2, 1).contiguous(), *hw_shape)\n",
    "            attn_maps.append(cur_attn_map)\n",
    "\n",
    "    # [batch_size, num_groups, height, width]\n",
    "    final_grouping = attn_maps[-1]\n",
    "\n",
    "    return final_grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217561,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "sX37CrAn9_Mw"
   },
   "outputs": [],
   "source": [
    "class GroupViTCrossAttentionLayer(nn.Module):\n",
    "    def __init__(self, config: GroupViTVisionConfig):\n",
    "        super().__init__()\n",
    "        self.attn = GroupViTAttention(config)\n",
    "        self.norm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.mlp = GroupViTMLP(config)\n",
    "        self.norm_post = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, query, key):\n",
    "        x = query\n",
    "        x = x + self.attn(query, encoder_hidden_states=key)[0]\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        x = self.norm_post(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217561,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "LfMDcjbu-BFh"
   },
   "outputs": [],
   "source": [
    "class GroupViTAssignAttention(nn.Module):\n",
    "    def __init__(self, config: GroupViTVisionConfig):\n",
    "        super().__init__()\n",
    "        self.scale = config.hidden_size**-0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.proj = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.assign_eps = config.assign_eps\n",
    "\n",
    "    def get_attn(self, attn, gumbel=True, hard=True):\n",
    "        if gumbel and self.training:\n",
    "            attn = gumbel_softmax(attn, dim=-2, hard=hard)\n",
    "        else:\n",
    "            if hard:\n",
    "                attn = hard_softmax(attn, dim=-2)\n",
    "            else:\n",
    "                attn = nn.functional.softmax(attn, dim=-2)\n",
    "\n",
    "        return attn\n",
    "\n",
    "    def forward(self, query, key):\n",
    "        value = key\n",
    "        # [batch_size, query_length, channels]\n",
    "        query = self.q_proj(query)\n",
    "\n",
    "        # [batch_size, key_length, channels]\n",
    "        key = self.k_proj(key)\n",
    "\n",
    "        # [batch_size, key_length, channels]\n",
    "        value = self.v_proj(value)\n",
    "\n",
    "        # [batch_size, query_length, key_length]\n",
    "        raw_attn = (query @ key.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        attn = self.get_attn(raw_attn)\n",
    "        soft_attn = self.get_attn(raw_attn, gumbel=False, hard=False)\n",
    "\n",
    "        attn = attn / (attn.sum(dim=-1, keepdim=True) + self.assign_eps)\n",
    "\n",
    "        out = attn @ value\n",
    "\n",
    "        out = self.proj(out)\n",
    "\n",
    "        return out, soft_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217562,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "yOJBrOGt-GKD"
   },
   "outputs": [],
   "source": [
    "class GroupViTTokenAssign(nn.Module):\n",
    "    def __init__(self, config: GroupViTVisionConfig, num_group_token, num_output_group):\n",
    "        super().__init__()\n",
    "        self.num_output_group = num_output_group\n",
    "        # norm on group_tokens\n",
    "        self.norm_tokens = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        assign_mlp_ratio = (\n",
    "            config.assign_mlp_ratio\n",
    "            if isinstance(config.assign_mlp_ratio, collections.abc.Iterable)\n",
    "            else (config.assign_mlp_ratio, config.assign_mlp_ratio)\n",
    "        )\n",
    "        tokens_dim, channels_dim = [int(x * config.hidden_size) for x in assign_mlp_ratio]\n",
    "        self.mlp_inter = GroupViTMixerMLP(config, num_group_token, tokens_dim, num_output_group)\n",
    "        self.norm_post_tokens = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        # norm on x\n",
    "        self.norm_x = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.pre_assign_attn = GroupViTCrossAttentionLayer(config)\n",
    "\n",
    "        self.assign = GroupViTAssignAttention(config)\n",
    "        self.norm_new_x = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.mlp_channels = GroupViTMLP(config, config.hidden_size, channels_dim, config.hidden_size)\n",
    "\n",
    "    def project_group_token(self, group_tokens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            group_tokens (torch.Tensor): group tokens, [batch_size, num_group_tokens, channels]\n",
    "\n",
    "        Returns:\n",
    "            projected_group_tokens (torch.Tensor): [batch_size, num_output_groups, channels]\n",
    "        \"\"\"\n",
    "        # [B, num_output_groups, C] <- [B, num_group_tokens, C]\n",
    "        projected_group_tokens = self.mlp_inter(group_tokens)\n",
    "        projected_group_tokens = self.norm_post_tokens(projected_group_tokens)\n",
    "        return projected_group_tokens\n",
    "\n",
    "    def forward(self, image_tokens, group_tokens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_tokens (`torch.Tensor`): image tokens, of shape [batch_size, input_length, channels]\n",
    "            group_tokens (`torch.Tensor`): group tokens, [batch_size, num_group_tokens, channels]\n",
    "        \"\"\"\n",
    "\n",
    "        group_tokens = self.norm_tokens(group_tokens)\n",
    "        image_tokens = self.norm_x(image_tokens)\n",
    "        # [batch_size, num_output_groups, channels]\n",
    "        projected_group_tokens = self.project_group_token(group_tokens)\n",
    "        projected_group_tokens = self.pre_assign_attn(projected_group_tokens, image_tokens)\n",
    "        new_image_tokens, attention = self.assign(projected_group_tokens, image_tokens)\n",
    "        new_image_tokens += projected_group_tokens\n",
    "\n",
    "        new_image_tokens = new_image_tokens + self.mlp_channels(self.norm_new_x(new_image_tokens))\n",
    "\n",
    "        return new_image_tokens, attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217562,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "1eBLeUct-JCq"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GroupViTModelOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n",
    "            Contrastive loss for image-text similarity.\n",
    "        logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n",
    "            The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n",
    "            similarity scores.\n",
    "        logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n",
    "            The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n",
    "            similarity scores.\n",
    "        segmentation_logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels, logits_height, logits_width)`):\n",
    "            Classification scores for each pixel.\n",
    "\n",
    "            <Tip warning={true}>\n",
    "\n",
    "            The logits returned do not necessarily have the same size as the `pixel_values` passed as inputs. This is\n",
    "            to avoid doing two interpolations and lose some quality when a user needs to resize the logits to the\n",
    "            original image size as post-processing. You should always check your logits shape and resize as needed.\n",
    "\n",
    "            </Tip>\n",
    "\n",
    "        text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n",
    "            The text embeddings obtained by applying the projection layer to the pooled output of\n",
    "            [`GroupViTTextModel`].\n",
    "        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n",
    "            The image embeddings obtained by applying the projection layer to the pooled output of\n",
    "            [`GroupViTVisionModel`].\n",
    "        text_model_output (`BaseModelOutputWithPooling`):\n",
    "            The output of the [`GroupViTTextModel`].\n",
    "        vision_model_output (`BaseModelOutputWithPooling`):\n",
    "            The output of the [`GroupViTVisionModel`].\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits_per_image: torch.FloatTensor = None\n",
    "    logits_per_text: torch.FloatTensor = None\n",
    "    segmentation_logits: torch.FloatTensor = None\n",
    "    text_embeds: torch.FloatTensor = None\n",
    "    image_embeds: torch.FloatTensor = None\n",
    "    text_model_output: BaseModelOutputWithPooling = None\n",
    "    vision_model_output: BaseModelOutputWithPooling = None\n",
    "\n",
    "    def to_tuple(self) -> Tuple[Any]:\n",
    "        return tuple(\n",
    "            self[k] if k not in [\"text_model_output\", \"vision_model_output\"] else getattr(self, k).to_tuple()\n",
    "            for k in self.keys()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217562,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "8atGssmE-OgA"
   },
   "outputs": [],
   "source": [
    "class GroupViTPatchEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Image to Patch Embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int = 224,\n",
    "        patch_size: Union[int, Tuple[int, int]] = 16,\n",
    "        num_channels: int = 3,\n",
    "        embed_dim: int = 768,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        image_size = image_size if isinstance(image_size, collections.abc.Iterable) else (image_size, image_size)\n",
    "        patch_size = patch_size if isinstance(patch_size, collections.abc.Iterable) else (patch_size, patch_size)\n",
    "        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.projection = nn.Conv2d(num_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor, interpolate_pos_encoding: bool = False) -> torch.Tensor:\n",
    "        batch_size, num_channels, height, width = pixel_values.shape\n",
    "        if not interpolate_pos_encoding:\n",
    "            if height != self.image_size[0] or width != self.image_size[1]:\n",
    "                raise ValueError(\n",
    "                    f\"Input image size ({height}*{width}) doesn't match model\"\n",
    "                    f\" ({self.image_size[0]}*{self.image_size[1]}).\"\n",
    "                )\n",
    "        x = self.projection(pixel_values).flatten(2).transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217562,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "pvmCYxCX-Q5x"
   },
   "outputs": [],
   "source": [
    "class GroupViTVisionEmbeddings(nn.Module):\n",
    "    def __init__(self, config: GroupViTVisionConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embeddings = GroupViTPatchEmbeddings(\n",
    "            image_size=config.image_size,\n",
    "            patch_size=config.patch_size,\n",
    "            num_channels=config.num_channels,\n",
    "            embed_dim=config.hidden_size,\n",
    "        )\n",
    "        num_patches = self.patch_embeddings.num_patches\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches, config.hidden_size))\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.config = config\n",
    "\n",
    "    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher\n",
    "        resolution images.\n",
    "\n",
    "        Source:\n",
    "        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n",
    "        \"\"\"\n",
    "\n",
    "        npatch = embeddings.shape[1]\n",
    "        if npatch == self.position_embeddings.shape[1] and height == width:\n",
    "            return self.position_embeddings\n",
    "        patch_pos_embed = self.position_embeddings\n",
    "        num_original_pos_embed = patch_pos_embed.shape[1]\n",
    "        dim = embeddings.shape[-1]\n",
    "        feat_height = height // self.config.patch_size\n",
    "        feat_width = width // self.config.patch_size\n",
    "        # we add a small number to avoid floating point error in the interpolation\n",
    "        # see discussion at https://github.com/facebookresearch/dino/issues/8\n",
    "        feat_height, feat_width = feat_height + 0.1, feat_width + 0.1\n",
    "        original_height = original_width = math.sqrt(num_original_pos_embed)\n",
    "        reshaped_patch_pos_embed = patch_pos_embed.reshape(1, int(original_height), int(original_width), dim).permute(\n",
    "            0, 3, 1, 2\n",
    "        )\n",
    "        scale_factor = (feat_height / original_height, feat_width / original_width)\n",
    "        patch_pos_embed = nn.functional.interpolate(\n",
    "            reshaped_patch_pos_embed,\n",
    "            scale_factor=scale_factor,\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return patch_pos_embed\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor, interpolate_pos_encoding: bool = False) -> torch.Tensor:\n",
    "        batch_size, num_channels, height, width = pixel_values.shape\n",
    "        embeddings = self.patch_embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n",
    "\n",
    "        embeddings = self.layernorm(embeddings)\n",
    "\n",
    "        batch_size, seq_len, _ = embeddings.size()\n",
    "\n",
    "        # add positional encoding to each token\n",
    "        if interpolate_pos_encoding:\n",
    "            embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)\n",
    "        else:\n",
    "            embeddings = embeddings + self.position_embeddings\n",
    "\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217563,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "jZ_xSDus-ULA"
   },
   "outputs": [],
   "source": [
    "# Copied from transformers.models.clip.modeling_clip.CLIPTextEmbeddings with CLIP->GroupViT\n",
    "class GroupViTTextEmbeddings(nn.Module):\n",
    "    def __init__(self, config: GroupViTTextConfig):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(config.max_position_embeddings, embed_dim)\n",
    "\n",
    "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
    "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        seq_length = input_ids.shape[-1] if input_ids is not None else inputs_embeds.shape[-2]\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, :seq_length]\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.token_embedding(input_ids)\n",
    "\n",
    "        position_embeddings = self.position_embedding(position_ids)\n",
    "        embeddings = inputs_embeds + position_embeddings\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217563,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "WI0LJqF--anN"
   },
   "outputs": [],
   "source": [
    "class GroupViTStage(nn.Module):\n",
    "    \"\"\"This corresponds to the `GroupingLayer` class in the GroupViT implementation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: GroupViTVisionConfig,\n",
    "        depth: int,\n",
    "        num_prev_group_token: int,\n",
    "        num_group_token: int,\n",
    "        num_output_group: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.num_group_token = num_group_token\n",
    "        if num_group_token > 0:\n",
    "            self.group_token = nn.Parameter(torch.zeros(1, num_group_token, config.hidden_size))\n",
    "        else:\n",
    "            self.group_token = None\n",
    "        self.gradient_checkpointing = False\n",
    "        self.layers = nn.ModuleList([GroupViTEncoderLayer(config) for _ in range(depth)])\n",
    "\n",
    "        if num_group_token > 0:\n",
    "            self.downsample = GroupViTTokenAssign(\n",
    "                config=config,\n",
    "                num_group_token=num_group_token,\n",
    "                num_output_group=num_output_group,\n",
    "            )\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "        if num_prev_group_token > 0 and num_group_token > 0:\n",
    "            self.group_projector = nn.Sequential(\n",
    "                nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps),\n",
    "                GroupViTMixerMLP(config, num_prev_group_token, config.hidden_size // 2, num_group_token),\n",
    "            )\n",
    "        else:\n",
    "            self.group_projector = None\n",
    "\n",
    "    @property\n",
    "    def with_group_token(self):\n",
    "        return self.group_token is not None\n",
    "\n",
    "    def split_x(self, x):\n",
    "        if self.with_group_token:\n",
    "            return x[:, : -self.num_group_token], x[:, -self.num_group_token :]\n",
    "        else:\n",
    "            return x, None\n",
    "\n",
    "    def concat_x(self, x: torch.Tensor, group_token: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        if group_token is None:\n",
    "            return x\n",
    "        return torch.cat([x, group_token], dim=1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        prev_group_token: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.FloatTensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`): attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "                `(config.encoder_attention_heads,)`.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the grouping tensors of Grouping block.\n",
    "        \"\"\"\n",
    "        if self.with_group_token:\n",
    "            group_token = self.group_token.expand(hidden_states.size(0), -1, -1)\n",
    "            if self.group_projector is not None:\n",
    "                group_token = group_token + self.group_projector(prev_group_token)\n",
    "        else:\n",
    "            group_token = None\n",
    "\n",
    "        x = hidden_states\n",
    "\n",
    "        cat_x = self.concat_x(x, group_token)\n",
    "        for layer in self.layers:\n",
    "            layer_out = layer(cat_x, attention_mask=None, causal_attention_mask=None)\n",
    "            cat_x = layer_out[0]\n",
    "\n",
    "        x, group_token = self.split_x(cat_x)\n",
    "\n",
    "        attention = None\n",
    "        if self.downsample is not None:\n",
    "            x, attention = self.downsample(x, group_token)\n",
    "\n",
    "        outputs = (x, group_token)\n",
    "        if output_attentions:\n",
    "            outputs = outputs + (attention,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217563,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "lZNeThbG-bdS"
   },
   "outputs": [],
   "source": [
    "class GroupViTMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: GroupViTVisionConfig,\n",
    "        hidden_size: Optional[int] = None,\n",
    "        intermediate_size: Optional[int] = None,\n",
    "        output_size: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.activation_fn = ACT2FN[config.hidden_act]\n",
    "        hidden_size = hidden_size if hidden_size is not None else config.hidden_size\n",
    "        intermediate_size = intermediate_size if intermediate_size is not None else config.intermediate_size\n",
    "        output_size = output_size if output_size is not None else hidden_size\n",
    "        self.fc1 = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.fc2 = nn.Linear(intermediate_size, output_size)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.fc1(hidden_states)\n",
    "        hidden_states = self.activation_fn(hidden_states)\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217563,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "F5PfLQe2-dnX"
   },
   "outputs": [],
   "source": [
    "class GroupViTMixerMLP(GroupViTMLP):\n",
    "    def forward(self, x):\n",
    "        x = super().forward(x.transpose(1, 2))\n",
    "        return x.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217564,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "zIDEv8MG-fPE"
   },
   "outputs": [],
   "source": [
    "class GroupViTAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        if self.head_dim * self.num_heads != self.embed_dim:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n",
    "                f\" {self.num_heads}).\"\n",
    "            )\n",
    "        self.scale = self.head_dim**-0.5\n",
    "        self.dropout = config.attention_dropout\n",
    "\n",
    "        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
    "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        causal_attention_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
    "\n",
    "        bsz, tgt_len, embed_dim = hidden_states.size()\n",
    "        is_cross_attention = encoder_hidden_states is not None\n",
    "\n",
    "        # get query proj\n",
    "        query_states = self.q_proj(hidden_states) * self.scale\n",
    "        if is_cross_attention:\n",
    "            key_states = self._shape(self.k_proj(encoder_hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(encoder_hidden_states), -1, bsz)\n",
    "        else:\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "\n",
    "        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
    "        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
    "        key_states = key_states.view(*proj_shape)\n",
    "        value_states = value_states.view(*proj_shape)\n",
    "\n",
    "        src_len = key_states.size(1)\n",
    "        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
    "\n",
    "        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n",
    "                f\" {attn_weights.size()}\"\n",
    "            )\n",
    "\n",
    "        # apply the causal_attention_mask first\n",
    "        if causal_attention_mask is not None:\n",
    "            if causal_attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is\"\n",
    "                    f\" {causal_attention_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        if output_attentions:\n",
    "            # this operation is a bit akward, but it's required to\n",
    "            # make sure that attn_weights keeps its gradient.\n",
    "            # In order to do so, attn_weights have to reshaped\n",
    "            # twice and have to be reused in the following\n",
    "            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "        else:\n",
    "            attn_weights_reshaped = None\n",
    "\n",
    "        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "\n",
    "        attn_output = torch.bmm(attn_probs, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "        attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
    "\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights_reshaped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217564,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "MxxVauJA-ioM"
   },
   "outputs": [],
   "source": [
    "# Copied from transformers.models.clip.modeling_clip.CLIPEncoderLayer with CLIP->GroupViT\n",
    "class GroupViTEncoderLayer(nn.Module):\n",
    "    def __init__(self, config: GroupViTConfig):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.self_attn = GroupViTAttention(config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n",
    "        self.mlp = GroupViTMLP(config)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        causal_attention_mask: torch.Tensor,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.FloatTensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`): attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "                `(config.encoder_attention_heads,)`.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "        \"\"\"\n",
    "        residual = hidden_states\n",
    "\n",
    "        hidden_states = self.layer_norm1(hidden_states)\n",
    "        hidden_states, attn_weights = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            causal_attention_mask=causal_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.layer_norm2(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs += (attn_weights,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217564,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "iFAIWiAZ-k9h"
   },
   "outputs": [],
   "source": [
    "class GroupViTPreTrainedModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
    "    models.\n",
    "    \"\"\"\n",
    "\n",
    "    config_class = GroupViTConfig\n",
    "    base_model_prefix = \"groupvit\"\n",
    "    supports_gradient_checkpointing = True\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "\n",
    "        init_range = self.config.initializer_range\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=init_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "        factor = self.config.initializer_factor\n",
    "        if isinstance(module, GroupViTTextEmbeddings):\n",
    "            module.token_embedding.weight.data.normal_(mean=0.0, std=factor * 0.02)\n",
    "            module.position_embedding.weight.data.normal_(mean=0.0, std=factor * 0.02)\n",
    "        elif isinstance(module, GroupViTAttention):\n",
    "            factor = self.config.initializer_factor\n",
    "            in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n",
    "            out_proj_std = (module.embed_dim**-0.5) * factor\n",
    "            nn.init.normal_(module.q_proj.weight, std=in_proj_std)\n",
    "            nn.init.normal_(module.k_proj.weight, std=in_proj_std)\n",
    "            nn.init.normal_(module.v_proj.weight, std=in_proj_std)\n",
    "            nn.init.normal_(module.out_proj.weight, std=out_proj_std)\n",
    "        elif isinstance(module, GroupViTMLP):\n",
    "            factor = self.config.initializer_factor\n",
    "            in_proj_std = (\n",
    "                (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n",
    "            )\n",
    "            fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n",
    "            nn.init.normal_(module.fc1.weight, std=fc_std)\n",
    "            nn.init.normal_(module.fc2.weight, std=in_proj_std)\n",
    "\n",
    "    def _set_gradient_checkpointing(self, module, value=False):\n",
    "        if isinstance(module, (GroupViTTextEncoder, GroupViTVisionEncoder)):\n",
    "            module.gradient_checkpointing = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217564,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "jzru1jUe-wAJ"
   },
   "outputs": [],
   "source": [
    "GROUPVIT_START_DOCSTRING = r\"\"\"\n",
    "    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n",
    "    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n",
    "    behavior.\n",
    "\n",
    "    Parameters:\n",
    "        config ([`GroupViTConfig`]): Model configuration class with all the parameters of the model.\n",
    "            Initializing with a config file does not load the weights associated with the model, only the\n",
    "            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
    "\"\"\"\n",
    "\n",
    "GROUPVIT_TEXT_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n",
    "            it.\n",
    "\n",
    "            Indices can be obtained using [`CLIPTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
    "            [`PreTrainedTokenizer.__call__`] for details.\n",
    "\n",
    "            [What are input IDs?](../glossary#input-ids)\n",
    "        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "\n",
    "            [What are attention masks?](../glossary#attention-mask)\n",
    "        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n",
    "            config.max_position_embeddings - 1]`.\n",
    "\n",
    "            [What are position IDs?](../glossary#position-ids)\n",
    "        output_attentions (`bool`, *optional*):\n",
    "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
    "            tensors for more detail.\n",
    "        output_hidden_states (`bool`, *optional*):\n",
    "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
    "            more detail.\n",
    "        return_dict (`bool`, *optional*):\n",
    "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "\"\"\"\n",
    "\n",
    "GROUPVIT_VISION_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n",
    "            Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using\n",
    "            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details.\n",
    "        output_attentions (`bool`, *optional*):\n",
    "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
    "            tensors for more detail.\n",
    "        output_hidden_states (`bool`, *optional*):\n",
    "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
    "            more detail.\n",
    "        return_dict (`bool`, *optional*):\n",
    "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "\"\"\"\n",
    "\n",
    "GROUPVIT_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n",
    "            it.\n",
    "\n",
    "            Indices can be obtained using [`CLIPTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
    "            [`PreTrainedTokenizer.__call__`] for details.\n",
    "\n",
    "            [What are input IDs?](../glossary#input-ids)\n",
    "        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "\n",
    "            [What are attention masks?](../glossary#attention-mask)\n",
    "        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n",
    "            config.max_position_embeddings - 1]`.\n",
    "\n",
    "            [What are position IDs?](../glossary#position-ids)\n",
    "        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n",
    "            Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See\n",
    "            [`CLIPImageProcessor.__call__`] for details.\n",
    "        return_loss (`bool`, *optional*):\n",
    "            Whether or not to return the contrastive loss.\n",
    "        output_attentions (`bool`, *optional*):\n",
    "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
    "            tensors for more detail.\n",
    "        output_hidden_states (`bool`, *optional*):\n",
    "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
    "            more detail.\n",
    "        return_dict (`bool`, *optional*):\n",
    "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217564,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "diwTOUkc-yuM"
   },
   "outputs": [],
   "source": [
    "class GroupViTVisionEncoder(nn.Module):\n",
    "    def __init__(self, config: GroupViTVisionConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.stages = nn.ModuleList(\n",
    "            [\n",
    "                GroupViTStage(\n",
    "                    config=config,\n",
    "                    depth=config.depths[i],\n",
    "                    num_group_token=config.num_group_tokens[i],\n",
    "                    num_output_group=config.num_output_groups[i],\n",
    "                    num_prev_group_token=config.num_output_groups[i - 1] if i > 0 else 0,\n",
    "                )\n",
    "                for i in range(len(config.depths))\n",
    "            ]\n",
    "        )\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[tuple, BaseModelOutput]:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_groupings = () if output_attentions else None\n",
    "\n",
    "        group_tokens = None\n",
    "\n",
    "        for i, stage in enumerate(self.stages):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_outputs = stage(hidden_states, group_tokens, output_attentions)\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            group_tokens = layer_outputs[1]\n",
    "\n",
    "            if output_attentions and layer_outputs[2] is not None:\n",
    "                all_groupings = all_groupings + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, all_hidden_states, all_groupings] if v is not None)\n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=hidden_states, hidden_states=all_hidden_states, attentions=all_groupings\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677512217565,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "ErgnsQgL-zvh"
   },
   "outputs": [],
   "source": [
    "class GroupViTTextEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder consisting of `config.num_hidden_layers` self-attention layers. Each layer is a\n",
    "    [`GroupViTEncoderLayer`].\n",
    "\n",
    "    Args:\n",
    "        config: GroupViTTextConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: GroupViTTextConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layers = nn.ModuleList([GroupViTEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        inputs_embeds,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        causal_attention_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutput]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n",
    "                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n",
    "                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n",
    "                than the model's internal embedding lookup matrix.\n",
    "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            causal_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Causal mask for the text model. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            output_hidden_states (`bool`, *optional*):\n",
    "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
    "                for more detail.\n",
    "            return_dict (`bool`, *optional*):\n",
    "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        encoder_states = () if output_hidden_states else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "        for idx, encoder_layer in enumerate(self.layers):\n",
    "            if output_hidden_states:\n",
    "                encoder_states = encoder_states + (hidden_states,)\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(encoder_layer),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    causal_attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = encoder_layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    causal_attention_mask,\n",
    "                    output_attentions=output_attentions,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            encoder_states = encoder_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677512217566,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "fsgeLvGW_gB0"
   },
   "outputs": [],
   "source": [
    "# Copied from transformers.models.clip.modeling_clip.CLIPTextTransformer with CLIPText->GroupViTText, CLIPEncoder->GroupViTTextEncoder, CLIP_TEXT->GROUPVIT_TEXT\n",
    "class GroupViTTextTransformer(nn.Module):\n",
    "    def __init__(self, config: GroupViTTextConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        embed_dim = config.hidden_size\n",
    "        self.embeddings = GroupViTTextEmbeddings(config)\n",
    "        self.encoder = GroupViTTextEncoder(config)\n",
    "        self.final_layer_norm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(GROUPVIT_TEXT_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=GroupViTTextConfig)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if input_ids is None:\n",
    "            raise ValueError(\"You have to specify input_ids\")\n",
    "\n",
    "        input_shape = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, input_shape[-1])\n",
    "\n",
    "        hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n",
    "\n",
    "        bsz, seq_len = input_shape\n",
    "        # CLIP's text model uses causal mask, prepare it here.\n",
    "        # https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/clip/model.py#L324\n",
    "        causal_attention_mask = self._build_causal_attention_mask(bsz, seq_len, hidden_states.dtype).to(\n",
    "            hidden_states.device\n",
    "        )\n",
    "        # expand attention_mask\n",
    "        if attention_mask is not None:\n",
    "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "            attention_mask = _expand_mask(attention_mask, hidden_states.dtype)\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            inputs_embeds=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            causal_attention_mask=causal_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        last_hidden_state = encoder_outputs[0]\n",
    "        last_hidden_state = self.final_layer_norm(last_hidden_state)\n",
    "\n",
    "        # text_embeds.shape = [batch_size, sequence_length, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        # casting to torch.int for onnx compatibility: argmax doesn't support int64 inputs with opset 14\n",
    "        pooled_output = last_hidden_state[\n",
    "            torch.arange(last_hidden_state.shape[0], device=last_hidden_state.device),\n",
    "            input_ids.to(dtype=torch.int, device=last_hidden_state.device).argmax(dim=-1),\n",
    "        ]\n",
    "\n",
    "        if not return_dict:\n",
    "            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPooling(\n",
    "            last_hidden_state=last_hidden_state,\n",
    "            pooler_output=pooled_output,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def _build_causal_attention_mask(self, bsz, seq_len, dtype):\n",
    "        # lazily create causal attention mask, with full attention between the vision tokens\n",
    "        # pytorch uses additive attention mask; fill with -inf\n",
    "        mask = torch.empty(bsz, seq_len, seq_len, dtype=dtype)\n",
    "        mask.fill_(torch.tensor(torch.finfo(dtype).min))\n",
    "        mask.triu_(1)  # zero out the lower diagonal\n",
    "        mask = mask.unsqueeze(1)  # expand mask\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 1409,
     "status": "ok",
     "timestamp": 1677512218961,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "QAAhPken_ibc"
   },
   "outputs": [],
   "source": [
    "class GroupViTTextModel(GroupViTPreTrainedModel):\n",
    "    config_class = GroupViTTextConfig\n",
    "\n",
    "    def __init__(self, config: GroupViTTextConfig):\n",
    "        super().__init__(config)\n",
    "        self.text_model = GroupViTTextTransformer(config)\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self) -> nn.Module:\n",
    "        return self.text_model.embeddings.token_embedding\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.text_model.embeddings.token_embedding = value\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(GROUPVIT_TEXT_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=GroupViTTextConfig)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import CLIPTokenizer, GroupViTTextModel\n",
    "\n",
    "        >>> tokenizer = CLIPTokenizer.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "        >>> model = GroupViTTextModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "\n",
    "        >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n",
    "\n",
    "        >>> outputs = model(**inputs)\n",
    "        >>> last_hidden_state = outputs.last_hidden_state\n",
    "        >>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n",
    "        ```\"\"\"\n",
    "        return self.text_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1677512218961,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "pNJ4HozD_ls2"
   },
   "outputs": [],
   "source": [
    "class GroupViTVisionTransformer(nn.Module):########################\n",
    "    def __init__(self, config: GroupViTVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        embed_dim = config.hidden_size\n",
    "\n",
    "        self.embeddings = GroupViTVisionEmbeddings(config)\n",
    "        self.encoder = GroupViTVisionEncoder(config)\n",
    "        self.layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(GROUPVIT_VISION_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=GroupViTVisionConfig)\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if pixel_values is None:\n",
    "            raise ValueError(\"You have to specify pixel_values\")\n",
    "\n",
    "        hidden_states = self.embeddings(pixel_values)\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            hidden_states=hidden_states,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            output_attentions=output_attentions,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        last_hidden_state = encoder_outputs[0]\n",
    "\n",
    "        # normalize the last hidden state\n",
    "        last_hidden_state = self.layernorm(last_hidden_state)\n",
    "        pooled_output = last_hidden_state.mean(dim=1)\n",
    "\n",
    "        if not return_dict:\n",
    "            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPooling(\n",
    "            last_hidden_state=last_hidden_state,\n",
    "            pooler_output=pooled_output,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1677512218961,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "Tf5zCFfp_pS0"
   },
   "outputs": [],
   "source": [
    "class GroupViTVisionModel(GroupViTPreTrainedModel):\n",
    "    config_class = GroupViTVisionConfig\n",
    "    main_input_name = \"pixel_values\"\n",
    "\n",
    "    def __init__(self, config: GroupViTVisionConfig, projection_dim=128):\n",
    "        super().__init__(config)\n",
    "        self.vision_model = GroupViTVisionTransformer(config)\n",
    "\n",
    "        self.projection_dim = projection_dim\n",
    "        self.projection_intermediate_dim = 4096\n",
    "        self.vision_embed_dim = config.hidden_size\n",
    "\n",
    "        self.visual_projection = nn.Sequential(\n",
    "            nn.Linear(self.vision_embed_dim, self.projection_intermediate_dim, bias=True),\n",
    "            nn.BatchNorm1d(self.projection_intermediate_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.projection_intermediate_dim, self.projection_dim, bias=True),\n",
    "        )\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self) -> GroupViTPatchEmbeddings:\n",
    "        return self.vision_model.embeddings.patch_embeddings\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(GROUPVIT_VISION_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=GroupViTVisionConfig)\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from PIL import Image\n",
    "        >>> import requests\n",
    "        >>> from transformers import AutoProcessor, GroupViTVisionModel\n",
    "\n",
    "        >>> processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "        >>> model = GroupViTVisionModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "\n",
    "        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "        >>> image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "        >>> inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "        >>> outputs = model(**inputs)\n",
    "        >>> last_hidden_state = outputs.last_hidden_state\n",
    "        >>> pooled_output = outputs.pooler_output  # pooled CLS states\n",
    "        ```\"\"\"\n",
    "\n",
    "        # print(type(pixel_values), type(output_attentions), type(output_hidden_states), type(return_dict))\n",
    "        # print(pixel_values.shape, output_attentions, output_hidden_states, return_dict)\n",
    "        # print('pixel_values=', pixel_values.shape)\n",
    "        output_attentions = True\n",
    "        output_hidden_states = False\n",
    "        return_dict = True\n",
    "        # print(pixel_values.shape, output_attentions, output_hidden_states, return_dict)\n",
    "        vision_outputs = self.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        # print(vision_outputs)\n",
    "\n",
    "        attentions = vision_outputs[2]\n",
    "            \n",
    "        # [batch_size_image, num_group, height, width]\n",
    "        grouping = get_grouping_from_attentions(attentions, pixel_values.shape[2:])\n",
    "        seg_logits = grouping\n",
    "\n",
    "        pooled_output = vision_outputs[1]  # pooled_output\n",
    "        image_features = self.visual_projection(pooled_output)\n",
    "\n",
    "        # print(image_features.shape)\n",
    "        return vision_outputs, seg_logits, image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1677512218962,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "Q3lIeXoH_slB"
   },
   "outputs": [],
   "source": [
    "@add_start_docstrings(GROUPVIT_START_DOCSTRING)\n",
    "class GroupViTModel(GroupViTPreTrainedModel):\n",
    "    config_class = GroupViTConfig\n",
    "\n",
    "    def __init__(self, config: GroupViTConfig):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # if not isinstance(config.text_config, GroupViTTextConfig):\n",
    "        #     raise ValueError(\n",
    "        #         \"config.text_config is expected to be of type GroupViTTextConfig but is of type\"\n",
    "        #         f\" {type(config.text_config)}.\"\n",
    "        #     )\n",
    "\n",
    "        if not isinstance(config.vision_config, GroupViTVisionConfig):\n",
    "            raise ValueError(\n",
    "                \"config.vision_config is expected to be of type GroupViTVisionConfig but is of type\"\n",
    "                f\" {type(config.vision_config)}.\"\n",
    "            )\n",
    "\n",
    "        # text_config = config.text_config\n",
    "        vision_config = config.vision_config\n",
    "\n",
    "        self.projection_dim = config.projection_dim\n",
    "        self.projection_intermediate_dim = config.projection_intermediate_dim\n",
    "        # self.text_embed_dim = text_config.hidden_size\n",
    "        self.vision_embed_dim = vision_config.hidden_size\n",
    "        print('hidden_size', vision_config.hidden_size)\n",
    "\n",
    "        # self.text_model = GroupViTTextTransformer(text_config)\n",
    "        self.vision_model = GroupViTVisionTransformer(vision_config)\n",
    "\n",
    "        self.visual_projection = nn.Sequential(\n",
    "            nn.Linear(self.vision_embed_dim, self.projection_intermediate_dim, bias=True),\n",
    "            nn.BatchNorm1d(self.projection_intermediate_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.projection_intermediate_dim, self.projection_dim, bias=True),\n",
    "        )\n",
    "        # self.text_projection = nn.Sequential(\n",
    "        #     nn.Linear(self.text_embed_dim, self.projection_intermediate_dim, bias=True),\n",
    "        #     nn.BatchNorm1d(self.projection_intermediate_dim),\n",
    "        #     nn.ReLU(inplace=True),\n",
    "        #     nn.Linear(self.projection_intermediate_dim, self.projection_dim, bias=True),\n",
    "        # )\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * self.config.logit_scale_init_value)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    # @add_start_docstrings_to_model_forward(GROUPVIT_TEXT_INPUTS_DOCSTRING)\n",
    "    # def get_text_features(\n",
    "    #     self,\n",
    "    #     input_ids: Optional[torch.Tensor] = None,\n",
    "    #     attention_mask: Optional[torch.Tensor] = None,\n",
    "    #     position_ids: Optional[torch.Tensor] = None,\n",
    "    #     output_attentions: Optional[bool] = None,\n",
    "    #     output_hidden_states: Optional[bool] = None,\n",
    "    #     return_dict: Optional[bool] = None,\n",
    "    # ) -> torch.FloatTensor:\n",
    "    #     r\"\"\"\n",
    "    #     Returns:\n",
    "    #         text_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The text embeddings obtained by\n",
    "    #         applying the projection layer to the pooled output of [`GroupViTTextModel`].\n",
    "\n",
    "    #     Examples:\n",
    "\n",
    "    #     ```python\n",
    "    #     >>> from transformers import CLIPTokenizer, GroupViTModel\n",
    "\n",
    "    #     >>> model = GroupViTModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "    #     >>> tokenizer = CLIPTokenizer.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "\n",
    "    #     >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n",
    "    #     >>> text_features = model.get_text_features(**inputs)\n",
    "    #     ```\"\"\"\n",
    "    #     # Use GROUPVIT model's config for some fields (if specified) instead of those of vision & text components.\n",
    "    #     output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "    #     output_hidden_states = (\n",
    "    #         output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "    #     )\n",
    "    #     return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "    #     text_outputs = self.text_model(\n",
    "    #         input_ids=input_ids,\n",
    "    #         attention_mask=attention_mask,\n",
    "    #         position_ids=position_ids,\n",
    "    #         output_attentions=output_attentions,\n",
    "    #         output_hidden_states=output_hidden_states,\n",
    "    #         return_dict=return_dict,\n",
    "    #     )\n",
    "\n",
    "    #     pooled_output = text_outputs[1]\n",
    "    #     text_features = self.text_projection(pooled_output)\n",
    "\n",
    "    #     return text_features\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(GROUPVIT_VISION_INPUTS_DOCSTRING)\n",
    "    def get_image_features(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> torch.FloatTensor:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "            image_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The image embeddings obtained by\n",
    "            applying the projection layer to the pooled output of [`GroupViTVisionModel`].\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from PIL import Image\n",
    "        >>> import requests\n",
    "        >>> from transformers import AutoProcessor, GroupViTModel\n",
    "\n",
    "        >>> model = GroupViTModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "        >>> processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "\n",
    "        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "        >>> image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "        >>> inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "        >>> image_features = model.get_image_features(**inputs)\n",
    "        ```\"\"\"\n",
    "        # Use GROUPVIT model's config for some fields (if specified) instead of those of vision & text components.\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        print(pixel_values.shape, output_attentions, output_hidden_states, return_dict)\n",
    "\n",
    "        vision_outputs = self.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = vision_outputs[1]  # pooled_output\n",
    "        print('01 ', pooled_output.shape)\n",
    "\n",
    "        image_features = self.visual_projection(pooled_output)\n",
    "        print('02 ', image_features.shape)\n",
    "\n",
    "        return image_features\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(GROUPVIT_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=GroupViTModelOutput, config_class=GroupViTConfig)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        return_loss: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_segmentation: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, GroupViTModelOutput]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from PIL import Image\n",
    "        >>> import requests\n",
    "        >>> from transformers import AutoProcessor, GroupViTModel\n",
    "\n",
    "        >>> model = GroupViTModel.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "        >>> processor = AutoProcessor.from_pretrained(\"nvidia/groupvit-gcc-yfcc\")\n",
    "\n",
    "        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "        >>> image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "        >>> inputs = processor(\n",
    "        ...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True\n",
    "        ... )\n",
    "\n",
    "        >>> outputs = model(**inputs)\n",
    "        >>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "        >>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n",
    "        ```\"\"\"\n",
    "        # Use GROUPVIT model's config for some fields (if specified) instead of those of vision & text components.\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_segmentation = (\n",
    "            output_segmentation if output_segmentation is not None else self.config.output_segmentation\n",
    "        )\n",
    "        if output_segmentation:\n",
    "            output_attentions = True\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # print(type(pixel_values), type(output_attentions), type(output_hidden_states), type(return_dict))\n",
    "        print(pixel_values.shape, output_attentions, output_hidden_states, return_dict)\n",
    "\n",
    "        vision_outputs = self.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        # print(vision_outputs)\n",
    "\n",
    "        # text_outputs = self.text_model(\n",
    "        #     input_ids=input_ids,\n",
    "        #     attention_mask=attention_mask,\n",
    "        #     position_ids=position_ids,\n",
    "        #     output_attentions=output_attentions,\n",
    "        #     output_hidden_states=output_hidden_states,\n",
    "        #     return_dict=return_dict,\n",
    "        # )\n",
    "\n",
    "        image_embeds = vision_outputs[1]\n",
    "        image_embeds = self.visual_projection(image_embeds)\n",
    "\n",
    "        # text_embeds = text_outputs[1]\n",
    "        # text_embeds = self.text_projection(text_embeds)\n",
    "\n",
    "        # normalized features\n",
    "        image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n",
    "        # text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        # logit_scale = self.logit_scale.exp()\n",
    "        # logits_per_text = torch.matmul(text_embeds, image_embeds.t()) * logit_scale\n",
    "        # logits_per_image = logits_per_text.t()\n",
    "\n",
    "        seg_logits = None\n",
    "        if output_segmentation:\n",
    "            # grouped features\n",
    "            # [batch_size_image, num_group, hidden_size]\n",
    "            image_group_embeds = vision_outputs[0]\n",
    "            print('image_group_embeds_01', image_group_embeds.shape, type(image_group_embeds)) # torch.Size([1, 8, 384]) <class 'torch.Tensor'>\n",
    "\n",
    "            # [batch_size_image*num_group, hidden_size]\n",
    "            image_group_embeds = self.visual_projection(image_group_embeds.reshape(-1, image_group_embeds.shape[-1]))\n",
    "            print('image_group_embeds_02', image_group_embeds.shape, type(image_group_embeds)) # torch.Size([8, 256]) <class 'torch.Tensor'>\n",
    "\n",
    "            if output_hidden_states:\n",
    "                attentions = vision_outputs[3]\n",
    "                print('attentions_01', attentions.shape, type(attentions)) # *\n",
    "\n",
    "            else:\n",
    "                attentions = vision_outputs[2]\n",
    "                print('attentions_02', attentions[0].shape, type(attentions[0]), attentions[1].shape, type(attentions[1])) # torch.Size([1, 64, 196]) torch.Size([1, 8, 64]) <class 'torch.Tensor'>\n",
    "                \n",
    "            # [batch_size_image, num_group, height, width]\n",
    "            grouping = get_grouping_from_attentions(attentions, pixel_values.shape[2:])\n",
    "            print(pixel_values.shape)\n",
    "            print(pixel_values.shape[2:])\n",
    "            print('grouping_01', grouping.shape, type(grouping)) # torch.Size([1, 8, 224, 224]) <class 'torch.Tensor'>\n",
    "            seg_logits = grouping\n",
    "\n",
    "            # # normalized features\n",
    "            # image_group_embeds = image_group_embeds / image_group_embeds.norm(dim=-1, keepdim=True)\n",
    "            # print('image_group_embeds_03', image_group_embeds.shape, type(image_group_embeds)) # torch.Size([8, 256]) <class 'torch.Tensor'>\n",
    "\n",
    "            # # [batch_size_image x num_group, batch_size_text]\n",
    "            # logits_per_image_group = torch.matmul(image_group_embeds, text_embeds.t()) * logit_scale\n",
    "            # print('logits_per_image_group_01', logits_per_image_group.shape, type(logits_per_image_group)) # torch.Size([8, 3]) <class 'torch.Tensor'>\n",
    "\n",
    "            # # [batch_size_image, batch_size_text, num_group]\n",
    "            # logits_per_image_group = logits_per_image_group.reshape(\n",
    "            #     image_embeds.shape[0], -1, text_embeds.shape[0]\n",
    "            # ).permute(0, 2, 1)\n",
    "            # print('logits_per_image_group_02', logits_per_image_group.shape, type(logits_per_image_group)) # torch.Size([1, 3, 8]) <class 'torch.Tensor'>\n",
    "\n",
    "\n",
    "            # # [batch_size_image, batch_size_text, height x width]\n",
    "            # flatten_grouping = grouping.reshape(grouping.shape[0], grouping.shape[1], -1)\n",
    "            # print('flatten_grouping_01', flatten_grouping.shape, type(flatten_grouping)) # torch.Size([1, 8, 50176]) <class 'torch.Tensor'>\n",
    "\n",
    "\n",
    "            # # [batch_size_image, batch_size_text, height, width]\n",
    "            # seg_logits = torch.matmul(logits_per_image_group, flatten_grouping) * logit_scale\n",
    "            # print('seg_logits_01', seg_logits.shape, type(seg_logits)) # torch.Size([1, 3, 50176]) <class 'torch.Tensor'>\n",
    "\n",
    "            # seg_logits = seg_logits.reshape(\n",
    "            #     seg_logits.shape[0], seg_logits.shape[1], grouping.shape[2], grouping.shape[3]\n",
    "            # )\n",
    "            # print('seg_logits_02', seg_logits.shape, type(seg_logits)) # torch.Size([1, 3, 224, 224]) <class 'torch.Tensor'>\n",
    "\n",
    "        loss = None\n",
    "        if return_loss:\n",
    "            loss = groupvit_loss(logits_per_text)\n",
    "\n",
    "        if not return_dict:\n",
    "            if seg_logits is not None:\n",
    "                output = (\n",
    "                    logits_per_image,\n",
    "                    logits_per_text,\n",
    "                    seg_logits,\n",
    "                    text_embeds,\n",
    "                    image_embeds,\n",
    "                    text_outputs,\n",
    "                    vision_outputs,\n",
    "                )\n",
    "            else:\n",
    "                output = (logits_per_image, logits_per_text, text_embeds, image_embeds, text_outputs, vision_outputs)\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return GroupViTModelOutput(\n",
    "            loss=loss,\n",
    "            # logits_per_image=logits_per_image,\n",
    "            # logits_per_text=logits_per_text,\n",
    "            segmentation_logits=seg_logits,\n",
    "            # text_embeds=text_embeds,\n",
    "            image_embeds=image_embeds,\n",
    "            # text_model_output=text_outputs,\n",
    "            vision_model_output=vision_outputs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuwerq7N9s5S"
   },
   "source": [
    "### Set Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1677512218962,
     "user": {
      "displayName": "野菜浅",
      "userId": "15074908195438095604"
     },
     "user_tz": -480
    },
    "id": "dvuxcmejkKt8",
    "outputId": "4e6311d9-6fd0-43ed-c40a-e5541e35f105"
   },
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description='Train MoCo on CIFAR-10')\n",
    "\n",
    "# parser.add_argument('--finetune_data_dir', default=os.path.join(ROOT_PATH, '../lab_06', 'fine-tune_set', 'siim-acr-pneumothorax'))\n",
    "# parser.add_argument('--pretrain_data_dir', default=os.path.join(ROOT_PATH, '../lab_05', 'unlabel_pre-training_set'))\n",
    "\n",
    "# parser.add_argument('--image_size', default=256, type=int)\n",
    "\n",
    "# parser.add_argument('--lr', '--learning-rate', default=0.06, type=float, metavar='LR', help='initial learning rate', dest='lr')\n",
    "# parser.add_argument('--epochs', default=200, type=int, metavar='N', help='number of total epochs to run')\n",
    "# parser.add_argument('--batch-size', default=4, type=int, metavar='N', help='mini-batch size')\n",
    "\n",
    "# '''\n",
    "# args = parser.parse_args()  # running in command line\n",
    "# '''\n",
    "# args = parser.parse_args('')  # running in ipynb\n",
    "\n",
    "\n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygQeHtsngrC8"
   },
   "source": [
    "### Dataset Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_length_decode(rle, height=1024, width=1024, fill_value=1):\n",
    "    component = np.zeros((height, width), np.float32)\n",
    "    component = component.reshape(-1)\n",
    "    rle = np.array([int(s) for s in rle.strip().split(' ')])\n",
    "    rle = rle.reshape(-1, 2)\n",
    "    start = 0\n",
    "    for index, length in rle:\n",
    "        start = start+index\n",
    "        end = start+length\n",
    "        component[start: end] = fill_value\n",
    "        start = end\n",
    "    component = component.reshape(width, height).T\n",
    "    return component\n",
    "\n",
    "def run_length_encode(component):\n",
    "    component = component.T.flatten()\n",
    "    start = np.where(component[1:] > component[:-1])[0]+1\n",
    "    end = np.where(component[:-1] > component[1:])[0]+1\n",
    "    length = end-start\n",
    "    rle = []\n",
    "    for i in range(len(length)):\n",
    "        if i == 0:\n",
    "            rle.extend([start[0], length[0]])\n",
    "        else:\n",
    "            rle.extend([start[i]-end[i-1], length[i]])\n",
    "    rle = ' '.join([str(r) for r in rle])\n",
    "    return rle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(phase, size, mean, std):\n",
    "    list_transforms = []\n",
    "    if phase == \"train\":\n",
    "        list_transforms.extend(\n",
    "            [\n",
    "#                 HorizontalFlip(),\n",
    "                ShiftScaleRotate(\n",
    "                    shift_limit=0,  # no resizing\n",
    "                    scale_limit=0.1,\n",
    "                    rotate_limit=10, # rotate\n",
    "                    p=0.5,\n",
    "                    border_mode=cv2.BORDER_CONSTANT\n",
    "                ),\n",
    "#                 GaussNoise(),\n",
    "            ]\n",
    "        )\n",
    "    list_transforms.extend(\n",
    "        [\n",
    "            Resize(size, size),\n",
    "            Normalize(mean=mean, std=std, p=1),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    list_trfms = Compose(list_transforms)\n",
    "    return list_trfms\n",
    "\n",
    "def provider(\n",
    "    fold,\n",
    "    total_folds,\n",
    "    data_folder,\n",
    "    df_path,\n",
    "    phase,\n",
    "    size,\n",
    "    mean=None,\n",
    "    std=None,\n",
    "    batch_size=8,\n",
    "    num_workers=4,\n",
    "):\n",
    "    df_all = pd.read_csv(df_path)\n",
    "    df = df_all.drop_duplicates('ImageId')\n",
    "    df_with_mask = df[df[\" EncodedPixels\"] != \" -1\"]\n",
    "    df_with_mask['has_mask'] = 1\n",
    "    df_without_mask = df[df[\" EncodedPixels\"] == \" -1\"]\n",
    "    df_without_mask['has_mask'] = 0\n",
    "    df_without_mask_sampled = df_without_mask.sample(len(df_with_mask), random_state=69) # random state is imp\n",
    "    df = pd.concat([df_with_mask, df_without_mask_sampled])\n",
    "    \n",
    "    #NOTE: equal number of positive and negative cases are chosen.\n",
    "    \n",
    "    kfold = StratifiedKFold(total_folds, shuffle=True, random_state=69)\n",
    "    train_idx, val_idx = list(kfold.split(df[\"ImageId\"], df[\"has_mask\"]))[fold]\n",
    "    train_df, val_df = df.iloc[train_idx], df.iloc[val_idx]\n",
    "    df = train_df if phase == \"train\" else val_df\n",
    "    # NOTE: total_folds=5 -> train/val : 80%/20%\n",
    "    \n",
    "    fnames = df['ImageId'].values\n",
    "    \n",
    "    image_dataset = SIIMDataset(df_all, fnames, data_folder, size, mean, std, phase)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        image_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        # pin_memory=True,\n",
    "        shuffle=True,\n",
    "        generator=torch.Generator(device='cuda')\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get from path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader = provider(\n",
    "#     fold=0,\n",
    "#     total_folds=5,\n",
    "#     data_folder=data_folder,\n",
    "#     df_path=train_rle_path,\n",
    "#     phase=\"train\",\n",
    "#     size=512,\n",
    "#     mean = (0.485, 0.456, 0.406),\n",
    "#     std = (0.229, 0.224, 0.225),\n",
    "#     batch_size=16,\n",
    "#     num_workers=4,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(dataloader)) # get a batch from the dataloader\n",
    "# images, masks = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot some random images in the `batch`\n",
    "# idx = random.choice(range(16))\n",
    "# plt.imshow(images[idx][0], cmap='bone')\n",
    "# plt.imshow(masks[idx][0], alpha=0.2, cmap='Reds')\n",
    "# plt.show()\n",
    "# if len(np.unique(masks[idx][0])) == 1: # only zeros\n",
    "#     print('Chosen image has no ground truth mask, rerun the cell')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set dataset & loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12047\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>masks</th>\n",
       "      <th>tensor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/yasaisen/Desktop/11_research/11_research...</td>\n",
       "      <td>/home/yasaisen/Desktop/11_research/11_research...</td>\n",
       "      <td>/home/yasaisen/Desktop/11_research/11_research...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/yasaisen/Desktop/11_research/11_research...</td>\n",
       "      <td>/home/yasaisen/Desktop/11_research/11_research...</td>\n",
       "      <td>/home/yasaisen/Desktop/11_research/11_research...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/yasaisen/Desktop/11_research/11_research...</td>\n",
       "      <td>/home/yasaisen/Desktop/11_research/11_research...</td>\n",
       "      <td>/home/yasaisen/Desktop/11_research/11_research...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/yasaisen/Desktop/11_research/11_research...</td>\n",
       "      <td>/home/yasaisen/Desktop/11_research/11_research...</td>\n",
       "      <td>/home/yasaisen/Desktop/11_research/11_research...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/yasaisen/Desktop/11_research/11_research...</td>\n",
       "      <td>/home/yasaisen/Desktop/11_research/11_research...</td>\n",
       "      <td>/home/yasaisen/Desktop/11_research/11_research...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              images  \\\n",
       "0  /home/yasaisen/Desktop/11_research/11_research...   \n",
       "1  /home/yasaisen/Desktop/11_research/11_research...   \n",
       "2  /home/yasaisen/Desktop/11_research/11_research...   \n",
       "3  /home/yasaisen/Desktop/11_research/11_research...   \n",
       "4  /home/yasaisen/Desktop/11_research/11_research...   \n",
       "\n",
       "                                               masks  \\\n",
       "0  /home/yasaisen/Desktop/11_research/11_research...   \n",
       "1  /home/yasaisen/Desktop/11_research/11_research...   \n",
       "2  /home/yasaisen/Desktop/11_research/11_research...   \n",
       "3  /home/yasaisen/Desktop/11_research/11_research...   \n",
       "4  /home/yasaisen/Desktop/11_research/11_research...   \n",
       "\n",
       "                                              tensor  \n",
       "0  /home/yasaisen/Desktop/11_research/11_research...  \n",
       "1  /home/yasaisen/Desktop/11_research/11_research...  \n",
       "2  /home/yasaisen/Desktop/11_research/11_research...  \n",
       "3  /home/yasaisen/Desktop/11_research/11_research...  \n",
       "4  /home/yasaisen/Desktop/11_research/11_research...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # non-segmap_data\n",
    "\n",
    "images, masks, tensor = [], [], []\n",
    "\n",
    "i = 0\n",
    "\n",
    "for get_folder in os.listdir('/home/yasaisen/Desktop/11_research/11_research_main/lab_06/fine-tune_set/siim-acr-pneumothorax'):\n",
    "    if get_folder == 'png_images':\n",
    "        for get_file in os.listdir(os.path.join('/home/yasaisen/Desktop/11_research/11_research_main/lab_06/fine-tune_set/siim-acr-pneumothorax', 'png_images')):\n",
    "            # if get_file in os.listdir(os.path.join(Config.data_dir, 'png_masks')):\n",
    "            images += [os.path.join('/home/yasaisen/Desktop/11_research/11_research_main/lab_06/fine-tune_set/siim-acr-pneumothorax', 'png_images', get_file)]\n",
    "            masks += [os.path.join('/home/yasaisen/Desktop/11_research/11_research_main/lab_06/fine-tune_set/siim-acr-pneumothorax', 'png_masks', get_file)]\n",
    "            tensor += [os.path.join('/home/yasaisen/Desktop/11_research/11_research_main/lab_06/fine-tune_set/siim-acr-pneumothorax', 'step_set_tensor_v4.0', 'tensor_' + get_file.replace(get_file.split('.')[-1], '') + 'pt')]\n",
    "            i = i+1\n",
    "\n",
    "PathDF = pd.DataFrame({'images': images, 'masks': masks, 'tensor': tensor})\n",
    "print(i)\n",
    "PathDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SIIMDataset(Dataset):\n",
    "    def __init__(self, df, fnames, data_folder, size, mean, std, phase):\n",
    "        self.df = df\n",
    "        self.root = data_folder\n",
    "        self.size = size\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.phase = phase\n",
    "        self.transforms = get_transforms(phase, size, mean, std)\n",
    "        self.gb = self.df.groupby('ImageId')\n",
    "        self.fnames = fnames\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = PathDF.iloc[idx]['images']\n",
    "        mask_path = PathDF.iloc[idx]['masks']\n",
    "        tensor_path = PathDF.iloc[idx]['tensor']\n",
    "        \n",
    "        image = cv2.imread(img_path)\n",
    "\n",
    "        mask = np.asarray(Image.open(mask_path)).astype('float32')\n",
    "        mask = mask/255\n",
    "        \n",
    "        augmented = self.transforms(image=image, mask=mask)\n",
    "        image = augmented['image']\n",
    "        mask = augmented['mask']\n",
    "\n",
    "        tensor = torch.load(tensor_path)\n",
    "\n",
    "        return tensor.squeeze(), mask.unsqueeze(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return PathDF.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SIIMDataset(Dataset):\n",
    "#     def __init__(self, df, fnames, data_folder, size, mean, std, phase):\n",
    "#         self.df = df\n",
    "#         self.root = data_folder\n",
    "#         self.size = size\n",
    "#         self.mean = mean\n",
    "#         self.std = std\n",
    "#         self.phase = phase\n",
    "#         self.transforms = get_transforms(phase, size, mean, std)\n",
    "#         self.gb = self.df.groupby('ImageId')\n",
    "#         self.fnames = fnames\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         image_id = self.fnames[idx]\n",
    "#         df = self.gb.get_group(image_id)\n",
    "#         annotations = df[' EncodedPixels'].tolist()\n",
    "#         image_path = os.path.join(self.root, image_id + \".png\")\n",
    "#         image = cv2.imread(image_path)\n",
    "#         mask = np.zeros([1024, 1024])\n",
    "#         if annotations[0] != ' -1':\n",
    "#             for rle in annotations:\n",
    "#                 mask += run_length_decode(rle)\n",
    "#         mask = (mask >= 1).astype('float32') # for overlap cases\n",
    "#         augmented = self.transforms(image=image, mask=mask)\n",
    "#         image = augmented['image']\n",
    "#         mask = augmented['mask']\n",
    "#         return image, mask.unsqueeze(0)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsAVAtRoiBbG"
   },
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define base encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = smp.Unet(\"resnet34\", encoder_weights=\"imagenet\", activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.up_1 = nn.Sequential(nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2), nn.ReLU())\n",
    "    self.up_2 = nn.Sequential(nn.ConvTranspose2d( 64, 32, kernel_size=4, stride=4), nn.ReLU())\n",
    "    self.up_3 = nn.Sequential(nn.ConvTranspose2d( 32, 16, kernel_size=2, stride=2), nn.ReLU())\n",
    "    self.up_4 = nn.Sequential(nn.ConvTranspose2d( 16,  8, kernel_size=4, stride=4), nn.ReLU())\n",
    "    self.up_5 = nn.Sequential(nn.ConvTranspose2d(  8,  4, kernel_size=2, stride=2), nn.ReLU())\n",
    "    self.up_6 = nn.Sequential(nn.ConvTranspose2d(  4,  2, kernel_size=4, stride=4), nn.ReLU())\n",
    "    self.up_7 = nn.Sequential(nn.ConvTranspose2d(  2,  1, kernel_size=2, stride=2), nn.Sigmoid())\n",
    "\n",
    "  def forward(self, x):              # ([1, 128])\n",
    "    x = x.view(x.size(0), 128, 1, 1) # ([128,   1,   1])\n",
    "    x = self.up_1(x)                 # ([ 64,   3,   3])\n",
    "    x = self.up_2(x)                 # ([ 32,   7,   7])\n",
    "    x = self.up_3(x)                 # ([ 16,  14,  14])\n",
    "    x = self.up_4(x)                 # ([  8,  28,  28])\n",
    "    x = self.up_5(x)                 # ([  4,  56,  56])\n",
    "    x = self.up_6(x)                 # ([  2, 112, 112])\n",
    "    x = self.up_7(x)                 # ([  1, 224, 224])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Decoder().cuda()\n",
    "\n",
    "# t = torch.randn((32,128))\n",
    "# print(t.shape)\n",
    "# print(model(t).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = torch.load('/home/yasaisen/Desktop/11_research/11_research_main/lab_06/fine-tune_set/siim-acr-pneumothorax/step_set_tensor_v4.0/tensor_0_test_1_.pt')\n",
    "# print(t.shape)\n",
    "# print(model(t).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define MoCo wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YXcpXBwi8KV"
   },
   "source": [
    "### Define train/test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(input, target):\n",
    "    input = torch.sigmoid(input)\n",
    "    smooth = 1.0\n",
    "    iflat = input.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    return ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if not (target.size() == input.size()):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n",
    "                             .format(target.size(), input.size()))\n",
    "        max_val = (-input).clamp(min=0)\n",
    "        loss = input - input * target + max_val + \\\n",
    "            ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class MixedLoss(nn.Module):\n",
    "    def __init__(self, alpha, gamma):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.focal = FocalLoss(gamma)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        loss = self.alpha*self.focal(input, target) - torch.log(dice_loss(input, target))\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define train & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, threshold):\n",
    "    X_p = np.copy(X)\n",
    "    preds = (X_p > threshold).astype('uint8')\n",
    "    return preds\n",
    "\n",
    "def metric(probability, truth, threshold=0.5, reduction='none'):\n",
    "    '''Calculates dice of positive and negative images seperately'''\n",
    "    '''probability and truth must be torch tensors'''\n",
    "    batch_size = len(truth)\n",
    "    with torch.no_grad():\n",
    "        probability = probability.view(batch_size, -1)\n",
    "        truth = truth.view(batch_size, -1)\n",
    "        assert(probability.shape == truth.shape)\n",
    "\n",
    "        p = (probability > threshold).float()\n",
    "        t = (truth > 0.5).float()\n",
    "\n",
    "        t_sum = t.sum(-1)\n",
    "        p_sum = p.sum(-1)\n",
    "        neg_index = torch.nonzero(t_sum == 0)\n",
    "        pos_index = torch.nonzero(t_sum >= 1)\n",
    "\n",
    "        dice_neg = (p_sum == 0).float()\n",
    "        dice_pos = 2 * (p*t).sum(-1)/((p+t).sum(-1))\n",
    "\n",
    "        dice_neg = dice_neg[neg_index]\n",
    "        dice_pos = dice_pos[pos_index]\n",
    "        dice = torch.cat([dice_pos, dice_neg])\n",
    "\n",
    "        # dice_neg = np.nan_to_num(dice_neg.mean().item(), 0)\n",
    "        # dice_pos = np.nan_to_num(dice_pos.mean().item(), 0)\n",
    "        # dice = dice.mean().item()\n",
    "\n",
    "        num_neg = len(neg_index)\n",
    "        num_pos = len(pos_index)\n",
    "\n",
    "    return dice, dice_neg, dice_pos, num_neg, num_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Meter:\n",
    "    '''A meter to keep track of iou and dice scores throughout an epoch'''\n",
    "    def __init__(self, phase, epoch):\n",
    "        self.base_threshold = 0.5 # <<<<<<<<<<< here's the threshold\n",
    "        self.base_dice_scores = []\n",
    "        self.dice_neg_scores = []\n",
    "        self.dice_pos_scores = []\n",
    "        self.iou_scores = []\n",
    "\n",
    "    def update(self, targets, outputs):\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        dice, dice_neg, dice_pos, _, _ = metric(probs, targets, self.base_threshold)\n",
    "        self.base_dice_scores.extend(dice)\n",
    "        self.dice_pos_scores.extend(dice_pos)\n",
    "        self.dice_neg_scores.extend(dice_neg)\n",
    "        preds = predict(probs, self.base_threshold)\n",
    "        iou = compute_iou_batch(preds, targets, classes=[1])\n",
    "        self.iou_scores.append(iou)\n",
    "\n",
    "    def get_metrics(self):\n",
    "        dice = np.nanmean(self.base_dice_scores)\n",
    "        dice_neg = np.nanmean(self.dice_neg_scores)\n",
    "        dice_pos = np.nanmean(self.dice_pos_scores)\n",
    "\n",
    "        dices = [dice, dice_neg, dice_pos]\n",
    "        \n",
    "        iou = np.nanmean(self.iou_scores)\n",
    "        return dices, iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_log(phase, epoch, epoch_loss, meter, start):\n",
    "    '''logging the metrics at the end of an epoch'''\n",
    "    dices, iou = meter.get_metrics()\n",
    "    dice, dice_neg, dice_pos = dices\n",
    "    print(\"Loss: %0.4f | dice: %0.4f | dice_neg: %0.4f | dice_pos: %0.4f | IoU: %0.4f\" % (epoch_loss, dice, dice_neg, dice_pos, iou))\n",
    "    return dice, iou\n",
    "\n",
    "def compute_ious(pred, label, classes, ignore_index=255, only_present=True):\n",
    "    '''computes iou for one ground truth mask and predicted mask'''\n",
    "    pred[label == ignore_index] = 0\n",
    "    ious = []\n",
    "    for c in classes:\n",
    "        label_c = label == c\n",
    "        if only_present and np.sum(label_c) == 0:\n",
    "            ious.append(np.nan)\n",
    "            continue\n",
    "        pred_c = pred == c\n",
    "        intersection = np.logical_and(pred_c, label_c).sum()\n",
    "        union = np.logical_or(pred_c, label_c).sum()\n",
    "        if union != 0:\n",
    "            ious.append(intersection / union)\n",
    "    return ious if ious else [1]\n",
    "\n",
    "\n",
    "def compute_iou_batch(outputs, labels, classes=None):\n",
    "\n",
    "    # print(outputs.shape, np.unique(outputs))\n",
    "    # print(labels.shape, np.unique(labels))\n",
    "\n",
    "    '''computes mean iou for a batch of ground truth masks and predicted masks'''\n",
    "    ious = []\n",
    "    preds = np.copy(outputs) # copy is imp\n",
    "    labels = np.array(labels) # tensor to np\n",
    "    for pred, label in zip(preds, labels):\n",
    "        ious.append(np.nanmean(compute_ious(pred, label, classes)))\n",
    "    iou = np.nanmean(ious)\n",
    "\n",
    "    # print(iou.shape, np.unique(iou))\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    '''This class takes care of training and validation of our model'''\n",
    "    def __init__(self, model):\n",
    "        self.fold = 1\n",
    "        self.total_folds = 5\n",
    "        self.num_workers = 0\n",
    "        self.batch_size = {\"train\": 4, \"val\": 4}\n",
    "        self.accumulation_steps = 32 // self.batch_size['train']\n",
    "        self.lr = 5e-4\n",
    "        self.num_epochs = 40\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.phases = [\"train\", \"val\"]\n",
    "        self.device = torch.device(\"cuda:0\")\n",
    "        torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "        self.net = model\n",
    "        self.criterion = MixedLoss(10.0, 2.0)\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode=\"min\", patience=3, verbose=True)\n",
    "        self.net = self.net.to(self.device)\n",
    "        cudnn.benchmark = True\n",
    "        self.dataloaders = {\n",
    "            phase: provider(\n",
    "                fold=1,\n",
    "                total_folds=5,\n",
    "                data_folder=data_folder,\n",
    "                df_path=train_rle_path,\n",
    "                phase=phase,\n",
    "                size=1024,\n",
    "                mean=(0.485, 0.456, 0.406),\n",
    "                std=(0.229, 0.224, 0.225),\n",
    "                batch_size=self.batch_size[phase],\n",
    "                num_workers=self.num_workers,\n",
    "            )\n",
    "            for phase in self.phases\n",
    "        }\n",
    "        self.losses = {phase: [] for phase in self.phases}\n",
    "        self.iou_scores = {phase: [] for phase in self.phases}\n",
    "        self.dice_scores = {phase: [] for phase in self.phases}\n",
    "        \n",
    "    def forward(self, images, targets):\n",
    "        images = images.to(self.device)\n",
    "        masks = targets.to(self.device)\n",
    "        outputs = self.net(images)\n",
    "        loss = self.criterion(outputs, masks)\n",
    "        return loss, outputs\n",
    "\n",
    "    def iterate(self, epoch, phase):\n",
    "        meter = Meter(phase, epoch)\n",
    "        start = time.strftime(\"%H:%M:%S\")\n",
    "        print(f\"Starting epoch: {epoch} | phase: {phase} | ⏰: {start}\")\n",
    "        batch_size = self.batch_size[phase]\n",
    "        self.net.train(phase == \"train\")\n",
    "        dataloader = self.dataloaders[phase]\n",
    "        running_loss = 0.0\n",
    "        total_batches = len(dataloader)\n",
    "#         tk0 = tqdm(dataloader, total=total_batches)\n",
    "        self.optimizer.zero_grad()\n",
    "        for itr, batch in enumerate(dataloader):\n",
    "            images, targets = batch\n",
    "            loss, outputs = self.forward(images, targets)\n",
    "            loss = loss / self.accumulation_steps\n",
    "            if phase == \"train\":\n",
    "                loss.backward()\n",
    "                if (itr + 1 ) % self.accumulation_steps == 0:\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "            running_loss += loss.item()\n",
    "            outputs = outputs.detach().cpu()\n",
    "            meter.update(targets, outputs)\n",
    "#             tk0.set_postfix(loss=(running_loss / ((itr + 1))))\n",
    "        epoch_loss = (running_loss * self.accumulation_steps) / total_batches\n",
    "        dice, iou = epoch_log(phase, epoch, epoch_loss, meter, start)\n",
    "        self.losses[phase].append(epoch_loss)\n",
    "        self.dice_scores[phase].append(dice)\n",
    "        self.iou_scores[phase].append(iou)\n",
    "        torch.cuda.empty_cache()\n",
    "        return epoch_loss\n",
    "\n",
    "    def start(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.iterate(epoch, \"train\")\n",
    "            state = {\n",
    "                \"epoch\": epoch,\n",
    "                \"best_loss\": self.best_loss,\n",
    "                \"state_dict\": self.net.state_dict(),\n",
    "                \"optimizer\": self.optimizer.state_dict(),\n",
    "            }\n",
    "            val_loss = self.iterate(epoch, \"val\")\n",
    "            self.scheduler.step(val_loss)\n",
    "            if val_loss < self.best_loss:\n",
    "                print(\"******** New optimal found, saving state ********\")\n",
    "                state[\"best_loss\"] = self.best_loss = val_loss\n",
    "                torch.save(state, \"./model.pth\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86lHkiKox3KO"
   },
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0 | phase: train | ⏰: 14:02:14\n",
      "Loss: 12.0283 | dice: 0.0059 | dice_neg: 0.0000 | dice_pos: 0.0264 | IoU: 0.0134\n",
      "Starting epoch: 0 | phase: val | ⏰: 14:08:55\n",
      "Loss: 11.9182 | dice: 0.0059 | dice_neg: 0.0000 | dice_pos: 0.0265 | IoU: 0.0134\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 1 | phase: train | ⏰: 14:15:13\n"
     ]
    }
   ],
   "source": [
    "model_trainer = Trainer(model)\n",
    "model_trainer.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT TRAINING\n",
    "losses = model_trainer.losses\n",
    "dice_scores = model_trainer.dice_scores # overall dice\n",
    "iou_scores = model_trainer.iou_scores\n",
    "\n",
    "def plot(scores, name):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(range(len(scores[\"train\"])), scores[\"train\"], label=f'train {name}')\n",
    "    plt.plot(range(len(scores[\"train\"])), scores[\"val\"], label=f'val {name}')\n",
    "    plt.title(f'{name} plot'); plt.xlabel('Epoch'); plt.ylabel(f'{name}');\n",
    "    plt.legend(); \n",
    "    plt.show()\n",
    "\n",
    "plot(losses, \"BCE loss\")\n",
    "plot(dice_scores, \"Dice score\")\n",
    "plot(iou_scores, \"IoU score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, root, df, size, mean, std, tta=4):\n",
    "        self.root = root\n",
    "        self.size = size\n",
    "        self.fnames = list(df[\"ImageId\"])\n",
    "        self.num_samples = len(self.fnames)\n",
    "        self.transform = Compose(\n",
    "            [\n",
    "                Normalize(mean=mean, std=std, p=1),\n",
    "                Resize(size, size),\n",
    "                ToTensorV2(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.fnames[idx]\n",
    "        path = os.path.join(self.root, fname + \".png\")\n",
    "        image = cv2.imread(path)\n",
    "        images = self.transform(image=image)[\"image\"]\n",
    "        return images\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "def post_process(probability, threshold, min_size):\n",
    "    mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]\n",
    "    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n",
    "    predictions = np.zeros((1024, 1024), np.float32)\n",
    "    num = 0\n",
    "    for c in range(1, num_component):\n",
    "        p = (component == c)\n",
    "        if p.sum() > min_size:\n",
    "            predictions[p] = 1\n",
    "            num += 1\n",
    "    return predictions, num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 512\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "num_workers = 8\n",
    "batch_size = 16\n",
    "best_threshold = 0.5\n",
    "min_size = 3500\n",
    "device = torch.device(\"cuda:0\")\n",
    "df = pd.read_csv(sample_submission_path)\n",
    "testset = DataLoader(\n",
    "    TestDataset(test_data_folder, df, size, mean, std),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "model = model_trainer.net # get the model from model_trainer object\n",
    "model.eval()\n",
    "state = torch.load('./model.pth', map_location=lambda storage, loc: storage)\n",
    "model.load_state_dict(state[\"state_dict\"])\n",
    "encoded_pixels = []\n",
    "for i, batch in enumerate(tqdm(testset)):\n",
    "    preds = torch.sigmoid(model(batch.to(device)))\n",
    "    preds = preds.detach().cpu().numpy()[:, 0, :, :] # (batch_size, 1, size, size) -> (batch_size, size, size)\n",
    "    for probability in preds:\n",
    "        if probability.shape != (1024, 1024):\n",
    "            probability = cv2.resize(probability, dsize=(1024, 1024), interpolation=cv2.INTER_LINEAR)\n",
    "        predict, num_predict = post_process(probability, best_threshold, min_size)\n",
    "        if num_predict == 0:\n",
    "            encoded_pixels.append('-1')\n",
    "        else:\n",
    "            r = run_length_encode(predict)\n",
    "            encoded_pixels.append(r)\n",
    "df['EncodedPixels'] = encoded_pixels\n",
    "df.to_csv('submission.csv', columns=['ImageId', 'EncodedPixels'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/facebookresearch/moco/blob/colab-notebook/colab/moco_cifar10_demo.ipynb",
     "timestamp": 1677256490794
    }
   ]
  },
  "environment": {
   "name": "pytorch-gpu.1-4.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06768cfdf690415e9affdf2a74e59a30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "17032952ca804b558931b93c0fde1540": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cc091016bb8d423f80dddca00096ba0b",
       "IPY_MODEL_6bbbd0d7f0f749939610ccc1aa47bfec",
       "IPY_MODEL_eb9b54187a9b4404b93ce96cb5297a1e"
      ],
      "layout": "IPY_MODEL_48854cfeccd84183a1819d703353a4fa"
     }
    },
    "1d94bafa0c204861869e3c8656f88338": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20faf3fda02c4257b5ac68099de45581": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "23596ead25e842da821eee9281ddd44b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce5a59adc10e4ad3bbd5ee949704a493",
      "placeholder": "​",
      "style": "IPY_MODEL_2e92aabde375495f880ae9752b55e057",
      "value": " 4.64k/4.64k [00:00&lt;00:00, 115kB/s]"
     }
    },
    "2e92aabde375495f880ae9752b55e057": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "31ea43544fcd44a19f86c920afaa12ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3deab35d86db420c963d21fde4f4f770": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1d94bafa0c204861869e3c8656f88338",
      "placeholder": "​",
      "style": "IPY_MODEL_06768cfdf690415e9affdf2a74e59a30",
      "value": "Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"
     }
    },
    "3e187d79da6f477180cac4b95d949388": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "48854cfeccd84183a1819d703353a4fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5528a5ca02bc4cde9fa1bad142519e35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_79cc1507b29d4125ae5e3c704b2b8b75",
      "placeholder": "​",
      "style": "IPY_MODEL_31ea43544fcd44a19f86c920afaa12ab",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "55606bf393b940278f76baf65170eeab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5bffaeb6f389499c958f4a12dca192e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5d806cd2fe7e4424b7fb90371815f4d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3deab35d86db420c963d21fde4f4f770",
       "IPY_MODEL_e3214b7f823d4a12bed90f36b6cd3bbe",
       "IPY_MODEL_6bec28962d4740aabe8e9896e00134b7"
      ],
      "layout": "IPY_MODEL_5da754b21ef34dc9a7eff14bdc3a34bc"
     }
    },
    "5da754b21ef34dc9a7eff14bdc3a34bc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6bbbd0d7f0f749939610ccc1aa47bfec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4d8fcd96c2b44c1a21e6a0cf46d736d",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5bffaeb6f389499c958f4a12dca192e4",
      "value": 170498071
     }
    },
    "6bec28962d4740aabe8e9896e00134b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d083420df95d42ecb16cedd5037dc2a3",
      "placeholder": "​",
      "style": "IPY_MODEL_55606bf393b940278f76baf65170eeab",
      "value": " 223M/223M [00:02&lt;00:00, 86.7MB/s]"
     }
    },
    "73487b45ea444f8e81410a9627c85b40": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79cc1507b29d4125ae5e3c704b2b8b75": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7cdb20bd656249fe85674962d2a4ba7e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9cf2f5d1a7894a73861ce8bd69675ed1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a4d8fcd96c2b44c1a21e6a0cf46d736d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aeabbf27ff5e4ef5a0c3de652c86d632": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5528a5ca02bc4cde9fa1bad142519e35",
       "IPY_MODEL_c3b291f042fe422aa3a2b22f1208d0ca",
       "IPY_MODEL_23596ead25e842da821eee9281ddd44b"
      ],
      "layout": "IPY_MODEL_d74ac37d81974a4780581b554f3e7d23"
     }
    },
    "c3b291f042fe422aa3a2b22f1208d0ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73487b45ea444f8e81410a9627c85b40",
      "max": 4642,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3e187d79da6f477180cac4b95d949388",
      "value": 4642
     }
    },
    "cc091016bb8d423f80dddca00096ba0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7cdb20bd656249fe85674962d2a4ba7e",
      "placeholder": "​",
      "style": "IPY_MODEL_20faf3fda02c4257b5ac68099de45581",
      "value": "100%"
     }
    },
    "ce5a59adc10e4ad3bbd5ee949704a493": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf90228e48af46809d35402d94eb568e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d083420df95d42ecb16cedd5037dc2a3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d74ac37d81974a4780581b554f3e7d23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9070ee51e9c4ebe868777345a25a62d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3214b7f823d4a12bed90f36b6cd3bbe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f48ad91e88b340258d9fbe66bc58696b",
      "max": 223137427,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9cf2f5d1a7894a73861ce8bd69675ed1",
      "value": 223137427
     }
    },
    "eb9b54187a9b4404b93ce96cb5297a1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d9070ee51e9c4ebe868777345a25a62d",
      "placeholder": "​",
      "style": "IPY_MODEL_cf90228e48af46809d35402d94eb568e",
      "value": " 170498071/170498071 [00:14&lt;00:00, 14802420.28it/s]"
     }
    },
    "f48ad91e88b340258d9fbe66bc58696b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
